---
title: "First Order Vector Autoregression"
author: "Benjamin Christoffersen"
date: "`r Sys.Date()`"
output: 
  md_document:
    variant: markdown_github
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.height = 4, fig.width = 7, dpi = 128,
  cache.path = "cache/firstOrderVAR-cache/", fig.path = "fig/firstOrderVAR-fig/", 
  error = FALSE)
options(digits = 4, scipen = 10, width = 70)
```

```{r confs, echo = FALSE, cache = 1}
n_threads <- 4 # I ran this on a quad core machine

# you can replace this with e.g.,  
# max(parallel::detectCores(logical = FALSE), 2)))
```


## Introduction

We will simulate and estimate a first order vector auto-regression model with 
the in this example using the particle filter and smoother. For details see 
[this vignette](../vignettes/Particle_filtering.pdf) which can also be found by 
calling `vignette("Particle_filtering", package = "dynamichazard")`. The models 
we are going to simulate from and estimate are of the form 

$$
\begin{split}
 	y_{it} &\sim g(\cdot\vert\eta_{it}) &  \\
 	\vec{\eta}_t &= X_tR^+\vec{\alpha}_t + Z_t\vec{\beta} +  \vec{o}_t \\
 	\vec{\alpha}_t &= F\vec{\alpha}_{t - 1} + R\vec{\epsilon}_t &
 		\quad \vec{\epsilon}_t \sim N(\vec{0}, Q) \\
	& &	\quad \vec{\alpha}_0 \sim N(\vec{a}_0, Q_0)
\end{split}, \qquad
\begin{array}{l} i = 1, \dots, n_t \\ t = 1, \dots, d \end{array}
$$

where the $y_{it}$'s are Poisson distributed event indicators conditional on 
knowing the log time at risk, $\vec{o}_t$, covariates, $X_t$ and $Z_t$, and latent states, $\vec{\alpha}_1,\dots,\vec{\alpha}_d$. 
The model is a precise-wise constant exponentially distributed event times
conditional on knowing the latent states. Further, we set $Z_t = X_t$ so the 
states have a non-zero mean. The true values are 

$$
F = \begin{pmatrix}0.9 & 0 \\ 0 & 0.9 \end{pmatrix}, \quad
Q = \begin{pmatrix}0.33^2 & 0 \\ 0 & 0.33^2 \end{pmatrix}, \quad
R = I_2, \quad 
\vec{\beta} = (-6.5, -2)^\top
$$

```{r assign_kit_info, echo = FALSE}
git_key <- system("git rev-parse --short HEAD", intern = TRUE)
git_bra <- system("git branch", intern = TRUE)
regexp <- "^(\\*\ )(.+)$"
git_bra <- git_bra[grepl(regexp, git_bra)]
git_bra <- gsub(regexp, "\\2", git_bra)
```

where $I_2$ is the two-dimensional identity matrix and 
$\vec{a}_0$ and $Q_0$ are given by the invariant distribution. The unknown 
parameters to be estimated is everything but $Q_0$ and $R$ (since we fix $Q_0$ doing the estimation and we set $\vec{a}_0 = (0, 0)^\top$). This 
example is run on the git branch "`r git_bra`" with key "`r git_key`". The code
can be found on 
[the github site for the package](.).
All functions which assignments are not shown and are not in the 
`dynamichazard` package can be found on the github site.

## Simulation

We start by simulating the data. Feel free to skip this part as the specifications 
are given above. First we assign the parameters for the simulation

```{r assign_get_Q_0, echo = FALSE}
# function to find Q_0
get_Q_0 <- function(Qmat, Fmat){
  # compute with above formulas
  eg  <- eigen(Fmat)
  las <- eg$values
  if(any(abs(las) >= 1))
    stop("Divergent series")
  U   <- eg$vectors
  U_t <- t(U)
  T.  <- crossprod(U, Qmat %*% U)
  Z   <- T. / (1 - tcrossprod(las))
  solve(U_t, t(solve(U_t, t(Z))))
}
```

```{r assign_sim_params, cache = 1}
n_obs     <- 1000L
n_periods <- 200L

Fmat <- matrix(c(.9, 0, 0, .9), 2)
Rmat <- diag(1    , 2)
Qmat <- diag(.33^2, 2)
Q_0  <- get_Q_0(Qmat, Fmat)
beta <- c(-6.5, -2)
```

`get_Q_0` is a function to get the covariance matrix for the invariant distribution.
We then simulate and plot the latent states

```{r sim_plot_latent}
set.seed(54432125)
betas <- matrix(nrow = n_periods + 1, ncol = 2)
betas[1, ] <- rnorm(2) %*% chol(Q_0)
for(i in 1:n_periods + 1)
  betas[i, ] <- Fmat %*% betas[i - 1, ] + drop(rnorm(2) %*% chol(Qmat))

betas <- t(t(betas) + beta)

# plot of latent variables
cols <- c("black", "darkblue")
matplot(betas, type = "l", lty = 1, col = cols)
for(i in 1:2)
  abline(h = beta[i], lty = 2, col = cols[i])
```

Then we simulate the observations

```{r sim_obs}
df <- replicate(n_obs, {
  # left-censoring
  tstart <- max(0L, sample.int((n_periods - 1L) * 2L, 1) - n_periods + 1L)

  # covariates
  x <- runif(1, -1, 1)
  covars <- c(1, x)

  # outcome (stop time and event indicator)
  y <- FALSE
  for(tstop in (tstart + 1L):n_periods){
    fail_time <- rexp(1) / exp(covars %*% betas[tstop + 1L, ])
    if(fail_time <= 1){
      y <- TRUE
      tstop <- tstop - 1L + fail_time
      break
    }
  }

  c(tstart = tstart, tstop = tstop, x = x, y = y)
})
df <- data.frame(t(df))
head(df, 10)
```

We left-censor the observations since we otherwise may end up with a low number
of observations towards the end. 

### Model without latent variables
We can fit a model without the latent variables (i.e., a constant coefficient
model) as follows

```{r survreg_fit}
library(survival)
surv_fit <- survreg(Surv(tstop - tstart, y) ~ x, df, dist = "exponential")
summary(surv_fit) # signs are flipped
logLik(surv_fit)
```

The signs are flipped as stated in `help("survreg")`. They are though close to 
$\vec{\beta}$ as expected. Further, we can compare the log-likelihood with this
models with the log-likelihood approximation we get from the particle filter
in the next section.

## Particle filter and smoother

We start off by using the generalized two-filter smoother from [@briers09]. We 
estimate the parameters with an EM algorithm by calling `PF_EM` as 
follows

```{r load_dynam}
library(dynamichazard)
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("brier_far_F", "pf_Brier", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r brier_far_F, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
system.time(pf_Brier <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = diag(1, 2), Fmat = matrix(c(.1, 0, 0, .1), 2), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 750, N_smooth = 1, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Brier_O_N_square",
    # we add some extra variation to the proposal distributions
    Q_tilde = diag(.3^2, 2), n_threads = n_threads)))
```

`system.time` is used to show the computation time. The estimated parameters are

```{r brier_ests_far_F}
pf_Brier$F
chol(pf_Brier$Q)
pf_Brier$fixed_effects
```

The effective sample size in the smoothing step at each time point is 

```{r brier_eff_far_F}
plot(pf_Brier$effective_sample_size$smoothed_clouds, type = "h", 
     ylim = range(0, pf_Brier$effective_sample_size$smoothed_clouds), 
     ylab = "Effective sample size")
```

```{r plot_clouds, echo = FALSE}
plot_cloud <- function(
  pf_fit, type = "smoothed_clouds", start_at_zero = FALSE){
  par_old <- par(no.readonly = TRUE)
  on.exit(par(par_old))
  
  # plot random effects
  jpeg(tempfile()) # avoid output from the plot
  out <- plot(pf_fit, type = type, col = c("black", "blue"))
  dev.off()
  par(par_old)

  # then we use it and add the fixed effects
  y <- t(t(out$mean) + pf_fit$fixed_effects)
  ylim <- range(betas, y)
  x <- if(start_at_zero) 1:nrow(out$mean) - 1  else 1:nrow(out$mean)
  matplot(x, t(t(out$mean) + pf_fit$fixed_effects), type = "l",
          lty = 1, col = c("black", "blue"), ylim = ylim)

  # add actual points
  matplot(0:n_periods, betas, type = "l", add = TRUE,
          lty = 2, col = c("black", "blue"), ylim = ylim)

  # show places where we had a hard time sampling
  ess <- pf_fit$effective_sample_size[[type]]
  idx <- which(ess <= 100)
  for(i in idx - start_at_zero)
    abline(v = i, lty = 2)
  invisible(ess)
}
```

The smoothed estimates of the latent states looks as follows (the vertical dashed lines are points where 
we did not sample well)

```{r brier_plot_far_F}
plot_cloud(pf_Brier)
```

The dashed non-vertical lines are the true curve and the continuous is the smoothed estimate. The 
approximate log-likelihoods from the particle filter at the EM iterations are

```{r brier_log_like_far_F}
plot(pf_Brier$log_likes)
```

We can compare the output above with the smoother from [@fearnhead10]

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("fear_far_F", "pf_Fear", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r fear_far_F, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
system.time(pf_Fear <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = diag(1, 2), Fmat = matrix(c(.1, 0, 0, .1), 2), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 500, N_smooth = 2000, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Fearnhead_O_N",
    Q_tilde = diag(.3^2, 2), n_threads = n_threads)))
```

The estimates are similar

```{r fear_ests_far_F}
pf_Fear$F
chol(pf_Fear$Q)
pf_Fear$fixed_effects
```

The effective sample size are better now though

```{r fear_eff_far_F}
plot(pf_Fear$effective_sample_size$smoothed_clouds, type = "h", 
     ylim = range(0, pf_Fear$effective_sample_size$smoothed_clouds), 
     ylab = "Effective sample size")
```

However, there are many options to reduce the computation time for the former smoother as e.g, 
mentioned in [@briers09] but those are not implemented. The smoothed estimates are 
also rather close

```{r fear_plot_far_F}
plot_cloud(pf_Fear)
```

The log-likelihoods are almost the same

```{r fear_log_like_far_F}
plot(pf_Fear$log_likes)
```

The log-likelihood where flat toward the ensds and not monotonically increasing

```{r fear_log_like_far_F_tail}
plot(tail(pf_Fear$log_likes, 20), type = "l")
```

```{r clean_up, echo = FALSE}
rm(pf_Brier)
```

This will not happen in an EM algorithm but may happen in an MCEM algorithm due
to the Monte Carlo error. We may want to take a few more EM iterations with an
increase the number of particle. We can do this as follows

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("more_parts", "pf_Fear_more", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r more_parts, cache = 1, dependson = "fear_far_F"}
set.seed(30520116)
pf_Fear_more <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = pf_Fear$Q, Fmat = pf_Fear$F, 
  fixed_effects = pf_Fear$fixed_effects, by = 1, type = "VAR", 
  model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 5000, N_smooth = 10000, N_first = 5000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 10, smoother = "Fearnhead_O_N",
    Q_tilde = diag(.3^2, 2), n_threads = n_threads))
```

The log-likelihoods from these iterations are 

```{r more_parts_pf_Fear_log_like}
plot(pf_Fear_more$log_likes)
```

and the final estimates are

```{r more_parts_pf_Fear_show_params}
pf_Fear_more$F
chol(pf_Fear_more$Q)
pf_Fear_more$fixed_effects
```



### Log-likehood evaluation 
We may question what the log-likelihood is at the true parameters. We can use 
the `PF_forward_filter` function to perform approximate log-likelihood evaluation

```{r pF_forward_filter_true, cache = 1, dependson = c("assign_sim_params", "confs")}
fw_true <- PF_forward_filter(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE),
  N_fw = 10000, N_first = 10000, df, type = "VAR",
  model = "exponential", max_T = n_periods, by = 1,
  control = PF_control(
    N_fw_n_bw = 1, N_smooth = 1, N_first = 1,
    method = "AUX_normal_approx_w_cloud_mean",
    smoother = "Fearnhead_O_N", Q_tilde = diag(.3^2, 2), 
    n_threads = n_threads),
  Fmat = Fmat, a_0 = c(0, 0), Q = Qmat, Q_0 = Q_0, R = diag(1, 2),
  fixed_effects = beta)
logLik(fw_true)
```

The log-likelihood from the final model we estimated is

```{r end_log_like_before}
print(tail(pf_Fear_more$log_likes, 1), digits = 7)
```

or we can get a more precise estimate by calling (though the log-likelihood 
is evaluated at the final parameter estimates and not the iteration one
step prior as above)

```{r end_log_like_before_precise, cache = 1, dependson = "fear_far_F"}
print(
  logLik(PF_forward_filter(pf_Fear_more, N_fw = 20000, N_first = 20000)), 
  digits = 7)
```

A question is what happens if we start at the true parameter values? We may expect that 
we only take a few EM iterations and end up at the MLE or another local 
maximum not to far from the true parameters

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  tmp <- knitr::load_cache("fear_true", "pf_Fear_close", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r fear_true, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
pf_Fear_close <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Fmat = Fmat, a_0 = c(0, 0), Q = Qmat, Q_0 = Q_0, fixed_effects = beta,
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 500, N_smooth = 2000, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Fearnhead_O_N",
    Q_tilde = diag(.3^2, 2), n_threads = n_threads))
```

The final estimates are

```{r fear_true_log_like_ests}
pf_Fear_close$F
chol(pf_Fear_close$Q)
pf_Fear_close$fixed_effects
```

The log-likelihoods at the EM iterations look as follows

```{r fear_true_log_like}
plot(pf_Fear_close$log_likes, type = "l")
```

## References
