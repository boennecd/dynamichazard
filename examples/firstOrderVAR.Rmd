---
title: "First Order Vector Autoregression"
author: "Benjamin Christoffersen"
date: "`r Sys.Date()`"
output: html_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.height = 4, fig.width = 7, dpi = 128,
  cache.path = "cache/firstOrderVAR-cache/", fig.path = "fig/firstOrderVAR-fig/", 
  error = FALSE)
options(digits = 4, scipen = 10, width = 70)
```


## Introduction

We will simulate and estimate a first order vector auto-regression model in 
this example using the particle filter and smoother. For details see 
[this vignette](../vignettes/Particle_filtering.pdf) which can also be found by 
calling `vignette("Particle_filtering", package = "dynamichazard")`. The models 
we are going to simulate from and estimate are of the form 

$$
\begin{split}
 	y_{it} &\sim g(\cdot\vert\eta_{it}) &  \\
 	\vec{\eta}_t &= X_tR^+\vec{\alpha}_t + Z_t\vec{\beta} +  \vec{o}_t \\
 	\vec{\alpha}_t &= F\vec{\alpha}_{t - 1} + R\vec{\epsilon}_t &
 		\quad \vec{\epsilon}_t \sim N(\vec{0}, Q) \\
	& &	\quad \vec{\alpha}_0 \sim N(\vec{a}_0, Q_0)
\end{split}, \qquad
\begin{array}{l} i = 1, \dots, n_t \\ t = 1, \dots, d \end{array}
$$

where the $y_{it}$ is individual $i$'s indicator at time $t$ for whether he dies 
between time $(t - 1, t]$. The indicators, $y_{it}$, 
are binomial distributed with the complementary log-log link function conditional on 
knowing the log times at risk, $\vec{o}_1,\cdots,\vec{o}_d$, covariates, $X_t$ and $Z_t$, and latent states, $\vec{\alpha}_1,\dots,\vec{\alpha}_d$. 
The total survival time of individual $i$ is $T_i$ which is 
piecewise constant exponentially distributed
conditional on knowing the latent states. Further, we set $Z_t = X_t$ so the 
states have a non-zero mean. The true values are 

$$
F = \begin{pmatrix}0.9 & 0 \\ 0 & 0.9 \end{pmatrix}, \quad
Q = \begin{pmatrix}0.33^2 & 0 \\ 0 & 0.33^2 \end{pmatrix}, \quad
R = I_2, \quad 
\vec{\beta} = (-6.5, -2)^\top
$$

```{r assign_kit_info, echo = FALSE}
git_key <- system("git rev-parse --short HEAD", intern = TRUE)
git_bra <- system("git branch", intern = TRUE)
regexp <- "^(\\*\ )(.+)$"
git_bra <- git_bra[grepl(regexp, git_bra)]
git_bra <- gsub(regexp, "\\2", git_bra)
```

where $I_2$ is the two-dimensional identity matrix and 
$\vec{a}_0$ and $Q_0$ are given by the invariant distribution. The unknown 
parameters to be estimated is everything but $Q_0$ and $R$ (since we fix $Q_0$ doing the estimation and we set $\vec{a}_0 = (0, 0)^\top$). This 
example is run on the git branch "`r git_bra`" with ID "`r git_key`". The code
can be found on 
[the github site for the package](https://github.com/boennecd/dynamichazard/tree/master/examples).
All functions which assignments are not shown and are not in the 
`dynamichazard` package can be found on the github site.

## Simulation

We start by simulating the data. Feel free to skip this part as the specifications 
are given above. First we assign the parameters for the simulation

```{r load_dynam}
library(dynamichazard)
```

```{r assign_sim_params, cache = 1}
n_obs     <- 1000L
n_periods <- 200L

Fmat <- matrix(c(.9, 0, 0, .9), 2)
Rmat <- diag(1    , 2)
Qmat <- diag(.33^2, 2)
Q_0  <- get_Q_0(Qmat, Fmat)
beta <- c(-6.5, -2)
```

`get_Q_0` is a function to get the covariance matrix for the invariant distribution.
Then we simulate and plot the latent states

```{r sim_plot_latent}
set.seed(54432125)
betas <- matrix(nrow = n_periods + 1, ncol = 2)
betas[1, ] <- rnorm(2) %*% chol(Q_0)
for(i in 1:n_periods + 1)
  betas[i, ] <- Fmat %*% betas[i - 1, ] + drop(rnorm(2) %*% chol(Qmat))

betas <- t(t(betas) + beta)

# plot of latent variables
cols <- c("black", "darkblue")
matplot(betas, type = "l", lty = 1, col = cols)
for(i in 1:2)
  abline(h = beta[i], lty = 2, col = cols[i])
```

We simulate the observations as follows

```{r sim_obs}
df <- replicate(n_obs, {
  # left-censoring
  tstart <- max(0L, sample.int((n_periods - 1L) * 2L, 1) - n_periods + 1L)

  # covariates
  x <- runif(1, -1, 1)
  covars <- c(1, x)

  # outcome (stop time and event indicator)
  y <- FALSE
  for(tstop in (tstart + 1L):n_periods){
    fail_time <- rexp(1) / exp(covars %*% betas[tstop + 1L, ])
    if(fail_time <= 1){
      y <- TRUE
      tstop <- tstop - 1L + fail_time
      break
    }
  }

  c(tstart = tstart, tstop = tstop, x = x, y = y)
})
df <- data.frame(t(df))
head(df, 10)
```

We left-censor the observations since we otherwise may end up with a low number
of observations towards the end. 

### Model without latent variables
We can fit a model without the latent variables (i.e., a constant coefficient
model) as follows

```{r survreg_fit}
surv_fit <- survreg(Surv(tstop - tstart, y) ~ x, df, dist = "exponential")
summary(surv_fit) # signs are flipped
logLik(surv_fit)
```

The signs are flipped as stated in `help("survreg")`. They are though close to 
$\vec{\beta}$ as expected. Further, we can compare the log-likelihood with this
models with the log-likelihood approximation we get from the particle filter
in the next section.

## Particle filter and smoother

We start off by using the generalized two-filter smoother from @briers09. We 
estimate the parameters with an EM algorithm by calling `PF_EM` as 
follows

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("brier_far_F", "pf_Brier", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r confs, cache = 1}
n_threads <- 6

# you can replace this with e.g.,  
# max(parallel::detectCores(logical = FALSE), 2)))
```

```{r brier_far_F, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
system.time(pf_Brier <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = diag(1, 2), Fmat = matrix(c(.1, 0, 0, .1), 2), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 750, N_smooth = 1, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Brier_O_N_square",
    # we add some extra variation to the proposal distributions
    nu = 8, covar_fac = 1.1, n_threads = n_threads)))
```

`system.time` is used to show the computation time. The estimated parameters are

```{r brier_ests_far_F}
pf_Brier$F
show_covar <- function(Q){
  cat("Standard deviations\n")
  print(sqrt(diag(Q)))
  cat("Lower correlation matrix\n")
  tmp <- cov2cor(Q)
  print(tmp[-1, -ncol(tmp)])
}
show_covar(pf_Brier$Q)
pf_Brier$fixed_effects
```

The effective sample size in the smoothing step at each time point is 

```{r brier_eff_far_F}
plot(pf_Brier$effective_sample_size$smoothed_clouds, type = "h", 
     ylim = range(0, pf_Brier$effective_sample_size$smoothed_clouds), 
     ylab = "Effective sample size")
```

```{r plot_clouds, echo = FALSE}
plot_cloud <- function(
  pf_fit, type = "smoothed_clouds", start_at_zero = FALSE){
  par_old <- par(no.readonly = TRUE)
  on.exit(par(par_old))
  
  # plot random effects
  jpeg(tempfile()) # avoid output from the plot
  out <- plot(pf_fit, type = type, col = c("black", "blue"))
  dev.off()
  par(par_old)

  # then we use it and add the fixed effects
  y <- t(t(out$mean) + pf_fit$fixed_effects)
  ylim <- range(betas, y)
  x <- if(start_at_zero) 1:nrow(out$mean) - 1  else 1:nrow(out$mean)
  matplot(x, t(t(out$mean) + pf_fit$fixed_effects), type = "l",
          lty = 1, col = c("black", "blue"), ylim = ylim)

  # add actual points
  matplot(0:n_periods, betas, type = "l", add = TRUE,
          lty = 2, col = c("black", "blue"), ylim = ylim)

  # show places where we had a hard time sampling
  ess <- pf_fit$effective_sample_size[[type]]
  idx <- which(ess <= 100)
  for(i in idx - start_at_zero)
    abline(v = i, lty = 2)
  invisible(ess)
}
```

The smoothed estimates of the latent states looks as follows (the vertical dashed lines are points where 
we did not sample well)

```{r brier_plot_far_F}
plot_cloud(pf_Brier)
```

The dashed non-vertical lines are the true curve and the continuous is the smoothed estimate. The 
approximate log-likelihoods from the particle filter at each EM iteration are

```{r brier_log_like_far_F}
plot(pf_Brier$log_likes)
```

We can compare the output above with the smoother from @fearnhead10

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("fear_far_F", "pf_Fear", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r fear_far_F, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
system.time(pf_Fear <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = diag(1, 2), Fmat = matrix(c(.1, 0, 0, .1), 2), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 1000, N_smooth = 2000, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Fearnhead_O_N",
    nu = 8, covar_fac = 1.1, n_threads = n_threads)))
```

The estimates are similar

```{r fear_ests_far_F}
pf_Fear$F
show_covar(pf_Fear$Q)
pf_Fear$fixed_effects
```

The effective sample size are better now though

```{r fear_eff_far_F}
plot(pf_Fear$effective_sample_size$smoothed_clouds, type = "h", 
     ylim = range(0, pf_Fear$effective_sample_size$smoothed_clouds), 
     ylab = "Effective sample size")
```

However, there are many options to reduce the computation time for the former smoother as e.g, 
mentioned in @briers09 but those are not implemented. The smoothed estimates are 
also rather close to what we saw before 

```{r fear_plot_far_F}
plot_cloud(pf_Fear)
```

The log-likelihoods are almost the same

```{r fear_log_like_far_F}
plot(pf_Fear$log_likes)
```

The log-likelihood where flat toward the end and not monotonically increasing

```{r fear_log_like_far_F_tail}
plot(tail(pf_Fear$log_likes, 20))
```

This will not happen in an EM algorithm but may happen in an MCEM algorithm due
to the Monte Carlo error. We may want to take a few more EM iterations with
more particles. We can do this as follows

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("more_parts", "pf_Fear_more", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r more_parts, cache = 1, dependson = "fear_far_F"}
set.seed(30520116)
pf_Fear_more <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = pf_Fear$Q, Fmat = pf_Fear$F, 
  fixed_effects = pf_Fear$fixed_effects, by = 1, type = "VAR", 
  model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 5000, N_smooth = 10000, N_first = 5000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 10, smoother = "Fearnhead_O_N",   
    nu = 8, covar_fac = 1.1, n_threads = n_threads))
```

The log-likelihoods from these iterations are 

```{r more_parts_pf_Fear_log_like}
plot(pf_Fear_more$log_likes)
```

and the final estimates are

```{r more_parts_pf_Fear_show_params}
pf_Fear_more$F
show_covar(pf_Fear_more$Q)
pf_Fear_more$fixed_effects
```

### Averaging
Averaging is sometimes argued for in MCEM algorithm. E.g., see @cappe05 
section 11.1.2.2. The idea is to take an average the estimates or sufficient 
sufficient statistic after some iteration number. As of this writing, 
the present implementation does not allow one to change the number of particles
at each iteration as suggested in @cappe05.

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("averaging", "pf_Fear_avg", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r averaging, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
system.time(pf_Fear_avg <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = diag(1, 2), Fmat = matrix(c(.1, 0, 0, .1), 2), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods, 
  control = PF_control(
    N_fw_n_bw = 200, N_smooth = 400, N_first = 1000, eps = 1e-5,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 1000L, smoother = "Fearnhead_O_N", averaging_start = 150L,
    nu = 8, covar_fac = 1.1, n_threads = n_threads)))
```

```{r show_averaging_res}
pf_Fear_avg$F
show_covar(pf_Fear_avg$Q)
pf_Fear_avg$fixed_effects

plot(pf_Fear_avg$log_likes, type = "l")
```

### Stochastic gradient descent

We will run stochastic gradient descent as in @polyak92 which is described 
in @cappe05 [page 414]. 

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("sgd", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r sgd, cache=1}
#####
# fit to start from
set.seed(30520116)
pf_start <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), Q = diag(1, 2), Fmat = matrix(c(.1, 0, 0, .1), 2), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods, 
  control = PF_control(
    N_fw_n_bw = 200, N_smooth = 400, N_first = 1000, eps = 1e-5,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 1L, smoother = "Fearnhead_O_N",
    nu = 8, covar_fac = 1.1, n_threads = n_threads))

# function to perform stochastic gradient descent
sgd <- function(n_runs, ns, lrs, debug = FALSE, use_O_n_sq = FALSE, 
                avg_start = 100L)
{
  #####
  # get object to perform computation and matrices and vectors for output
  comp_obj <- PF_get_score_n_hess(pf_start, use_O_n_sq = use_O_n_sq)
  state <- matrix(
    NA_real_, n_runs + 1L, length(pf_start$F) + length(pf_start$Q))
  obs   <- matrix(
    NA_real_, n_runs + 1L, length(pf_start$fixed_effects))
  state[1, ] <- c(pf_start$F, pf_start$Q)
  obs  [1, ] <- pf_start$fixed_effects
  lls <- rep(NA_real_, n_runs)
  
  for(i in 1:n_runs){
    comp_obj$set_n_particles(N_fw = ns[i], N_first = ns[i])
    fw <- comp_obj$run_particle_filter()
    lls[i] <- logLik(fw)
    score <- comp_obj$get_get_score_n_hess(only_score = TRUE)
    state_i <- state[i + 1L, ]  <- state[i, ]  + lrs[i] * score$state$score
    obs_i   <- obs  [i + 1L, ]  <- obs  [i, ]  + lrs[i] * score$observation$score
    
    if(debug){
      msg <- paste0(sprintf("It %4d Log-likelihood %6.1f", i, lls[i])) 
      cat(msg, "\n", rep("-", nchar(msg)), "\n", sep = "")
    }
    
    if(i > avg_start){
      idx <- avg_start:i
      state_i <- colMeans(state[idx, ])
      obs_i   <- colMeans(obs  [idx, ])
      
    }
    
    o <- comp_obj$set_parameters(state = state_i, obs = obs_i)
    
    if(debug){
      print(o$Fmat)
      print(o$Q)
      print(o$fixed_effects)
      cat("\n")
    }
  }
  
  list(o = o, lls = lls, state = state, obs = obs)
}

# number of iterations
n_runs <- 200L
# number of particles to use
ns <- ceiling(exp(seq(log(500), log(5000), length.out = n_runs)))
# step sizes
lrs <- .005 * (1:n_runs)^(-1/2)

out <- sgd(n_runs = n_runs, ns = ns, lrs = lrs)
lls <- out$lls
o <- out$o
```

The results are illustrated below

```{r show_sgd_res}
# approximate log-likelihood at each iteration
par(mar = c(5, 4, .5, .5))
plot(lls, xlab = "Iteration", ylab = "Log-likelihood", type = "l")
print(max(lls), digits = 5)

# the estimates
o$Fmat
show_covar(o$Q)
o$fixed_effects
```

We can also use the method from @Poyiadjis11. The present implementation 
scales poorly in the number of particles but the method has a variance 
that may be linear in the number of time periods instead of quadratic. 
This is important for long time series.

```{r O_N_sq_sgd, cache = 1}
# number of iterations
n_runs <- 200L
# number of particles to use
ns <- ceiling(exp(seq(log(500), log(1000), length.out = n_runs)))
# step sizes
lrs <- .005 * (1:n_runs)^(-1/2)

out <- sgd(n_runs = n_runs, ns = ns, lrs = lrs, use_O_n_sq = TRUE, 
           avg_start = 100L)
lls <- out$lls
o <- out$o
```

The results are illustrated below (the log-likelihoods approximation from 
the forward particle filter are poor approximations due to few particles).

```{r O_N_sq_show_sgd_res}
# approximate log-likelihood at each iteration
par(mar = c(5, 4, .5, .5))
plot(lls, xlab = "Iteration", ylab = "Log-likelihood", type = "l")
print(max(lls), digits = 5)

# the estimates
o$Fmat
show_covar(o$Q)
o$fixed_effects
```

### Log-likelihood evaluation 
We may question what the log-likelihood is at the true parameters. We can use 
the `PF_forward_filter` function to perform approximate log-likelihood evaluation

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("pF_forward_filter_true", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r pF_forward_filter_true, cache = 1, dependson = c("assign_sim_params", "confs")}
fw_precise <- PF_forward_filter( 
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE),
  N_fw = 100000, N_first = 100000, df, type = "VAR",
  model = "exponential", max_T = n_periods, by = 1,
  control = PF_control(
    N_fw_n_bw = 1, N_smooth = 1, N_first = 1,
    method = "AUX_normal_approx_w_cloud_mean",
    smoother = "Fearnhead_O_N",
    n_threads = n_threads, nu = 8, covar_fac = 1.1),
  Fmat = Fmat, a_0 = c(0, 0), Q = Qmat, Q_0 = Q_0, R = diag(1, 2),
  fixed_effects = beta)
logLik(fw_precise)
```

The log-likelihood from the final model we estimated is

```{r end_log_like_before}
print(tail(pf_Fear_more$log_likes, 1), digits = 7)
print(tail(pf_Brier$log_likes, 1), digits = 7)
```

or we can get a more precise estimate by calling (though the log-likelihood 
is evaluated at the final parameter estimates and not the iteration one
step prior as above)

```{r end_log_like_before_precise, cache = 1, dependson = "fear_far_F"}
print( 
  logLik(PF_forward_filter(pf_Fear_more, N_fw = 20000, N_first = 20000)), 
  digits = 7)
```

A question is what happens if we start at the true parameter values? We may expect that 
we only take a few EM iterations and end up at the MLE or another local 
maximum not to far from the true parameters

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  tmp <- knitr::load_cache("fear_true", "pf_Fear_close", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r fear_true, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
pf_Fear_close <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Fmat = Fmat, a_0 = c(0, 0), Q = Qmat, Q_0 = Q_0, fixed_effects = beta,
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 500, N_smooth = 2000, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Fearnhead_O_N",
    nu = 8, covar_fac = 1.1, n_threads = n_threads))
```

The final estimates are

```{r fear_true_log_like_ests}
pf_Fear_close$F
show_covar(pf_Fear_close$Q)
pf_Fear_close$fixed_effects
```

The log-likelihoods at the EM iterations look as follows

```{r fear_true_log_like}
plot(pf_Fear_close$log_likes, type = "l")
```

### Restricted model 
The model can be written as in terms of 2 parameters for the state model by 
writing it as 

$$
F = \begin{pmatrix}
      \theta & 0 \\ 
      0 & \theta
    \end{pmatrix}, \quad
Q = \begin{pmatrix}
    \exp(2\psi) & 0 \\
    0 & \exp(2\psi)
    \end{pmatrix}
$$

We can estimate the restricted model as follows (see the vignette mentioned in 
the beginning for details about the arguments to `PF_EM`).

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("est_restrict", "pf_Fear_restrict", path = "examples/cache/firstOrderVAR-cache/") 
-->

```{r est_restrict, cache = 1}
G <- matrix(0, 2^2, 1)
G[1, 1] <- G[4, 1] <- 1
J <- matrix(1, 2, 1)
K <- matrix(nrow = 2 * (2 - 1) / 2, ncol = 0)

pf_Fear_restrict <- PF_EM(
  Surv(tstart, tstop, y) ~ x + ddFixed(x) + ddFixed_intercept(TRUE), df,
  Q_0 = diag(1, 2), G = G, J = J, K = K, theta = .1, psi = 0, phi = numeric(),
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 500, N_smooth = 2000, N_first = 2000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 100, smoother = "Fearnhead_O_N",
    nu = 8, covar_fac = 1.1, n_threads = n_threads))
```

The final estimates are

```{r est_restrict_show_ests}
pf_Fear_restrict$F
show_covar(pf_Fear_restrict$Q)
pf_Fear_restrict$fixed_effects
```

and a log-likelihood approximation is

```{r pf_Fear_restrict_ll, cache = 1, dependson = "est_restrict"}
print( 
  logLik(PF_forward_filter(pf_Fear_restrict, N_fw = 20000, N_first = 20000)), 
  digits = 7)
```

We have $2^2 + 2(2 + 1) / 2 - 4 = 3$ fewer parameters so the difference in 
log-likelihood to the full model seems reasonable.

## References
