---
title: "Restricted Vector Autoregression"
author: "Benjamin Christoffersen"
date: "`r Sys.Date()`"
output: html_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.height = 4, fig.width = 7, dpi = 128,
  cache.path = "cache/restrictedVAR-cache/", fig.path = "fig/restrictedVAR-fig/", 
  error = FALSE)
options(digits = 4, scipen = 10, width = 70)
```

```{r set_cols, echo = FALSE}
palette(
  c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", 
    "#D55E00", "#CC79A7"))
```


We will simulate and estimate a first order vector auto-regression model in 
this example using the particle filter and smoother. For details see 
[this vignette](../vignettes/Particle_filtering.pdf) which can also be found by 
calling `vignette("Particle_filtering", package = "dynamichazard")`. The models 
we are going to simulate from and estimate are of the form 

$$
\begin{split}
 y_{it} &\sim g(\cdot\vert\eta_{it}) &  \\
 	\eta_{it} &= x_i\alpha_{th(i)} + \beta_0 +  z_i\beta_{h(i)} +  o_{it} \\
 	\vec{\alpha}_t &= F\vec{\alpha}_{t - 1} + R\vec{\epsilon}_t &
 		\quad \vec{\epsilon}_t \sim N(\vec{0}, Q) \\
	& &	\quad \vec{\alpha}_0 \sim N(\vec{a}_0, Q_0)
\end{split}, \qquad
\begin{array}{l} i \in \{1, \dots, n\} \\ t = 1, \dots, d \end{array}
$$

where the $y_{it}$ is individual $i$'s indicator at time $t$ for whether he dies 
between time $(t - 1, t]$ and $h:\,\{1,\dots n\} \rightarrow \{1,\dots,4\}$ 
returns the group index individual $i$ belongs to. The indicators, $y_{it}$, are Poisson distributed conditional on 
knowing the log time at risk, $o_{it}$, covariates, $x_t$ and $z_t$, and latent states, $\vec{\alpha}_1,\dots,\vec{\alpha}_d$. 
The total survival time of individual $i$ is $T_i$ which is 
piecewise constant exponentially distributed
conditional on knowing the latent states. Further, we set $z_i = x_i$ so the 
states have a non-zero mean. The true values are 

$$
F = \begin{pmatrix}
    \theta_1 & . & . & \theta_3 \\ 
    . & \theta_2 & . & . \\
    . & . & \theta_2 & . \\
    \theta_3 & . & . & \theta_1 \end{pmatrix}, \quad
Q = \begin{pmatrix}
    \psi_1 & . & \psi_3 & . \\
    . & \psi_1 & . & . \\
    \psi_3 & . & \psi_2 & . \\ 
    . & . & . & \psi_2 \end{pmatrix}, \quad
R = I_4, \quad
$$

$$
\vec{\beta} = (-6.5, -1, -0.5, 0.5, 1)^\top, \quad
\vec{\theta} =(0.66, 0.8, 0.2)^\top, \quad
\vec{\psi} =(0.4, 0.25, -0.2)^\top
$$


```{r assign_kit_info, echo = FALSE}
git_key <- system("git rev-parse --short HEAD", intern = TRUE)
git_bra <- system("git branch", intern = TRUE)
regexp <- "^(\\*\ )(.+)$"
git_bra <- git_bra[grepl(regexp, git_bra)]
git_bra <- gsub(regexp, "\\2", git_bra)
```

where $I_4$ is the four-dimensional identity matrix, $.$ is used instead of $0$ 
to put emphasis on the non-zero elements, and 
$\vec{a}_0$ and $Q_0$ are given by the invariant distribution. The unknown 
parameters to be estimated is everything but $Q_0$ and $R$ (since we fix $Q_0$ doing the estimation and we set $\vec{a}_0 = (0, 0,0,0)^\top$). This 
example is run on the git branch "`r git_bra`" with ID "`r git_key`". The code
can be found on 
[the github site for the package](../examples).
All functions which assignments are not shown and are not in the 
`dynamichazard` package can be found on the github site.

We are going to estimate the parameters in an unrestricted model where we 
estimate the $Q$ and $F$ and a restricted model where we estimate 
$\vec{\theta}$ and $\vec{\psi}$. In the later case, we can write 

$$
\text{vec}(R^+F) = G\vec{\theta}, \qquad \nu(Q) = J\vec{\psi}
$$

where $\text{vec}(\cdot)$ is the [vectorization function](https://en.wikipedia.org/wiki/Vectorization_(mathematics))
and $\nu(\cdot)$ is the function [half-vectorizaiton function](https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Half-vectorization) 
that that removes all superdiagonal elements 
of a square $n\times n$ matrix and stack the remaining $n(n+1)/2$ elements. $G$
and $J$ are 

$$
\begin{split}
G &= \begin{pmatrix} 
    1 & . & . & . & . & . & . & . & . & . & . & . & . & . & . & 1 \\
    . & . & . & . & . & 1 & . & . & . & . & 1 & . & . & . & . & . \\
    . & . & . & 1 & . & . & . & . & . & . & . & . & 1 & . & . & .
\end{pmatrix}^\top \\
J &= \begin{pmatrix} 
    1 & . & . & . & 1 & . & . & . & . & . \\
    . & . & . & . & . & . & . & 1 & . & 1 \\
    . & . & 1 & . & . & . & . & . & . & .
\end{pmatrix}^\top
\end{split}
$$


## Simulation

```{r assign_get_Q_0, echo = FALSE}
# function to find Q_0
get_Q_0 <- function(Qmat, Fmat){
  # see https://math.stackexchange.com/q/2854333/253239
  eg  <- eigen(Fmat)
  las <- eg$values
  if(any(abs(las) >= 1))
    stop("Divergent series")
  U   <- eg$vectors
  U_t <- t(U)
  T.  <- crossprod(U, Qmat %*% U)
  Z   <- T. / (1 - tcrossprod(las))
  solve(U_t, t(solve(U_t, t(Z))))
}
```

We start by simulating the data. Feel free to skip this part as the specifications 
are given above. First we assign the parameters for the simulation

```{r set_params}
# assign G, theta, and F
G <- matrix(0., 4^2, 3)
idx <- 1 + (0:(4 - 1)) * 4 + 0:(4 - 1)
G[idx[-(2:3)], 1] <- 1
G[idx[  2:3 ], 2] <- 1
G[c(4, 13)   , 3] <-  1
theta <- c(.66, .8, .2)
(F. <- matrix(as.vector(G %*% theta), 4, 4))

# assign J, psi, and Q
J <- matrix(0., 4 * (4 + 1) / 2, 3)
tmp <- 0
for(i in 4:1){
  J[1 + tmp, if(i > 2) 1 else 2] <- 1
  tmp <- tmp + i
}
J[3, 3] <- 1
psi <- c(.4, .25, -.2)
Q <- matrix(0., 4, 4)
Q[lower.tri(Q, diag = TRUE)] <- J %*% psi
Q[upper.tri(Q)] <- t(Q)[upper.tri(Q)]
Q

# assign Q_0 and beta
Q_0  <- get_Q_0(Q, F.)
beta <- c(-6.5, -1, -0.5, 0.5, 1)
```

`get_Q_0` is a function to get the covariance matrix for the invariant distribution.
Then we simulate and plot the latent states

```{r sim_plot_latent}
set.seed(54432125)
n_periods <- 200
alphas <- matrix(nrow = n_periods + 1, ncol = 4)
alphas[1, ] <- rnorm(4) %*% chol(Q_0)
for(i in 1:n_periods + 1)
  alphas[i, ] <- F. %*% alphas[i - 1, ] + drop(rnorm(4) %*% chol(Q))

alphas <- t(t(alphas) + beta[-1])

# plot of latent variables
matplot(alphas, type = "l", lty = 1)
for(i in 1:4)
  abline(h = beta[i + 1], lty = 2, col = i)
```

We simulate the observations as follows

```{r sim_obs}
n_obs <- 1000
df <- sapply(1:n_obs, function(i){
  # find the group
  grp <- (i - 1L) %/% (n_obs / 4L) + 1L
  
  # left-censoring
  tstart <- max(0L, sample.int((n_periods - 1L) * 2L, 1) - n_periods + 1L)

  # covariates
  x <- runif(1, 0, 2)

  # outcome (stop time and event indicator)
  y <- FALSE
  for(tstop in (tstart + 1L):n_periods){
    fail_time <- rexp(1) / exp(beta[1] + x * alphas[tstop + 1L, grp])
    if(fail_time <= 1){
      y <- TRUE
      tstop <- tstop - 1L + fail_time
      break
    }
  }

  c(tstart = tstart, tstop = tstop, x = x, y = y, grp = grp)
})
df <- data.frame(t(df))
df$grp <- factor(df$grp)

# prepare data. Needed to avoid error in `ddFixed` 
df <- within(df, {
  x1 <- x * (grp == 1)
  x2 <- x * (grp == 2)
  x3 <- x * (grp == 3)
  x4 <- x * (grp == 4)
})
```

We left-censor the observations since we otherwise may end up with a low number
of observations towards the end. We show a few properties of the sample

```{r show_sims}
# how many die in each group
xtabs(~ grp + y, df)

# at what time do we have events?
tmp <- aggregate(y ~ grp + as.integer(df$tstop), df, sum, subset = df$y == 1)
tmp$time <- tmp$`as.integer(df$tstop)` + 1L
plot(c(1, n_periods), range(tmp$y, 0), type = "n", xlab = "time", 
     ylab = "# events")
for(i in 1:4){
  dat <- subset(tmp, grp == i)
  x <- dat$time + .2 * (i - 2.5)
  segments(x, rep(0, length(x)), x, dat$y, col = i)
}
```

### Model without latent variables
We can fit a model without the latent variables (i.e., a constant coefficient
model) as follows

```{r survreg_fit}
library(survival)
surv_fit <- survreg(Surv(tstop - tstart, y) ~ x1 + x2 + x3 + x4, df, 
                    dist = "exponential")
summary(surv_fit) # signs are flipped
logLik(surv_fit)
```

The signs are flipped as stated in `help("survreg")`. We can compare the 
log-likelihood with this models with the log-likelihood approximation we get 
from the particle filter in the next section.

## Particle filter and smoother
We use the generalized two-filter smoother from [@fearnhead10] were we estimate 
the full $F$ and $Q$ matrix.

```{r load_dynam}
library(dynamichazard)
```

```{r confs, cache = 1}
n_threads <- 4 # I ran this on a quad core machine

# you can replace this with e.g.,  
# max(parallel::detectCores(logical = FALSE), 2)))
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("pf_first", "pf_Fear", path = "examples/cache/restrictedVAR-cache/") 
-->

```{r pf_first, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
system.time(pf_Fear <- PF_EM(
  Surv(tstart, tstop, y) ~ ddFixed_intercept() + x1 + x2 + x3 + x4 + 
    ddFixed(x1) + ddFixed(x2) + ddFixed(x3) + ddFixed(x4), 
  df, Q_0 = diag(1, 4), Q = diag(1, 4), Fmat = diag(.1, 4), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 200, N_smooth = 500, N_first = 1000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 200, smoother = "Fearnhead_O_N",
    # we add some extra variation to the proposal distributions
    Q_tilde = diag(.2^2, 4), n_threads = n_threads)))
```

`system.time` is used to show the computation time. The estimates are

```{r show_ests}
pf_Fear$Q
chol(pf_Fear$Q)
chol(Q) # the true values

pf_Fear$F
pf_Fear$fixed_effects # beta
```

We can plot the smoothed state estimates as follows

```{r plot_smooth, fig.width = 10, fig.height = 6.66}
tmp <- t(t(alphas) - beta[-1])[-1, ]
par(mfcol = c(2, 2), mar = c(5, 4, 1, 1))
for(i in 1:4){
  plot(pf_Fear, qlvls = c(0.025, 0.975), ylim = range(tmp),
       cov_index = i, col = i)
  points(1:nrow(tmp), tmp[, i], pch = 16, col = i)
}
```

The crosses are the point-wise confidence bounds, the lines are the smoothed 
means, and the full dots are the actual values. The approximate log-likelihood at 
each iteration of the EM algorithm are

```{r show_log_like}
plot(pf_Fear$log_likes, type = "l", ylab = "log-likelihood")
# last elements
plot(tail(pf_Fear$log_likes, 50), type = "l", ylab = "log-likelihood")
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("pf_restrict", "pf_Fear_restric", path = "examples/cache/restrictedVAR-cache/") 
-->

### Restricted model 
We fit the model where we only estimate $\vec{\theta}$ and $\vec{\psi}$ as 
follows

```{r pf_restrict, cache = 1, dependson = c("assign_sim_params", "confs")}
set.seed(30520116)
pf_Fear_restric <- PF_EM(
  Surv(tstart, tstop, y) ~ ddFixed_intercept() + x1 + x2 + x3 + x4 + 
    ddFixed(x1) + ddFixed(x2) + ddFixed(x3) + ddFixed(x4), 
  df, Q_0 = diag(1, 4), G = G, J = J, theta = c(.1, .1, 0), 
  psi = c(1, 1, 0), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 200, N_smooth = 500, N_first = 1000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 200, smoother = "Fearnhead_O_N",
    Q_tilde = diag(.2^2, 4), n_threads = n_threads))
```

Notice that we pass `G`, `J`, `theta`, and `psi` instead of `Q` and `Fmat`. 
The estimates are

```{r pf_Fear_restric_ests}
pf_Fear_restric$Q
chol(pf_Fear_restric$Q)

pf_Fear_restric$F
pf_Fear_restric$fixed_effects # beta
```

The smoothed state estimates looks as follows

```{r pf_Fear_restric_plot_smooth, fig.width = 10, fig.height = 6.66}
tmp <- t(t(alphas) - beta[-1])[-1, ]
par(mfcol = c(2, 2), mar = c(5, 4, 1, 1))
for(i in 1:4){
  plot(pf_Fear_restric, qlvls = c(0.025, 0.975), ylim = range(tmp),
       cov_index = i, col = i)
  points(1:nrow(tmp), tmp[, i], pch = 16, col = i)
}
```

The approximate log-likelihood at each iteration of the EM algorithm are

```{r pf_Fear_restric_show_log_like}
plot(pf_Fear_restric$log_likes, type = "l", ylab = "log-likelihood")
# last elements
plot(tail(pf_Fear_restric$log_likes, 50), type = "l", ylab = "log-likelihood")
```

We can get a better estimate of the log-likelihood at the final estimate for 
the two model by running the particle filter with more estimates as follows

```{r better_est_log_like_before, echo = FALSE}
old_dig <- getOption("digits")
options(digits = 7)
```

```{r better_est_log_like, cache = 1, dependson = c("pf_restrict", "pf_first")}
logLik(PF_forward_filter(pf_Fear        , N_fw = 10000, N_first = 10000)) 
logLik(PF_forward_filter(pf_Fear_restric, N_fw = 10000, N_first = 10000))
```

```{r better_est_log_like_after, echo = FALSE}
options(digits = old_dig)
```

We expect the former to have a higher log-likelihood since we have 
$4^2 + 4(4 + 1) / 2 - 2 \cdot 3 = 20$ more parameters in the former model. 

### Misspecified model
Let suppose that we think an AR(1) process is appropriate for each state variable 
and the state variables are independent. That is, we want to estimate 

$$
F = \begin{pmatrix}
    \theta_1 & . & . & . \\ 
    . & \theta_2 & . & . \\
    . & . & \theta_3 & . \\
    . & . & . & \theta_4 \end{pmatrix}, \quad
Q = \begin{pmatrix}
    \psi_1 & . & . & . \\
    . & \psi_2 & . & . \\
    . & . & \psi_3 & . \\ 
    . & . & . & \psi_4 \end{pmatrix}
$$
We can estimate such a model by making the following calls

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache("est_miss", "pf_Fear_miss", path = "examples/cache/restrictedVAR-cache/") 
-->

```{r est_miss, cache = 1, dependson = c("assign_sim_params", "confs")}
# setup G and J
G_miss <- matrix(0., 4^2, 4)
for(i in 0:3)
  G_miss[1 + i * 4 + i, i + 1] <- 1
J_miss <- matrix(0., 4 * (4 + 1) / 2, 4)
tmp <- 0
for(i in 4:1){
  J_miss[1 + tmp, 5 - i] <- 1
  tmp <- tmp + i
}

# estimate
set.seed(30520116)
pf_Fear_miss <- PF_EM(
  Surv(tstart, tstop, y) ~ ddFixed_intercept() + x1 + x2 + x3 + x4 + 
    ddFixed(x1) + ddFixed(x2) + ddFixed(x3) + ddFixed(x4), 
  df, Q_0 = diag(1, 4), G = G_miss, J = J_miss, 
  theta = rep(.1, 4), psi = rep(1, 4), 
  by = 1, type = "VAR", model = "exponential", max_T = n_periods,
  control = PF_control(
    N_fw_n_bw = 200, N_smooth = 500, N_first = 1000, eps = .001,
    method = "AUX_normal_approx_w_cloud_mean",
    n_max = 200, smoother = "Fearnhead_O_N",
    Q_tilde = diag(.2^2, 4), n_threads = n_threads))
```

The estimates from this model are

```{r pf_miss_ests}
pf_Fear_miss$Q
pf_Fear_miss$F
pf_Fear_miss$fixed_effects
```

An approximation of the log-likelihood is

```{r pf_miss_log_likeli_before, echo = FALSE}
old_dig <- getOption("digits")
options(digits = 7)
```

```{r pf_miss_log_likeli, cache = 1, dependson = "est_miss"}
logLik(PF_forward_filter(pf_Fear_miss, N_fw = 10000, N_first = 10000)) 
```

```{r pf_miss_log_likeli_after, echo = FALSE}
options(digits = old_dig)
```


# References
