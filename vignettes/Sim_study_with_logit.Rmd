---
title: "Simulation study with logit"
author: "Benjamin Christoffersen"
date: "11 November 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE)
```

# Intro 
This note has three objectives. The note is not meant to be self contained. It is recommend to see (TODO: Insert reference) for the models in the `dynamichazard` package. The first objective is to test how the `ddhazard` fits compare with a Generalized Additive models and a "static" logistic model with simulated data. We will look at the following models/estimation methods from `ddhazard` function in the `dynamichazard` package:

* Fits with the Extended Kalman Filters (EKF) with different learning rates and with and without extra iterations in the scoring step
* Second order random walks with the EKF estimation method
* Mixture of fixed and time varying effects with the EKF estimation method
* Fits using the Unscented Kalman filter (UKF)

The second objective is to show how to estimate various models with the function `ddhazard`. For this reason, the note contains quite a bit of intermediate `R`. The headers marked with no \* are showing results of simulation or contains important comments. Meanwhile headers with an \* and \*\* shows increasingly less important code to understand the simulation. Consequently, you can skip to the header with no \* if you are only interested in the results

The final third objective is to show that both the EKF and UKF scales linearly with the number individuals (series). All method with use the logistic link function. We will do three runs of experiments in the following order:

1. A Model where all effects are time varying and we use the correct binning intervals
2. A model where only one parameter is time varying and  we use the correct binning intervals
3. A Model where all effects are time varying but we use incorrect binning intervals

The models will be compared in terms of Brier score, median absolute residuals and standard deviation of the absolute residuals. All metrics will be reported on out sample data. All plots will have true coefficients as continuous lines while dashed lines are estimates

You can install the version of the code used to compile this package by using the code below. Moreover, you will also find the source code for the vignette in the github package under the same version:

```{r, eval = FALSE}
# TODO: Make sure to update
devtools::install_github(
  "boennecd/dynamichazard@081323df8af85e7c26eaad449b720ccbfbaebb35")
```

## Notions
For clarity, here is a list of notions used:

* Run: An peppermint with one three previously specified settings where we make $k$ simulation with $n$ series in each
* Simulation: One simulation within a run with one set of coefficients $\vec{\beta}_0,\dots,\vec{\beta}_d$ and given number $n$ of series
* Series/individuals: A person/individual starting at time $0$ and either making it to the end of the time in the given simulation or dying at some time during the period
* Coefficients: the entries of the vectors $\vec{\beta}_t$ in a given simulation
* Covariates: vectors $\vec{x}_{it}$ for a given individual at a given time in simulation

## Findings 
The findings are: 

* The UKF method seems to perform well for both small and larger number of series
* Decreasing the learning rates in the correction step makes it more likely that a the EKF model succeeds in estimating the model
* Taking multiple iterations in the correction step of the EKF seems to be beneficial with moderate amount of series
* Miss specifying a fixed effect as time varying or setting the binning number incorrectly has little effect on the result
* The UKF and/or the EKF method with extra iteration in the correction step perform close to a Generalized Additive model in terms of out sample Brier score

It is worth stressing that it is my experience that you can always do "trail-and-error" with the initial covariance matrix, the covariance matrix at time zero and tuning parameters in order to get a model to fit a given dataset. This is disadvantages that any given data set may require some tuning by the user. Although as will be shown, it should not be required with tuning by the user in cases like the one presented here

## Setup
The following values will be used in the simulation:
```{r}
ns <- c(200, 800, 2000)
n_beta <- 5
T_max <- 10
n_sims <- 2

gsub("(^.+)(/dynamichazard.+$)", "...\\2", getwd())
source("../R/test_utils.R")
```

`ns` is the number of series (individuals) we will estimate in each of the simulation in each of the runs. Thus, we will perform simulations with a total of `r paste(ns[-length(ns)], collapse = ", ")` and `r tail(ns, 1)` series in each. Each simulation will have `n_beta =` `r n_beta` covariates plus an intercept. Each run will simulate `n_sims =` `r n_sims` times. Finally, we source the `test.utils.R` file to define the simulation function. `T_max` is the number of bins/intervals we will observe. Thus, we have 1, 2, ..., `T_max` + 1 covariate vectors (+1 for the time zero coefficient vector) 

# Fitting true model
We will make runs for various number of individuals in this section where we estimate a model where all effects are time varying and we use the correct binning intervals. Thus, the only  models that are miss specified are the one with `x2` being the only time varying parameter and the model where we fit are second order random walk

## Definition of simulation function
Below, we define a list of `default_args` to our simulation function which we can later use using `do.call`. 
```{r est_true_setup}
# Default arguments for simulation
default_args <- list(
  n_vars = n_beta, # Number of betas not including intercept
  beta_start = c(-1, -.5, 0, 1.5, 2), # start value of coeffecients
  intercept_start = -4, # start value of intercept
  sds = c(.1, rep(1, n_beta)), # std. deviations in state equation
  t_max = T_max, # Largest time we observe
  x_range = 1, # range of covariates
  x_mean = .5) # mean of covariates

```

Let $\vec{\beta}_t$ denote the time varying coefficient. Then the `beta_start` is the time 0 values of the coefficients and `intercept_start` is the starting value of the intercept. The `sds` is the standard deviations, $\sigma_i$, in the state equation. Hence, 
$$\beta_{j,t} = \beta_{j,t - 1} + \epsilon_{j,t},\qquad \epsilon_{j,t} \sim N(0,\sigma_j)$$

The `x_mean` and `x_range` defines how the covariate values are simulated. The above setting implies that $x_{itj} = \text{Unif}(0.5 - 1/2, 0.5 + 1/2)$ where $x_{itj}$ is the $i$'th individuals covariate $j$ at time $t$. The covariates vector $\vec{x}_{it}$ is updated with at times $1+\eta$ 
 where $\eta \sim \text{Pois}(1)$ and $\eta$s are drawn separately for each individual. The motivation for this behavior is that we can have different covariate update times than our binning time in a given study. For instance, say we are looking at a medical study and the covariates are laboratory values. The time of laboratory values from an individual's visit the doctor can differ with what ever binning periods we use in the state space model. Further, the time when laboratory values are updated can differ between patients. One might see his doctor every week or so while another only see his doctor every year
 
Below we illustrate how the covariate vector from a simulation can look:
```{r}
# We can simulate by
sims <- do.call(test_sim_func_logit, c(list(n_series = max(ns)), default_args))

# This is how the state vectors look
# We define a function so we can re-use it later
(plot_func <- function(ylim = c()) # we define a function here so we can use it later
  matplot(sims$betas, type = "l", lty = 1, ylab = "time", xlab = "beta",
          ylim = range(sims$betas, ylim), col = 1:(n_beta + 1)))()

```

Next, we can look at the number of failures in each simulation:
```{r}
# We get a decent amount of failures and survivers in some of the simulations
# We use do.call to avoid repeating the above argument list
# See ?do.call
set.seed(201605)
n_fails_in_sim <- rep(NA_real_, 15)
for(i in seq_along(n_fails_in_sim)){
  sims <- do.call(test_sim_func_logit, c(
    default_args, c(list(n_series = max(ns))))) # Take largest amount of series
  n_fails_in_sim[i] <- sum(sims$res$event)
}
n_fails_in_sim # number of failures in each simulation of max(ns) series
```

## \* Defintion of fit functions
We will define functions to estimate the different models given a data frame from a `test_sim_func_logit` as the first argument. This will reduce the amount of code later

### \*\* Defintion of static fit
Below, we define function to fit a model where the coefficients are fixed ($\vec{\beta}_t = \vec{\beta}$). It is estimated using `glm` 
```{r est_true_fit_funcs_static}
library(survival); library(dynamichazard)

# Set up function for static fit
fit_funcs = list()
fit_funcs$static <- function(s = sims$res)
  static_glm(formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
             data = s, max_T = T_max, by = 1, id = s$id) 

fit <- fit_funcs$static()
class(fit) # returns a glm object

# Estimates match simulations
plot_func(ylim = fit$coefficients)
abline(h = fit$coefficients, col = 1:(n_beta + 1), lty = 2)
```

### \* Defintion of `ddhazard` fit functions
Below, we define a function to simulate a first order random walk with a given learning rate and potential extra iterations in the scoring step (TODO: Insert reference to other vignette):
```{r est_true_fit_ddhazrd_out_of_box}
library(survival); library(dynamichazard)

# Set up function ddhazard fit function for convenience
# LR:       learning rate in correction step
# NR_eps:   tolerance in correction step. NULL yields no extra loops
fit_funcs$dd <- function(s = sims$res, LR = 1, NR_eps = NULL)
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = s, max_T = T_max, by = 1, id = s$id, 
      Q_0 = diag(1, n_beta + 1), Q = diag(.01, n_beta + 1),
      control = list(LR = LR, NR_eps = NR_eps, eps = 0.1))
  }, error = function(...) NA) # Return NA if fails

fit <- fit_funcs$dd(LR = .5)

# Plot estimates and actual coffecients
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs, col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)

# Same call with extra iterations
fit <- fit_funcs$dd(LR = .5, NR_eps = .01)

# Look at new plot
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs, col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)
```

Below, we define a function to simulate a first order random walk with the UKF method:
```{r}
# Fitting with UKF
fit_funcs$dd_UKF <- function(s = sims$res, alpha = 1, beta = 0){
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = s, max_T = T_max, by = 1, id = s$id, 
      Q_0 = diag(1, n_beta + 1), Q = diag(.01, n_beta + 1),
      control = list(
        eps = 0.1,
        alpha = alpha,   # Set tuning parameter
        beta = beta,        # Set tuning parameter
        method = "UKF")) # Set estimation method (EKF is default)
  }, error = function(...) NA) # Return NA if fails
}

fit <- fit_funcs$dd_UKF()

# Look at new plot
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs, col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)
```

Below, we define a function to simulate a first order random walk where only one parameter (`x2`) is time varying:
```{r}
# Fitting with fixed effects
fit_funcs$dd_fixed <- function(s = sims$res, LR = 1, NR_eps = NULL){
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ 
        -1 + ddFixed(rep(1, length(x1))) +       # Fix intercept
        ddFixed(x1) + x2 +                       # Note x2 is time variying
        ddFixed(x3) + ddFixed(x4) + ddFixed(x5),
             data = s, max_T = T_max, by = 1, id = s$id, 
             Q_0 = diag(1, 1), Q = diag(.01, 1),
             control = list(LR = LR, NR_eps = NR_eps, eps = 0.1))
  }, error = function(...) NA) # Return NA if fails
}

fit <- fit_funcs$dd_fixed()

# Look at new plot
plot_func(ylim = range(fit$state_vecs, fit$fixed_effects))
matplot(fit$state_vecs, col = 3, lty = 2, 
        type = "l", add = T)
abline(h = fit$fixed_effects, col = c(1:2, 4:6), lty = 2)
```

Next, we define a function to fit the model with a second order random walk:
```{r}
# Fitting with second order
fit_funcs$dd_2_order <- function(s = sims$res, LR = 1, NR_eps = NULL){
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = s, max_T = T_max, by = 1, id = s$id, 
      # Q_0 and Q needs more elements 
      Q_0 = diag(c(rep(1, n_beta + 1), rep(0.5, n_beta + 1))), 
      Q = diag(c(rep(.01, n_beta + 1), rep(0, n_beta + 1))),
      order = 2, # specify the order
      control = list(LR = LR, NR_eps = NR_eps, eps = 0.1))
  }, error = function(...) NA) # Return NA if fails
}

fit <- fit_funcs$dd_2_order(LR = .75)

# Look at new plot
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs[, 1:6], col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)
```

## \*\* Defintion of GAM fit function
We define the estimation method for the Generalized additive model in the next code snippet. We use `bam` function from the `mgcv` package which corresponds to `gam` but for very large datasets

```{r est_true_fit_mgcv}
library(mgcv)
fit_funcs$gam <- function(s = sims$res){
  # get data frame for fitting
  dat_frame <- get_survival_case_weigths_and_data(
    formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
               data = s, max_T = T_max, by = 1, id = s$id, use_weights = F)
  # fit model
  bam(
    formula = Y ~ 
      # cr is cubic basis with dim of k
      s(t, bs = "cr", k = 10, by = x1) + 
      s(t, bs = "cr", k = 10, by = x2) + 
      s(t, bs = "cr", k = 10, by = x3) + 
      s(t, bs = "cr", k = 10, by = x4) + 
      s(t, bs = "cr", k = 10, by = x5),
  family = binomial, data = dat_frame,
  method = "GCV.Cp")
}

# fit model
fit <- fit_funcs$gam()

# Compare plot
layout(matrix(1:6, nrow = 2))
for(i in 1:n_beta){
  plot(fit, pages = 0, rug = F, col = i + 1, select = i, lty = 2)
  lines(sims$betas[-1, i + 1], col = i + 1)
}
```

## \*\* Definition of prediction functions
The following code snippets define predictions methods for each of the estimation methods. We start off by defining a split function such that we can sample individuals (series) into a test set and a training test:
```{r est_true_pred_funcs_static}
split_func <- function(s = sims$res){
  # Sample ids
  test_ids <- sample(
    unique(s$id), floor(length(unique(s$id)) / 2), replace = F) 
  
  # Return seperate data frames
  return(list(test_dat = s[s$id %in% test_ids, ],
              fit_dat = s[!s$id %in% test_ids, ]))
}

# Illustrate use
tmp <- split_func()
# No ids intersect in the two sets
length(intersect(tmp$test_dat$id, tmp$fit_dat$id))
# The union is exactly the number of ids we simulated
length(union(tmp$test_dat$id, tmp$fit_dat$id))
```

Having define the splitting method, we turn the prediction methods. The idea is to define the `brier_funcs$general` function which takes in a prediction function, a fit and a data frame. Next, we then define individual prediction methods which will be passed to `brier_funcs$general`:

```{r}
# Define prediction function for static model
brier_funcs <- list() 
brier_funcs$general <- function(brier_func, fit, eval_data_frame){
  d_frame <- get_survival_case_weigths_and_data(
    formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
    data = eval_data_frame, max_T = T_max, by = 1, id = eval_data_frame$id, 
    use_weights = F)
  
  # Change start and stop times
  d_frame$tstart <- d_frame$t - 1
  d_frame$tstop <- d_frame$t
  
  # Compute residuals
  resids <- brier_func(fit, d_frame)

  # Return estimates
  list(brier = mean.default(resids^2),
       median_abs_res = median(abs(resids)),
       sd_res = sd(resids),
       # Info is added to debug
       info = cbind(resids = resids,
                    tstop = d_frame$tstop, 
                    id = d_frame$id))
}

# Prediction method for static model
brier_funcs$static <- function(fit, d_frame){
  preds <- predict(fit, newdata = d_frame, type = "response")
  return(d_frame$Y - preds)
}

# Test function
fit <- fit_funcs$static(tmp$fit_dat)
unlist(
  brier_funcs$general(brier_funcs$static, fit, tmp$fit_dat)[
    c("brier", "median_abs_res", "sd_res")]) # in sample stats
unlist(
  brier_funcs$general(brier_funcs$static, fit, tmp$test_dat)[
    c("brier", "median_abs_res", "sd_res")]) # out sample stats
```

```{r est_true_pred_ddhazard}
# Define prediction function for ddhazard model
brier_funcs$dd <- function(fit, d_frame){
  preds <- predict(fit, new_data = d_frame, tstart = "tstart", tstop = "tstop")
  return(d_frame$Y - preds$fits)
}

fit <- fit_funcs$dd(tmp$fit_dat, LR = .5)

unlist(
  brier_funcs$general(brier_funcs$dd, fit, tmp$fit_dat)[
    c("brier", "median_abs_res", "sd_res")]) # in sample stats
unlist(
  brier_funcs$general(brier_funcs$dd, fit, tmp$test_dat)[
    c("brier", "median_abs_res", "sd_res")]) # out sample stats
```

```{r est_true_pred_gam}
# Define prediction function for gam model
brier_funcs$gam <- function(fit, d_frame){
  preds <- predict(fit, newdata = d_frame, type = "response")
  return(d_frame$Y - preds)
}

fit <- fit_funcs$gam(tmp$fit_dat)
unlist(
  brier_funcs$general(brier_funcs$gam, fit, tmp$fit_dat)[
    c("brier", "median_abs_res", "sd_res")]) # in sample stats
unlist(
  brier_funcs$general(brier_funcs$gam, fit, tmp$test_dat)[
    c("brier", "median_abs_res", "sd_res")]) # out sample stats
```

## \*\* Definition of multiple simulations function
To make things easier, we define a function that takes in a function to simulate. Given a function to simulate, then the new function perform `n_sims =` `r n_sims` simulation for each of the runs values `ns` (`r ns`): 
```{r simulations_def, warning=FALSE}
simulate_n_print_res <- function(
  sim_func, # Function that takes one argment which is number of series
  LRs = c(.1 , .5, 1, .5, 1),           # Learning rates
  NR_eps = c(rep(.01, 3), NA, NA)) # Tolerance in scoring step
  {
  for(n in ns){
    out <- array(NA_real_, dim = c(n_sims, 5 + length(LRs), 3),
                 dimnames = list(
                   NULL,
                   c("static", 
                     paste("dd(", format(LRs, width = 3), ", ", 
                           format(NR_eps, width = 4), ")", sep = ""),
                     "2 order EKF", "Fixed effect", "UKF", "gam"),
                   c("Brier", "Median abs res", "sd res")))
    
    n_failures_and_surviers <- array(
      NA_integer_, dim = c(2, n_sims), 
      dimnames = list(c("# failures", "# survivers"), NULL))
    
    #*******
    # Progress bar for inpatient people (me)
    pb <- tcltk::tkProgressBar(paste("Estimating with n =", n), "",
							0, n_sims, 50)
    #*******
    
    for(i in 1:n_sims){
      #*******  
      info <- sprintf("%d%% done", 100 * (i - 1) / n_sims)
      tcltk::setTkProgressBar(pb, i - 1, paste("Estimating with n =", n), info)
      #*******
      
      # Sample until we get an outcome have suffecient amount of deaths and 
      # failures
      repeat{
        sims <- sim_func(n)
        
        # We want some survivers and some deaths
        if(sum(sims$res$event) > 40 && n - sum(sims$res$event) > 40)
          break
      }
      
      n_failures_and_surviers["# failures", i] <- sum(sims$res$event)
      n_failures_and_surviers["# survivers", i] <- n - sum(sims$res$event)
      
      # Split data
      sim_split <- split_func(sims$res)
      
      # Fit static model
      static_fit <- fit_funcs$static(sim_split$fit_dat)
      
      # Fit dd model
      dd_fits <- list(rep(NA, length(LRs)))
      for(k in seq_along(LRs)){
        dd_fits[[k]] <- fit_funcs$dd(
          sim_split$fit_dat, LR = LRs[k], 
          NR_eps = if(is.na(NR_eps[k])) NULL else NR_eps[k])
      }
      
      # Fit second order
      dd_2_order <- fit_funcs$dd_2_order(sim_split$fit_dat)
      
      # Fit fixed effect
      dd_fixed <- fit_funcs$dd_fixed(sim_split$fit_dat)
      
      # UKF fit 
      dd_UKF <- fit_funcs$dd_UKF(sim_split$fit_dat)
      
      # Fit gam model
      gam_fit <- fit_funcs$gam(sim_split$fit_dat)
      
      # Evalute on test data
      models <- c(list(static_fit), dd_fits, 
                  list(dd_2_order, dd_fixed, dd_UKF, gam_fit))
      
      eval_funcs = c(brier_funcs$static,
                     replicate(length(dd_fits) + 3, brier_funcs$dd),
                     brier_funcs$gam)
                     
      for(j in seq_along(models)){
        if(length(models[[j]]) == 1 && is.na(models[[j]]))
          next # We have to skip models fits that failed
        
        metrics <- brier_funcs$general(
          eval_funcs[[j]], models[[j]], sim_split$test_dat)
        out[i, j, "Brier"] <- metrics$brier
        out[i, j, "Median abs res"] <- metrics$median_abs_res
        out[i, j, "sd res"] <- metrics$sd_res
      }
    }
    
    #*******  
    close(pb)
    #*******  
    
    # Print results
    cat(paste("##############\nNumber of series in fit and test data are", 
              n/2, "\n"))
    
    cat("Number of failed fits:\n")
    print(apply(out[, , 1], 2, function(x) sum(is.na(x))))
    
    cat("\nNumber of failures and survivers in simulation and whether a given model\n",
        "failed to fit. We only print the first 10 simulation. 1 implies that the model\n",
        "or estimation method did not succed in fitting\n", sep = "")
    n_nas <- rbind(n_failures_and_surviers, 
                   t(is.na(out[, , 1])))[, seq_len(min(10, n_sims))]
    print(ifelse(n_nas ==0 , NA, n_nas), digits = 4, na.print = "")
    
    cat("\nMean of metrics:\n")
    print(apply(out, 3, colMeans, na.rm = T), digits = 4)
    
    cat("\nNumber of complete cases where all succeeded to fit", 
        n_cases_all_success <- sum(complete.cases(out[, , 1])))
    if(n_cases_all_success > 0){
      cat("\nMean of metrics where all succeeded to fit:\n")
      print(apply(out[complete.cases(out[, , 1]), , , drop = F], 3, colMeans), 
            digits = 4)
      cat("\nSds of metrics where all succeeded to fit:\n")
      print(apply(out[complete.cases(out[, , 1]), , , drop = F], c(2,3), sd), 
            digits = 4)
    }
    
    if(n != tail(ns, 1))
      cat("\n\n")
  }
}
```

## Simulating

We are now able to simulate from the model where all effects are time varying and we use the correct binning intervals with the code below. `dd (x, y)` stand for a `ddhazard` fit with a learning rate of `x` and a tolerance in the correction step of `y`. If `y` is `NA` then we have only used a single iteration
```{r simulations_all_variying, warning=FALSE}
set.seed(1243)
# Use simulation function
simulate_n_print_res(
  sim_func = function(n)
    do.call(test_sim_func_logit, c(default_args, c(list(n_series = n)))))
```

We should only compare across methods with mean metrics `where all succeeded to fit`. The logic being that those where the `ddhazard` method fail may be "hard".

## Conclussion on run
We find that the UKF method tends to succeed  with these setting in fitting the data. Moreover, decreasing the learning rate in the EKF makes it more likely to succeed in fitting the model. All models perform better than the static model apart from the case where the number of individuals in the training data is only `r min(ns)/2`

The miss specified models (`2 order EKF` and `Fixed effect`) tend to perform worse than the others models. Though, they still perform better than the static model labeled `static`

There does not seem to be a pattern between the ratio of failures to survives and whether a given model or method fails to fit

The UKF performs close to the gam model when the training data has less than `r max(ns)/2` observations while taking extra iterations in the EKF seems to be worth it in terms of out sample Brier score when the training data has more than `r max(ns)/2` observations. This may suggest that the UKF method is better for smaller data sets while the EKF with extra iterations in the scoring step is better suited for larger data sets. In all cases, at least one of models is close to the `gam` model in terms of out sample Brier score 

# Single time variying parameter
In this part, we will look at the performs when only a single coefficient (`x2`) varies. Thus, we can see if the model where only (`x2`) varies performs performs better. Or in other words how the miss specified models behaves

## \*\* Definition of simulation function
We start by defining the simulation function. The main change here are that we only set a single variance and that we set it larger than before:
```{r , warning=FALSE}
# Use simulation function
set.seed(9999)
sim_one_variying <- function(n){
  test_sim_func_logit(
    n_series = n, 
    sds = c(sqrt(3)), # Large variance
    is_fixed = c(1:2, 4:6), # All but parem three is fixed
    
    # Same values as before
    n_vars = n_beta, 
    beta_start = c(-1, -.5, 0, 1.5, 2), 
    intercept_start = -4, 
    t_max = T_max,
    x_range = 1, 
    x_mean = .5)
}
```

## \* Illustration of single simulation
```{r}
# We get a more variable number of failures and survivers (we simulate 200 
# series)
replicate(10, sum(sim_one_variying(200)$res$event)) # print # of failures

# Here is an example of a series
tmp <- sim_one_variying(200)
matplot(tmp$betas, type = "l", lty = 1, ylab = "Beta", xlab = "Time")
```

## Simulating
We can simulate with the following call:
```{r simulations_one_variying, warning=FALSE}
# Use simulation function
set.seed(8080)
simulate_n_print_res(sim_func = sim_one_variying)
```

## Conclussion on run
The main interest here is how the correctly specified model labeled `Fixed effect` performs relative to the other models. It seems to make a minor difference in terms of out sample Brier score for all the settings. This may suggest that incorrectly specifying an effects as time varying does not affect the result

# Incorrect binning time
Now, what happens if we get the binning wrong? This is the next experiment we will perform. Specifically, we will set the binning length to `0.1` instead `1` when we simulate. 

## \*\* Definition of simulation function
```{r}
set.seed(9001)
sim_finer_binning <- function(n){
  time_denom = 10 # how much finer do we want to bin?
  
  res <- test_sim_func_logit(
    n_series = n, 
  
    # We multiply through appropiately
    beta_start = c(-1, -.5, 0, 1.5, 2),
    intercept_start = - 8, # Note, we changed the intercept
    sds = c(.1, rep(1, n_beta)) / sqrt(time_denom),
    t_max = T_max * time_denom,
    lambda = 1 / time_denom, # note we change the time when covariates are 
                             # updated (the lambda parem in the rate ~ exp(.) 
                             # in the time between increaments)
    
    n_vars = n_beta,
    x_range = 1,
    x_mean = .5)
  
  # Change time denominator
  res$res$tstart <- res$res$tstart / time_denom
  res$res$tstop <- res$res$tstop / time_denom
  
  res
}
```

## \* Illustration of single simulation
```{r}
# We get more variable outcomes (we simulate 200 series)
replicate(10, sum(sim_finer_binning(200)$res$event)) # Number of failures

# Here is an example of the series
tmp <- sim_finer_binning(200)
matplot(tmp$betas, type = "l", lty = 1, ylab = "Beta", xlab = "Time")
```

## Simulating
We are now able to simulate with the following call:
```{r simulations_sim_finer_binning, warning=FALSE}
# Use simulation function
set.seed(747)
simulate_n_print_res(sim_func = sim_finer_binning)
```

## Conclussion on run
The UKF seems to perform well in all settings. Moreover, the extra iteration seems to be worth it when there are a moderate amount of observations. The miss specified `Fixed effect` seems to perform worse than the other fit/estimates. Finally, the mean Brier score is again not to fare off from the `gam` fit for the model/method with the best result

# Linear Time complexity
We will illustrate that the EKF and UKF have linear time complexity in the number of observation. This is particularly easy because the simulation function start of by simulating the coefficients as shown below:

```{r}
some_seed <- 69284
set.seed(some_seed)
res_1 <- test_sim_func_logit(100)

set.seed(some_seed)
res_2 <- test_sim_func_logit(1000) # different number of series

all.equal(res_1$betas, res_2$betas)
```

```{r}
# Define function to record run time for a given number of series
run_time_func <- function(n, sim_args = default_args){
  set.seed(7851348)
  sim_args$n_series <- n
  sims <- do.call(test_sim_func_logit, sim_args)
  
  print(sum(sims$res$event) / n)
  print(n)
  
  time_EKF <- system.time(fit_EKF <- fit_funcs$dd(sims$res))
  time_UKF <- system.time(
    fit_UKF <- ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = sims$res, max_T = T_max, by = 1, id = sims$res$id, 
      Q_0 = diag(.1, n_beta + 1), Q = diag(.1, n_beta + 1),
      control = list(
        eps = 0.1,
        alpha = 1,   
        beta = 0,        
        method = "UKF")))
                            
  
  if(is.na(fit_EKF) || is.na(fit_UKF))
    stop()
  
  list(time_EKF = time_EKF, time_UKF = time_UKF)
}

n_for_test <- 2^(10:19)
run_time <- sapply(n_for_test, run_time_func)

# Plot EKF and print log-log regression slope
ekf_time <- sapply(run_time["time_EKF", ], function(x) x[["user.self"]])
plot(n_for_test, ekf_time, type = "p", log = "xy", 
     xlab = "Number of series", ylab = "Computation time for EKF") 

coef(lm(log(ekf_time) ~ log(n_for_test))) # log-log slope is roughly one

# Plot UKF and print log-log regression slope
ukf_time <- sapply(run_time["time_UKF", ], function(x) x[["user.self"]])
plot(n_for_test, ukf_time, type = "p", log = "xy", 
     xlab = "Number of series", ylab = "Computation time for UKF") 

coef(lm(log(ukf_time) ~ log(n_for_test))) # log-log slope is roughly one
```
