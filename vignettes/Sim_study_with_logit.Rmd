---
title: "Simulation study with logit"
author: "Benjamin Christoffersen"
date: "11 November 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE)
```

# Intro 
This note has three objectives. The note is not ment to be self contained. It is recommend to see (TODO: Insert reference) for the models in the `dynamichazard` package. The first objective is to test how the `ddhazard` fits compare with a Generalized Additive models and a "static" logistic model with simulated data. We will look at the following models/estimation methods from `ddhazard` function in the `dynamichazard` package:

* Fits with the Extended Kalman Filters (EKF) with different learning rates and with and wihtout extra iterations in the scoring step
* Second order random walks with the EKF estimation method
* Mixture of fixed and time variying effects with the EKF estimation method
* Fits using the Uncented Kalman filter (UKF)

The second objective is to show how to estimate various models with the function `ddhazard`. For this reason, the note contains quite a bit of interemediate `R`. The headers marked with no \* are showing results of simulation or contains important comments. Meanwhile headers with an \* and \*\* shows increasingly less important code to understand the simulation. Consquently, you can skip to the header with no \* if you are only interested in the results

The final third objective is to show that both the EKF and UKF scales linearly with the number inviduals (series). All method with use the logistic link function. We will do three runs of experiments in the following order:

1. A Model where all effects are time variying and we use the correct binning intervals
2. A model where only one parameter is time variying and  we use the correct binning intervals
3. A Model where all effects are time variying but we use incorrect binning intervals

The models will be compared in terms of Brier score, median absolute residuals and standard deviation of the absolute residuals. All metrics will be reported on out sample data. All plots will have true coeffecients as continous lines while dashed lines are estimates

You can install the version of the code used to compile this package by using the code below. Moreover, you will also find the source code for the vignette in the github package under the same version:

```{r, eval = FALSE}
# TODO: Make sure to update
devtools::install_github(
  "boennecd/dynamichazard@081323df8af85e7c26eaad449b720ccbfbaebb35")
```

## Notions
For clearity, here is a list of notions used:

* Run: An expermint with one three previously specified settings where we make $k$ simulation with $n$ series in each
* Simulation: One simulation within a run with one set of coeffecients $\vec{\beta}_0,\dots,\vec{\beta}_d$ and given number $n$ of series
* Series/individuals: A person/individual starting at time $0$ and either making it to the end of the time in the given simulation or dying at some time during the period
* Coeffecients: the entries of the vectors $\vec{\beta}_t$ in a given simulation
* Covariates: vectors $\vec{x}_{it}$ for a given individual at a given time in simulation

## Findings 
TODO: write me!

## Setup
The following values will be used in the simulation:
```{r}
ns <- 2000 # c(200, 800, 2000)
n_beta <- 5
T_max <- 10
n_sims <- 20 # 100

gsub("(^.+)(/dynamichazard.+$)", "...\\2", getwd())
source("../R/test_utils.R")
```

`ns` is the number of series (indviduals) we will estimate in each of the simulation in each of the runs. Thus, we will perform simulations with a total of `r paste(ns[-length(ns)], collapse = ", ")` and `r tail(ns, 1)` series in each. Each simulation will have `n_beta =` `r n_beta` covariates plus an intercept. Each run will simulate `n_sims =` `r n_sims` times. Finally, we source the `test.utils.R` file to define the simulation function. `T_max` is the number of bins/intervals we will observe. Thus, we have 1, 2, ..., `T_max` + 1 covariate vectors (+1 for the time zero coeffecient vector) 

# Fitting true model
## Definition of simulation function
Below, we define a list of `default_args` to our simulation function which we can later use using `do.call`. 
```{r est_true_setup}
# Default arguments for simulation
default_args <- list(
  n_vars = n_beta, # Number of betas not including intercept
  beta_start = c(-1, -.5, 0, 1.5, 2), # start value of coeffecients
  intercept_start = -4, # start value of intercept
  sds = c(.1, rep(1, n_beta)), # std. deviations in state equation
  t_max = T_max, # Largest time we observe
  x_range = 1, # range of covariates
  x_mean = .5) # mean of covariates

```

Let $\vec{\beta}_t$ denote the time variying coeffecient. Then the `beta_start` is the time 0 values of the coffecients and `intercept_start` is the starting value of the intercept. The `sds` is the standard deviations, $\sigma_i$, in the state equation. Hence, 
$$\beta_{j,t} = \beta_{j,t - 1} + \epsilon_{j,t},\qquad \epsilon_{j,t} \sim N(0,\sigma_j)$$

The `x_mean` and `x_range` defines how the covariate values are simulated. The above setting implies that $x_{itj} = \text{Unif}(0.5 - 1/2, 0.5 + 1/2)$ where $x_{itj}$ is the $i$'th indviduals covariate $j$ at time $t$. The covaraites vector $\vec{x}_{it}$ is updated with at times $1+\eta$ 
 where $\eta \sim \text{Pois}(1)$ and $\eta$s are drawn seperatly for each indvidual. The motivation for this behavior is that we can have different covariate update times than our binning time in a given study. For instance, say we are looking at a medical study and the covariates are labratory values. The time of labratory values from an indvidual's visit the doctor can differ with what ever binning periods we use in the state space model. Further, the time when labratory values are updated can differ between patients. One might see his doctor every week or so while another only see his doctor every year
 
Below we illustrate how the covariate vector from a simulation can look:
```{r}
# We can simulate by
sims <- do.call(test_sim_func_logit, c(list(n_series = max(ns)), default_args))

# This is how the state vectors look
# We define a function so we can re-use it later
(plot_func <- function(ylim = c()) # we define a function here so we can use it later
  matplot(sims$betas, type = "l", lty = 1, ylab = "time", xlab = "beta",
          ylim = range(sims$betas, ylim), col = 1:(n_beta + 1)))()

```

Next, we can look at the number of failures in each simulation:
```{r}
# We get a decent amount of failures and survivers in some of the simulations
# We use do.call to avoid repeating the above argument list
# See ?do.call
set.seed(2016)
n_fails_in_sim <- rep(NA_real_, 15)
for(i in seq_along(n_fails_in_sim)){
  sims <- do.call(test_sim_func_logit, c(
    default_args, c(list(n_series = max(ns))))) # Take largest amount of series
  n_fails_in_sim[i] <- sum(sims$res$event)
}
n_fails_in_sim # number of failures in each simulation of max(ns) series
```

## \* Defintion of fit functions
We will define funtions to estimate the different models given a data frame from a `test_sim_func_logit` as the first argument. This will reduce the amount of code later

### \*\* Defintion of static fit
Below, we define function to fit a model where the coeffecients are fixed ($\vec{\beta}_t = \vec{\beta}$). It is estimated using `glm` 
```{r est_true_fit_funcs_static}
library(survival); library(dynamichazard)

# Set up function for static fit
fit_funcs = list()
fit_funcs$static <- function(s = sims$res)
  static_glm(formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
             data = s, max_T = T_max, by = 1, id = s$id) 

fit <- fit_funcs$static()
class(fit) # returns a glm object

# Estimates match simulations
plot_func(ylim = fit$coefficients)
abline(h = fit$coefficients, col = 1:(n_beta + 1), lty = 2)
```

### \* Defintion of `ddhazard` fit functions
Below, we define a function to simulate a first order random walk with a given learning rate and potential extra iterations in the scoring step (TODO: Insert reference to other vignette):
```{r est_true_fit_ddhazrd_out_of_box}
library(survival); library(dynamichazard)

# Set up function ddhazard fit function for convenience
# LR:       learning rate in correction step
# NR_eps:   tolerance in correction step. NULL yields no extra loops
fit_funcs$dd <- function(s = sims$res, LR = 1, NR_eps = NULL)
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = s, max_T = T_max, by = 1, id = s$id, 
      Q_0 = diag(1, n_beta + 1), Q = diag(.01, n_beta + 1),
      control = list(LR = LR, NR_eps = NR_eps))
  }, error = function(...) NA) # Return NA if fails

fit <- fit_funcs$dd(LR = .5)

# Plot estimates and actual coffecients
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs, col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)

# Same call with extra iterations
fit <- fit_funcs$dd(LR = .5, NR_eps = .01)

# Look at new plot
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs, col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)
```

Below, we define a function to simulate a first order random walk with the UKF method:
```{r}
# Fitting with UKF
fit_funcs$dd_UKF <- function(s = sims$res, alpha = 1){
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = s, max_T = T_max, by = 1, id = s$id, 
      Q_0 = diag(1, n_beta + 1), Q = diag(.01, n_beta + 1),
      control = list(
        alpha = alpha,   # Set tuning parameter
        beta = 0,        # Set tuning parameter
        method = "UKF")) # Set estimation method (EKF is default)
  }, error = function(...) NA) # Return NA if fails
}

fit <- fit_funcs$dd_UKF()

# Look at new plot
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs, col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)
```

Below, we define a function to simulate a first order random walk where only one parameter (`x2`) is time variying:
```{r}
# Fitting with fixed effects
fit_funcs$dd_fixed <- function(s = sims$res, LR = 1, NR_eps = NULL){
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ 
        -1 + ddFixed(rep(1, length(x1))) +       # Fix intercept
        ddFixed(x1) + x2 +                       # Note x2 is time variying
        ddFixed(x3) + ddFixed(x4) + ddFixed(x5),
             data = s, max_T = T_max, by = 1, id = s$id, 
             Q_0 = diag(1, 1), Q = diag(.01, 1),
             control = list(LR = LR, NR_eps = NR_eps))
  }, error = function(...) NA) # Return NA if fails
}

fit <- fit_funcs$dd_fixed()

# Look at new plot
plot_func(ylim = range(fit$state_vecs, fit$fixed_effects))
matplot(fit$state_vecs, col = 3, lty = 2, 
        type = "l", add = T)
abline(h = fit$fixed_effects, col = c(1:2, 4:6), lty = 2)
```

Next, we define a function to fit the model with a second order random walk:
```{r}
# Fitting with second order
fit_funcs$dd_2_order <- function(s = sims$res, LR = 1, NR_eps = NULL){
  tryCatch({
    ddhazard(
      formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
      data = s, max_T = T_max, by = 1, id = s$id, 
      # Q_0 and Q needs more elements 
      Q_0 = diag(c(rep(1, n_beta + 1), rep(0.5, n_beta + 1))), 
      Q = diag(c(rep(.01, n_beta + 1), rep(0, n_beta + 1))),
      order = 2, # specify the order
      control = list(LR = LR, NR_eps = NR_eps))
  }, error = function(...) NA) # Return NA if fails
}

fit <- fit_funcs$dd_2_order(LR = .75)

# Look at new plot
plot_func(ylim = fit$state_vecs)
matplot(fit$state_vecs[, 1:6], col = 1:(n_beta + 1), lty = 2, 
        type = "l", add = T)
```

## \*\* Defintion of GAM fit function
We define the estimation method for the Generalized additive model in the next code snippet. We use `bam` function from the `mgcv` package which corresponds to `gam` but for very large datasets

```{r est_true_fit_mgcv}
library(mgcv)
fit_funcs$gam <- function(s = sims$res){
  # get data frame for fitting
  dat_frame <- get_survival_case_weigths_and_data(
    formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
               data = s, max_T = T_max, by = 1, id = s$id, use_weights = F)
  # fit model
  bam(
    formula = Y ~ 
      # cr is cubic basis with dim of k
      s(t, bs = "cr", k = 10, by = x1) + 
      s(t, bs = "cr", k = 10, by = x2) + 
      s(t, bs = "cr", k = 10, by = x3) + 
      s(t, bs = "cr", k = 10, by = x4) + 
      s(t, bs = "cr", k = 10, by = x5),
  family = binomial, data = dat_frame,
  method = "GCV.Cp")
}

# fit model
fit <- fit_funcs$gam()

# Compare plot
layout(matrix(1:6, nrow = 2))
for(i in 1:n_beta){
  plot(fit, pages = 0, rug = F, col = i + 1, select = i, lty = 2)
  lines(sims$betas[-1, i + 1], col = i + 1)
}
```

## \*\* Definition of prediction functions
The following code snippets define predictions methods for each of the estimation methods. We start off by defining a split function such that we can sample individuals (series) into a test set and a traning test:
```{r est_true_pred_funcs_static}
split_func <- function(s = sims$res){
  # Sample ids
  test_ids <- sample(
    unique(s$id), floor(length(unique(s$id)) / 2), replace = F) 
  
  # Return seperate data frames
  return(list(test_dat = s[s$id %in% test_ids, ],
              fit_dat = s[!s$id %in% test_ids, ]))
}

# Illustrate use
tmp <- split_func()
# No ids intersect in the two sets
length(intersect(tmp$test_dat$id, tmp$fit_dat$id))
# The union is exactly the number of ids we simulated
length(union(tmp$test_dat$id, tmp$fit_dat$id))
```

Having define the splitting method, we turn the prediction methods. The idea is to define the `brier_funcs$general` function which takes in a prediction function, a fit and a data frame. Next, we then define individual prediction methods which will be passed to `brier_funcs$general`:

```{r}
# Define prediction function for static model
brier_funcs <- list() 
brier_funcs$general <- function(brier_func, fit, eval_data_frame){
  d_frame <- get_survival_case_weigths_and_data(
    formula = Surv(tstart, tstop, event) ~ . - tstart - tstop - id - event,
    data = eval_data_frame, max_T = T_max, by = 1, id = eval_data_frame$id, 
    use_weights = F)
  
  # Change start and stop times
  d_frame$tstart <- d_frame$t - 1
  d_frame$tstop <- d_frame$t
  
  # Compute residuals
  resids <- brier_func(fit, d_frame)

  # Return estimates
  list(brier = mean.default(resids^2),
       median_abs_res = median(abs(resids)),
       sd_abs_res = sd(abs(resids)),
       # Info is added to debug
       info = cbind(resids = resids,
                    tstop = d_frame$tstop, 
                    id = d_frame$id))
}

# Prediction method for static model
brier_funcs$static <- function(fit, d_frame){
  preds <- predict(fit, newdata = d_frame, type = "response")
  return(d_frame$Y - preds)
}

# Test function
fit <- fit_funcs$static(tmp$fit_dat)
unlist(
  brier_funcs$general(brier_funcs$static, fit, tmp$fit_dat)[
    c("brier", "median_abs_res", "sd_abs_res")]) # in sample stats
unlist(
  brier_funcs$general(brier_funcs$static, fit, tmp$test_dat)[
    c("brier", "median_abs_res", "sd_abs_res")]) # out sample stats
```

```{r est_true_pred_ddhazard}
# Define prediction function for ddhazard model
brier_funcs$dd <- function(fit, d_frame){
  preds <- predict(fit, new_data = d_frame, tstart = "tstart", tstop = "tstop")
  return(d_frame$Y - preds$fits)
}

fit <- fit_funcs$dd(tmp$fit_dat, LR = .5)

unlist(
  brier_funcs$general(brier_funcs$dd, fit, tmp$fit_dat)[
    c("brier", "median_abs_res", "sd_abs_res")]) # in sample stats
unlist(
  brier_funcs$general(brier_funcs$dd, fit, tmp$test_dat)[
    c("brier", "median_abs_res", "sd_abs_res")]) # out sample stats
```

```{r est_true_pred_gam}
# Define prediction function for gam model
brier_funcs$gam <- function(fit, d_frame){
  preds <- predict(fit, newdata = d_frame, type = "response")
  return(d_frame$Y - preds)
}

fit <- fit_funcs$gam(tmp$fit_dat)
unlist(
  brier_funcs$general(brier_funcs$gam, fit, tmp$fit_dat)[
    c("brier", "median_abs_res", "sd_abs_res")]) # in sample stats
unlist(
  brier_funcs$general(brier_funcs$gam, fit, tmp$test_dat)[
    c("brier", "median_abs_res", "sd_abs_res")]) # out sample stats
```

## \*\* Definition of multiple simulations function
To make things easier, we define a function that takes in a function to simulate. Given a function to simulate, then the new function perform `n_sims =` `r n_sims` simulation for each of the runs values `ns` (`r ns`): 
```{r simulations_def, warning=FALSE}
simulate_n_print_res <- function(
  sim_func, # Function that takes one argment which is number of series
  LRs = c(.5, 1, .5, 1),           # Learning rates
  NR_eps = c(rep(.01, 2), NA, NA)) # Tolerance in scoring step
  {
  for(n in ns){
    out <- array(NA_real_, dim = c(n_sims, 5 + length(LRs), 3),
                 dimnames = list(
                   NULL,
                   c("static", paste("dd (", LRs, ", ", NR_eps, ")", sep = ""),
                     "2 order EKF", "Fixed effect", "UKF", "gam"),
                   c("Brier", "Median abs res", "sd abs res")))
    
    n_failures_and_surviers <- array(
      NA_integer_, dim = c(2, n_sims), 
      dimnames = list(c("# failures", "# survivers"), NULL))
    
    #*******
    # Progress bar for inpatient people (me)
    pb <- tcltk::tkProgressBar(paste("Estimating with n =", n), "",
							0, n_sims, 50)
    #*******
    
    for(i in 1:n_sims){
      #*******  
      info <- sprintf("%d%% done", 100 * (i - 1) / n_sims)
      tcltk::setTkProgressBar(pb, i - 1, paste("Estimating with n =", n), info)
      #*******
      
      # Sample until we get an outcome have suffecient amount of deaths and 
      # failures
      repeat{
        sims <- sim_func(n)
        
        # We want some survivers and some deaths
        if(sum(sims$res$event) > 40 && n - sum(sims$res$event) > 40)
          break
      }
      
      n_failures_and_surviers["# failures", i] <- sum(sims$res$event)
      n_failures_and_surviers["# survivers", i] <- n - sum(sims$res$event)
      
      # Split data
      sim_split <- split_func(sims$res)
      
      # Fit static model
      static_fit <- fit_funcs$static(sim_split$fit_dat)
      
      # Fit dd model
      dd_fits <- list(rep(NA, length(LRs)))
      for(k in seq_along(LRs)){
        dd_fits[[k]] <- fit_funcs$dd(
          sim_split$fit_dat, LR = LRs[k], 
          NR_eps = if(is.na(NR_eps[k])) NULL else NR_eps[k])
      }
      
      # Fit second order
      dd_2_order <- fit_funcs$dd_2_order(sim_split$fit_dat)
      
      # Fit fixed effect
      dd_fixed <- fit_funcs$dd_fixed(sim_split$fit_dat)
      
      # UKF fit 
      dd_UKF <- fit_funcs$dd_UKF(sim_split$fit_dat)
      
      # Fit gam model
      gam_fit <- fit_funcs$gam(sim_split$fit_dat)
      
      # Evalute on test data
      models <- c(list(static_fit), dd_fits, 
                  list(dd_2_order, dd_fixed, dd_UKF, gam_fit))
      
      eval_funcs = c(brier_funcs$static,
                     replicate(length(dd_fits) + 3, brier_funcs$dd),
                     brier_funcs$gam)
                     
      for(j in seq_along(models)){
        if(length(models[[j]]) == 1 && is.na(models[[j]]))
          next # We have to skip models fits that failed
        
        metrics <- brier_funcs$general(
          eval_funcs[[j]], models[[j]], sim_split$test_dat)
        out[i, j, "Brier"] <- metrics$brier
        out[i, j, "Median abs res"] <- metrics$median_abs_res
        out[i, j, "sd abs res"] <- metrics$sd_abs_res
      }
    }
    
    #*******  
    close(pb)
    #*******  
    
    # Print results
    cat(paste("##############\nNumber of series in fit and test data are", 
              n, "\n"))
    
    cat("Number of failed fits:\n")
    print(apply(out[, , 1], 2, function(x) sum(is.na(x))))
    
    cat("\nNumber of failures and survivers in simulation and whether a given model failed to fit:\n")
    n_nas <- rbind(n_failures_and_surviers, 
                   t(is.na(out[, , 1])))[, seq_len(min(20, n_sims))]
    print(ifelse(n_nas ==0 , NA, n_nas), digits = 4, na.print = "")
    
    cat("\nMean of metrics:\n")
    print(apply(out, 3, colMeans, na.rm = T), digits = 4)
    
    cat("\nNumber of complete cases where all succeeded to fit", 
        n_cases_all_success <- sum(complete.cases(out[, , 1])))
    if(n_cases_all_success > 0){
      cat("\nMean of metrics where all succeeded to fit:\n")
      print(apply(out[complete.cases(out[, , 1]), , , drop = F], 3, colMeans), 
            digits = 4)
      cat("\nSds of metrics where all succeeded to fit:\n")
      print(apply(out[complete.cases(out[, , 1]), , , drop = F], c(2,3), sd), 
            digits = 4)
    }
    
    if(n != tail(ns, 1))
      cat("\n\n")
  }
}
```

## Simulating

We are now able to simulate from the model where all effects are time variying and we use the correct binning intervals with the code below. `dd (x, y)` stand for a `ddhazard` fit with a learning rate of `x` and a tolerance in the correction step of `y`. If `y` is `NA` then we have only used a single iteration
```{r simulations_all_variying, warning=FALSE}
set.seed(1243)
# Use simulation function
simulate_n_print_res(
  sim_func = function(n)
    do.call(test_sim_func_logit, c(default_args, c(list(n_series = n)))))
```

We should only compare accross methods with mean metrics where `where all succeeded fit`. The logic being that those where the `ddhazard` method fail may be "hard".

TODO: Write conclusion on findings

# Single time variying parameter
In this part, we will look at the performs when only a single coeffecient (`x2`) varies. Thus, we can see if the model where only (`x2`) varies performs beform better. Or in other words how the misspecified models behaves

## \*\* Definition of simulation function
We start by defining the simulation function. The main change here are that we only set a single variance and that we set it larger than before:
```{r , warning=FALSE}
# Use simulation function
set.seed(9999)
sim_one_variying <- function(n){
  test_sim_func_logit(
    n_series = n, 
    sds = c(sqrt(3)), # Large variance
    is_fixed = c(1:2, 4:6), # All but parem three is fixed
    
    # Same values as before
    n_vars = n_beta, 
    beta_start = c(-1, -.5, 0, 1.5, 2), 
    intercept_start = -4, 
    t_max = T_max,
    x_range = 1, 
    x_mean = .5)
}
```

## \* Illustration of single simulation
```{r}
# We get a more variable number of failures and survivers (we simulate 200 
# series)
replicate(10, sum(sim_one_variying(200)$res$event))

# Here is an example of a series
tmp <- sim_one_variying(200)
matplot(tmp$betas, type = "l", lty = 1, ylab = "Beta", xlab = "Time")
```

## Simulating
We can simulate with the following call:
```{r simulations_one_variying, warning=FALSE}
# Use simulation function
set.seed(8080)
simulate_n_print_res(sim_func = sim_one_variying)
```

TODO: Conclusion

# Incorrect binning time
Now, what happens if we get the binning wrong? This is the next expermint we will perform. Specifically, we will set the binning length to `0.1` instead `1` when we simulate. 

## \*\* Definition of simulation function
```{r}
set.seed(9001)
sim_finer_binning <- function(n){
  time_denom = 10 # how much finer do we want to bin?
  
  res <- test_sim_func_logit(
    n_series = n, 
  
    # We multiply through appropiately
    beta_start = c(-1, -.5, 0, 1.5, 2),
    intercept_start = - 8, # Note, we changed the intercept
    sds = c(.1, rep(1, n_beta)) / sqrt(time_denom),
    t_max = T_max * time_denom,
    lambda = 1 / time_denom, # note we change the time when covariates are 
                             # updated (the lambda parem in the rate ~ exp(.) 
                             # in the time between increaments)
    
    n_vars = n_beta,
    x_range = 1,
    x_mean = .5)
  
  # Change time denominator
  res$res$tstart <- res$res$tstart / time_denom
  res$res$tstop <- res$res$tstop / time_denom
  
  res
}
```

## \* Illustration of single simulation
This is how a single draw of coffecient can look:
```{r}
# We get more variable outcomes (we simulate 200 series)
replicate(10, sum(sim_finer_binning(200)$res$event))

# Here is an example of the series
tmp <- sim_finer_binning(200)
matplot(tmp$betas, type = "l", lty = 1, ylab = "Beta", xlab = "Time")
```

## Simulating
We are now able to simulate with the following call:
```{r simulations_sim_finer_binning, warning=FALSE}
# Use simulation function
set.seed(747)
simulate_n_print_res(sim_func = sim_finer_binning)
```

TODO: Conclussion

# Linear Time complexity
TODO: Show linear time complexity
