---
title: "Bootstrap illustration"
author: "Benjamin Christoffersen"
date: "`r Sys.Date()`"
bibliography: Bootstrap_illustration.bib
csl: ../bib_style.csl
output: pdf_document
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(
  default_opts = function(before, options, envir) {
    if (before){
      options(digist = 4)
    }},
  plot_2x3 = function(before, options, envir){
    if(before){
      options$fig.width = options$fig.height * 2
      par(mfcol = c(2, 3))
    }
  })
options(digist = 4)
knitr::opts_chunk$set(echo = TRUE, fig.height = 5, dpi = 36, 
                      cache = F)
knitr::opts_knit$set(default_opts = T)

options(ddhazard_max_threads = max(parallel::detectCores() - 2, 1))
```

# Introduction

This vignette will show how to bootstrap the confidence intervals of a `ddhazard` call. This vignette builds on the vignettes 'ddhazard' and 'Comparing methods for time varying logistic models'. Thus, it is recommended to read these first. You can get the version used to make this vignette by calling:`

```{r echo=FALSE}
tryCatch({
  current_sha <- paste0("@", httr::content(
    httr::GET("https://api.github.com/repos/boennecd/dynamichazard/git/refs/heads/master")
    )$object$sha)
}, error = function(...){ current_sha <<- "" })

stopifnot(length(current_sha) > 0 && class(current_sha) == "character")

current_version <- paste0("boennecd/dynamichazard", current_sha)
```

```{r}
current_version # The string you need to pass devtools::install_github
```

```{r eval=FALSE}
devtools::install_github(current_version)
```

You can also get the latest version on CRAN by calling:

```{r eval=FALSE}
install.packages("dynamichazard")
```

# PBC data set
We start by settings up the data set. We will use the pbc2 data set from the `survival` package as in the vignette 'Comparing methods for time varying logistic models':
```{r}
# PBC data set from survival with time variying covariates
# Details of tmerge are not important in this scope. The code is included
# to make you able to reproduce the results
# See: https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf
library(survival)
temp <- subset(pbc, id <= 312, select=c(id, sex, time, status, edema, age))
pbc2 <- tmerge(temp, temp, id=id, death = event(time, status))
pbc2 <- tmerge(pbc2, pbcseq, id=id, albumin = tdc(day, albumin),
               protime = tdc(day, protime), bili = tdc(day, bili))
pbc2 <- pbc2[, c("id", "tstart", "tstop", "death", "sex", "edema", 
                 "age", "albumin", "protime", "bili")]
```

Next, we fit the model as in the vignette 'Comparing methods for time varying logistic models':

```{r}
library(dynamichazard)
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ age + edema +
                        log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(10000, 6)), Q = diag(rep(0.001, 6)),
                   control = list(save_risk_set = T, save_data = T, eps = .1))
```

A plot of the estimates is given below. The dashed lines are 95% point-wise confidence intervals using the variances estimates from the Extended Kalman filter with smoothing:

```{r}
plot(dd_fit)
```

## Sampling individuals

We can bootstrap the estimates in the model by using `ddhazard_boot` function as done below:

```{r}
set.seed(7451)
R <- 10000
boot_out <- ddhazard_boot(
  dd_fit, 
  do_sample_weights = F,      # should re-sampeling be by weights or by
                              # sampling each individual discreetly
  R = R                       # Number of bootstrap samples
  )

# The list has the same structure and class as the list returned by boot::boot
# Though, a few elements are added 
class(boot_out)
```

Above, we bootstrap the model by sampling the individuals. I.e. individuals will have weights of $0,1,2,\dots$ in the estimation. We can plot 95% confidence bounds from the bootstrap coefficients with the Percentile Bootstrap method as follows:

```{r}
plot(dd_fit, ddhazard_boot = boot_out, level = 0.95)
```

The completely black line is the original estimates, the dashed lines are 2.5% and 97.5% quantiles of the bootstrap coefficient taken at each point and the transparent black lines each represent a bootstrap estimate. Linear interpolation on the normal quantile scale is used if we do not have a quantile that match exactly. E.g. say we want a quantile $\alpha = .975$ as above but $(R + 1)\alpha$ is not an integer where $R$ is the number of bootstraps. Then we compute the quantile, $t^*_{(R+1)\alpha}$ by:

$$t^*_{((R+1)\alpha)} = t^*_{(k)}+\frac{\Phi^{-1}(\alpha)-\Phi^{-1}(\frac{k}{R+1})}{
  \Phi^{-1}(\frac{k+1}{R+1}) - \Phi^{-1}(\frac{k}{R+1})}(t^*_{(k+1)} - t^*_{(k)}), 
    \qquad k=\lfloor (R+1)\alpha \rfloor$$
    
where $\Phi^{-1}$ is the inverse cumulative distribution function and $\lfloor \cdot \rfloor$ is the floor operator and $t_(j)^*$ is the $j$'th ordered value

## Strata
You can provide a strata to perform stratified sampling with. This is done by setting the `strata` argument in the call to `ddhazard_boot`. Notice that this has to be on an individual level (one indicator variable per individual) not observation level (not one indicator variable per row in the data set). Further, you can use the `unique_id` argument to match the individual entries with the entries in `strata`. As an example, we stratify by the `age` at the start of the study period with the code below:

```{r}
# Individuals have different number of rows in the dataset
xtabs(~xtabs(~pbc2$id))

# Though all the individual have the same age for all periods
# This age is the age at the start of the study
unique(tapply(pbc2$age, pbc2$id, function(x) length(unique(x))))

# Next, we find the age for each individual
unique_id <- unique(pbc2$id)
age <- sapply(unique_id, function(x) pbc2$age[pbc$id == x][1])
summary(age)

# We define a strata variable for those less than age 50
is_less_than_50 <- age < 50

# We perform stratified sampling over this variable as follows
set.seed(101)
boot_out_with_strata <- ddhazard_boot(
  dd_fit,
  unique_id = unique_id, 
  strata = is_less_than_50,
  R = R)

plot(dd_fit, ddhazard_boot = boot_out_with_strata)
```

The above code is only provided for illustrative purposes. There is no reason to do stratified sampling over the age variable (as far as I gather). However, it may be useful if you have e.g. categorical variables in your model and want to ensure that each bootstrap sample has a given amount of observation in each category. Lastly, setting `do_stratify_with_event = T` will yield an interaction factor between the passed `strata` and whether or not the given individual has an event. Stratified sampling will then be performed over this variable

## Sampling weights
We can also sample the weights. This is done as follows: within each stratum $j$ (e.g. males or females) let $r_j$ denote the number of individuals. Then we sample $r_j$ uniform variables $l_i\sim\text{Unif}(0,1)$ for $i = 1,\dots,r_j$ and normalize with a constant $c$ such that $\sum_{i=1}^{r_j} l_i/c = r_j$. The code below will sample the weights as described above:
```{r}
set.seed(401)
boot_out_by_weights <- ddhazard_boot(
  dd_fit, 
  do_sample_weights = T, # changed
  R = R)

plot(dd_fit, ddhazard_boot = boot_out_by_weights)
```

There is only on stratum in the above which is the entire sample

## Fixed effects
Fixed effects (time invariant effects) can also be bootstrap to get confidence bounds. The fixed effects bootstrap coefficients are added as the last entries of the element `t` of the returned object by `ddhazard_boot`. As an example we will estimate a model below where `log(protime)` and the intercept are fixed

```{r}
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ 
                     ddFixed(1) + ddFixed(log(protime)) # changed to fixed
                     + age + edema + log(albumin) +  + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(10000, 4)), Q = diag(rep(0.001, 4)),
                   control = list(
                     save_risk_set = T, save_data = T, eps = .1,
                     fixed_terms_method = "E_step")     # Fixed effects are 
                                                        # esimated in E-step
                   )

```

The time varying effects are plotted below:

```{r}
plot(dd_fit)
```

The fixed effects are estimated to:

```{r}
dd_fit$fixed_effects
```

We can bootstrap the estimates with a call similar to those we made before:

```{r}
set.seed(9001)
boot_out <- ddhazard_boot(
  dd_fit, 
  do_sample_weights = F,  # dont sample weights
  R = R)

# Plot time varying effects
plot(dd_fit, ddhazard_boot = boot_out)
```

We then turn the bootstrap confidence intervals of the fixed effects. These can be computed with the `boot.ci` function from the `boot` library as shown below:

```{r, fig.height=3.5}
library(boot)

# We start by printing confidence intervals for 
colnames(boot_out$t)[ncol(boot_out$t) - 1]

boot.ci(boot_out, index = ncol(boot_out$t) - 1, 
        # We specify the types of confidence intervals estimates here:
        type = c(
          "norm",  # A matrix of intervals calculated using the normal 
                   # approximation.
          "basic", # The intervals calculated using the basic bootstrap method.
          "perc",  # The intervals calculated using the bootstrap percentile 
                   # method.
          "bca")   # The intervals calculated using the adjusted bootstrap 
                   # percentile (BCa) method.
        )

# Then we print confidence intervals for 
colnames(boot_out$t)[ncol(boot_out$t)]
boot.ci(boot_out, index = ncol(boot_out$t) - 0, type = c(
  "norm", "basic", "perc", "bca"))
```

## Boot envelopes

We may also want to get simultaneous confidence intervals. An easy way to get such confidence intervals is with the `envelope` function with the `boot` library. For instance, we can simultaneous confidence intervals for the `bili` covariate as follows: 

```{r}
# Find the indices that correspondents to the log(bili) variable 
is_bili_coef <- grep("^log\\(bili\\):", colnames(boot_out$t))

# Use the envelope
envelopes <- envelope(boot_out, level = 0.95 ,index = is_bili_coef)

# Plot curves
plot(dd_fit, cov_index = 4, ylim = c(-1, 2.5))
lines(dd_fit$times, envelopes$point[1, ], col = "blue")
lines(dd_fit$times, envelopes$point[2, ], col = "blue")

lines(dd_fit$times, envelopes$overall[1, ], col = "red")
lines(dd_fit$times, envelopes$overall[2, ], col = "red")
```

The dashed black lines are from the smoothed covariance matrix. The blue lines are pointwise confidence intervals using the percentile method from the `envelope` function. The red line is the simultaneous confidence bounds using the envelope method in equation (4.17) of [@davison97]. The latter curves are formed by creating an envelope over each of the pointwise confidence intervals and hence the name

# How good is the coverage
```{r, echo=FALSE, include=FALSE}
rm(list = ls())
gc()
```


In this section, we will test the coverage of the pointwise confidence intervals using the smoothed covariance matrix and the bootstrap percentile method. We will test these in a simulation study where: 

 * The coefficients are drifting deterministically with a some normal noise added to them
 * Individuals have time invariant covariates

The simulation is to mimic a situation where we assume that the coefficients are not random (as the model implies) but we do not know the shape of the coefficient curves across time. We setup the parameters for the experiment below and plot the coefficients without noise:

```{r}
tmax <- 22                           # Number of periods
n_start_grps <- 3                    # Number of "start group" - see text
                                     # Number of multiple of tmax - 1 in each 
mlt <- 30                             # start group
n <- (tmax - 1) * mlt * n_start_grps # Total number of individuals
n

# Define the noise free coeffecients
beta <- cbind(
  x1 = rep(-2, (tmax - 1) + 1), 
  x2 = (0:(tmax - 1) - (tmax - 1)/2) / ((tmax - 1) / 2),
  x3 = ((tmax - 1):0 - (tmax - 1)/2) / ((tmax - 1) / 2),
  x4 = - sin(pi / 7 * (0:(tmax - 1))), 
  x5 = sin(pi / 7 * (0:(tmax - 1))))

# Plot noise free coeffecients
matplot(beta, type = "l", lty = 1, ylab = "Coeffecient without noise")
```

There will be a total of `n =` `r n` individuals in groups of three. We start observing each group at time `0`, `7` and `14`. We do so to have a "stable" number of individual through the experiment. The experiment ends after `tmax = ` `r tmax`. 

```{r, include=FALSE, echo=FALSE}
base_mat <- do.call(
  rbind, 
  sapply(1:n, function(i){
    n_vals <- ceiling(i / ((tmax - 1) * mlt)) * ((tmax - 1) / n_start_grps) + 1
    data.frame(id = rep(i, n_vals),
               tstart = tmax - n_vals:1)
  }, simplify = F))
base_mat$tstop <- base_mat$tstart + 1
nrows <- nrow(base_mat)

beta_sd <- .1
x_range <- 1

base_mat$x1 <- 1
for(s in c("x2", "x3", "x4", "x5", "eta", "dies"))
  base_mat[s] <- NA

sim_func <- function(){
  # Simulate covariates
  sim_mat <- base_mat
  for(i in 1:n)
    sim_mat[sim_mat$id == i, c("x2", "x3", "x4", "x5")] <- 
      sapply(runif(4, min = -x_range, max = x_range), rep, times =  sum(sim_mat$id == i))
  
  # Add noise to coefficients
  beta_w_err <- beta + rnorm(length(beta), sd = beta_sd)
  
  # Compute linear predictors and simulate outcome
  for(i in 1:tmax){
    is_in <- which(base_mat$tstop == i)
    sim_mat[is_in, "eta"] <- 
      as.matrix(sim_mat[is_in, c("x1", "x2", "x3", "x4", "x5")]) %*% 
      beta_w_err[i, ]
  }
  sim_mat$dies <- 1/(1 + exp(-sim_mat$eta)) > runif(nrows)
  
  # Remove rows after they have died
  has_died <- unlist(tapply(
    sim_mat$dies, sim_mat$id, function(x){
      is_death <- which(x)
      if(length(is_death) == 0) rep(F, length(x)) else 1:length(x) > is_death[1]
    }))
  sim_mat <- sim_mat[!has_died, ]
  
  # Find last time
  max_time <- tapply(sim_mat$tstop, sim_mat$id, max)
  
  # Remove redundant rows
  do_die <- tapply(sim_mat$dies, sim_mat$id, any)
  is_first <- unlist(tapply(sim_mat$tstart, sim_mat$id, function(x) x == min(x)))
  sim_mat <- sim_mat[is_first, ]
  sim_mat$tstop <- max_time
  sim_mat$dies <- do_die
  
  list(sims = sim_mat, beta_w_err = beta_w_err)
}
```

We add a bit of normally distributed noise to the coefficients with mean zero and standard deviation `r beta_sd`. The individuals' covariates are simulated from the uniform distribution from the range [`r -x_range`, `r x_range`]. The function `sim_func` is used to make the simulation. The definition of the function can be found in the markdown file for this vignette on the github site. We simulate a series below, illustrate the data matrix and plot the coefficients with noise added to them:

```{r}
# Simulate
set.seed(122044)
sim_list <- sim_func()

# Show data matrix
head(sim_list$sims, 10)
tail(sim_list$sims, 10)

# Plot coeffecients with noise
matplot(sim_list$beta_w_err, type = "l", lty = 1, ylab = "Coeffecient with noise")
```

We are now able to estimate the model as follows:

```{r, plot_2x3=TRUE}
# Estimate model
fit_expression <- expression({
  fit <- ddhazard(Surv(tstart, tstop, dies) ~ -1 + x1 + x2 + x3 + x4 + x5,
                data = sim_list$sims, id = sim_list$sims$id, max_T = tmax, 
                by = 1, Q_0 = diag(1e4, 5), Q = diag(1, 5),
                a_0 = rep(0, 5), control = list(
                  ridge_eps = 1e-3, eps = .01, criteria = "delta_likeli"))
})
eval(fit_expression)

# Plot estimates with pointwise confidence bounds from smoothed covariance
# matrix
for(i in 1:5){
  plot(fit, cov_index = i)
  points(sim_list$beta_w_err[, i], pch = 16, col = "red")
}
```

The plots shows the estimated coefficient with 95% pointwise confidence intervals from the smoothed covariance matrix. The dots are the actual estimates (i.e. those with noise added to them). A bootstrap estimate of the confidence bounds is made below:

```{R, plot_2x3=TRUE}
# Bootstrap with resampling individuals
boot_out <- ddhazard_boot(fit,  
                          do_sample_weights = F, R = 999, 
                          do_stratify_with_event = F)

# Plot estimated confidence bounds
for(i in 1:5){
  plot(fit, cov_index = i, ddhazard_boot = boot_out)
  points(sim_list$beta_w_err[, i], pch = 16, col = "red")
}
```

```{r, echo=FALSE, include=FALSE}
alpha <- .05

compute_coverage <- function(fit, boot_out, actual){
  # Compute coverage of error bounds from smoothed covariance matrix
  bds <- qnorm(1 - alpha / 2) * sqrt(t(apply(fit$state_vars, 3, diag))[-1, ])
  res_smooth <- colMeans(
    actual >= fit$state_vecs[-1, ] - bds & actual <= fit$state_vecs[-1, ] + bds)
  
  # Compute coverage from bootstrap
  enve <- sapply(1:ncol(boot_out$t), function(i){
      bt_ci <- boot.ci(boot_out, index = i, type = c("perc"), conf = 1 - alpha)
      c(bt_ci$percent[, 4:5])
    })
  enve_lb <- enve[1, ]
  enve_ub <- enve[2, ]
  dim(enve_lb) <- dim(enve_ub) <- dim(fit$state_vecs)
  
  res_boot <- colMeans(
    actual >= enve_lb[-1, ] & actual <= enve_ub[-1, ])
  
  list(smooth = res_smooth, boot = res_boot)
}
```

We can now pose the question how the pointwise coverage is for each coefficient. For this reason, we have defined the function `compute_coverage` which is not included but can be found in the markdown for this vignette on the github site:

```{r}
compute_coverage(fit, boot_out, sim_list$beta_w_err)
```

`compute_coverage` outputs a list of the true coverage of the `r (1 - alpha)*100`% confidence intervals from the smoothed covariance matrix and the percentile method from the bootstrap. These two are respectively the `smooth` and `boot` elements of the list. We can now repeat the above `M` times (defined below) as follows:

```{r, error=FALSE, warning=FALSE}
set.seed(520920)
R <- 999  # Number of bootstrap estimates in each trail
M <- 19  # Number of trails

# Define matrices for output
coverage_boot <- coverage_smooth <- matrix(
  NA_real_, nrow = M, ncol = ncol(fit$state_vecs))

# Sometimes estimations fails. We use this counter to keep track of the number
# of times
n_fails <- 0

for(i in 1:M){
  # Simulate data set
  sim_list <- sim_func()
  
  # Fit on whole data set
  did_succed <- F
  try({
    eval(fit_expression)
    did_succed <- T
  })
  if(!did_succed){
    n_fails <- n_fails + 1
    next
  }
  
  # Bootstrap fits
  boot_out <- ddhazard_boot(fit,
                            strata = as.factor(sim_list$sims$tstart),
                            do_stratify_with_event = F,
                            do_sample_weights = F, R = R)
  
  # Compute coverage and add to output
  coverage <- compute_coverage(fit, boot_out, sim_list$beta_w_err)
  coverage_smooth[i, ] <- coverage$smooth
  coverage_boot[i, ] <- coverage$boot
  
  # Clear memory
  rm(boot_out, fit, sim_list); gc()
}

n_fails # number of failed estimations 
```

The mean coverage of the two methods are printed below:

```{r}
colMeans(coverage_smooth, na.rm = T)
colMeans(coverage_boot, na.rm = T)
```

Finally, we can make a boxplot of the coverage in each trail as follows:

```{r}
boxplot(coverage_smooth, ylim = c(.6, 1), main = "Smoothed covariance")
abline(h = .95, lty = 1)

boxplot(coverage_boot, ylim = c(.6, 1), main = "Bootstrap")
abline(h = .95, lty = 1)
```

# References
