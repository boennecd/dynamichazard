@article{dempster77,
  title={Maximum likelihood from incomplete data via the {EM} algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society. Series B (methodological)},
  pages={1--38},
  year={1977},
  publisher={JSTOR}
}

@article{louis82,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345828},
 abstract = {A procedure is derived for extracting the observed information matrix when the EM algorithm is used to find maximum likelihood estimates in incomplete data problems. The technique requires computation of a complete-data gradient vector or second derivative matrix, but not those associated with the incomplete data likelihood. In addition, a method useful in speeding up the convergence of the EM algorithm is developed. Two examples are presented.},
 author = {Thomas A. Louis},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {226--233},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Finding the Observed Information Matrix when Using the {EM} Algorithm},
 volume = {44},
 year = {1982}
}

@article{meng93,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337198},
 abstract = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expectation/Conditional Maximization (CM), that takes advantage of the simplicity of complete-data conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler CM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
 author = {Xiao-Li Meng and Donald B. Rubin},
 journal = {Biometrika},
 number = {2},
 pages = {267--278},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Maximum Likelihood Estimation via the {ECM} Algorithm: {A} General Framework},
 volume = {80},
 year = {1993}
}

@Article{kitagawa94,
author="Kitagawa, Genshiro",
title="The two-filter formula for smoothing and an implementation of the {G}aussian-sum smoother",
journal="Annals of the Institute of Statistical Mathematics",
year="1994",
month="Dec",
day="01",
volume="46",
number="4",
pages="605--623",
abstract="A Gaussian-sum smoother is developed based on the two filter formula for smoothing. This facilitates the application of non-Gaussian state space modeling to diverse problems in time series analysis. It is especially useful when a higher order state vector is required and the application of the non-Gaussian smoother based on direct numerical computation is impractical. In particular, applications to the non-Gaussian seasonal adjustment of economic time series and to the modeling of seasonal time series with several outliers are shown.",
issn="1572-9052",
doi="10.1007/BF00773470",
url="https://doi.org/10.1007/BF00773470"
}

@article{kitagawa96,
  title={Monte Carlo filter and smoother for non-{G}aussian nonlinear state space models},
  author={Kitagawa, Genshiro},
  journal={Journal of computational and graphical statistics},
  volume={5},
  number={1},
  pages={1--25},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{Durbin97,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337587},
 abstract = {State space models are considered for observations which have non-Gaussian distributions. We obtain accurate approximations to the loglikelihood for such models by Monte Carlo simulation. Devices are introduced which improve the accuracy of the approximations and which increase computational efficiency. The loglikelihood function is maximised numerically to obtain estimates of the unknown hyperparameters. Standard errors of the estimates due to simulation are calculated. Details are given for the important special cases where the observations come from an exponential family distribution and where the observation equation is linear but the observation errors are non-Gaussian. The techniques are illustrated with a series for which the observations have a Poisson distribution and a series for which the observation have a t-distribution.},
 author = {J. Durbin and S. J. Koopman},
 journal = {Biometrika},
 number = {3},
 pages = {669--684},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Monte Carlo Maximum Likelihood Estimation for Non-{G}aussian State Space Models},
 volume = {84},
 year = {1997}
}

@article{de97,
  title={The scan sampler for time series models},
  author={De Jong, Piet},
  journal={Biometrika},
  volume={84},
  number={4},
  pages={929--937},
  year={1997},
  publisher={Oxford University Press}
}

@article{carpenter99,
  title={Improved particle filter for nonlinear problems},
  author={Carpenter, James and Clifford, Peter and Fearnhead, Paul},
  journal={IEE Proceedings-Radar, Sonar and Navigation},
  volume={146},
  number={1},
  pages={2--7},
  year={1999},
  publisher={IET}
}

@article{pitt99,
  title={Filtering via simulation: Auxiliary particle filters},
  author={Pitt, Michael K and Shephard, Neil},
  journal={Journal of the American statistical association},
  volume={94},
  number={446},
  pages={590--599},
  year={1999},
  publisher={Taylor \& Francis}
}

@Article{doucet00,
author="Doucet, Arnaud
and Godsill, Simon
and Andrieu, Christophe",
title="On sequential Monte Carlo sampling methods for Bayesian filtering",
journal="Statistics and Computing",
year="2000",
month="Jul",
day="01",
volume="10",
number="3",
pages="197--208",
abstract="In this article, we present an overview of methods for sequential simulation from posterior distributions. These methods are of particular interest in Bayesian filtering for discrete time dynamic models that are typically nonlinear and non-Gaussian. A general importance sampling framework is developed that unifies many of the methods which have been proposed over the last few decades in several different scientific disciplines. Novel extensions to the existing methods are also proposed. We show in particular how to incorporate local linearisation methods similar to those which have previously been employed in the deterministic filtering literature; these lead to very effective importance distributions. Furthermore we describe a method which uses Rao-Blackwellisation in order to take advantage of the analytic structure present in some important classes of state-space models. In a final section we develop algorithms for prediction, smoothing and evaluation of the likelihood in dynamic models.",
issn="1573-1375",
doi="10.1023/A:1008935410038",
url="https://doi.org/10.1023/A:1008935410038"
}

@incollection{pitt01,
  title={Auxiliary variable based particle filters},
  author={Pitt, Michael K and Shephard, Neil},
  booktitle={Sequential Monte Carlo methods in practice},
  pages={273--293},
  year={2001},
  publisher={Springer}
}

@article{andrieu02,
  title={Particle filtering for partially observed {G}aussian state space models},
  author={Andrieu, Christophe and Doucet, Arnaud},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={64},
  number={4},
  pages={827--836},
  year={2002},
  publisher={Wiley Online Library}
}

@inproceedings{douc05,
  title={Comparison of resampling schemes for particle filtering},
  author={Douc, Randal and Capp{\'e}, Olivier},
  booktitle={Image and Signal Processing and Analysis, 2005. ISPA 2005. Proceedings of the 4th International Symposium on},
  pages={64--69},
  year={2005},
  organization={IEEE}
}

@inproceedings{briers05,
  title={Sequential auxiliary particle belief propagation},
  author={Briers, Mark and Doucet, Arnaud and Singh, Sumeetpal S},
  booktitle={Information Fusion, 2005 8th International Conference on},
  volume={1},
  pages={8--pp},
  year={2005},
  organization={IEEE}
}

@book{cappe05,
 author = {Capp{\'e}, Olivier and Moulines, Eric and Ryden, Tobias},
 title = {Inference in Hidden Markov Models},
 year = {2005},
 isbn = {978-0-387-40264-2, 978-1-4419-2319-6},
 publisher = {Springer-Verlag New York},
}

@article{Lin05,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590681},
 abstract = {Sequential Monte Carlo methods, especially the particle filter (PF) and its various modifications, have been used effectively in dealing with stochastic dynamic systems. The standard PF samples the current state through the underlying state dynamics, then uses the current observation to evaluate the sample's importance weight. However, there is a set of problems in which the current observation provides significant information about the current state but the state dynamics are weak, and thus sampling using the current observation often produces more efficient samples than sampling using the state dynamics. In this article we propose a new variant of the PF, the independent particle filter (IPF), to deal with these problems. The IPF generates exchangeable samples of the current state from a sampling distribution that is conditionally independent of the previous states, a special case of which uses only the current observation. Each sample can then be matched with multiple samples of the previous states in evaluating the importance weight. We present some theoretical results showing that this strategy improves efficiency of estimation as well as reduces resampling frequency. We also discuss some extensions of the IPF, and use several synthetic examples to demonstrate the effectiveness of the method.},
 author = {Ming T. Lin and Junni L. Zhang and Qiansheng Cheng and Rong Chen},
 journal = {Journal of the American Statistical Association},
 number = {472},
 pages = {1412--1421},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Independent Particle Filters},
 volume = {100},
 year = {2005}
} 


@inproceedings{klaas06,
  title={Fast particle smoothing: If I had a million particles},
  author={Klaas, Mike and Briers, Mark and De Freitas, Nando and Doucet, Arnaud and Maskell, Simon and Lang, Dustin},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={481--488},
  year={2006},
  organization={ACM}
}

@article{doucet09,
  title={A tutorial on particle filtering and smoothing: Fifteen years later},
  author={Doucet, Arnaud and Johansen, Adam M},
  journal={Handbook of nonlinear filtering},
  volume={12},
  number={656-704},
  pages={3},
  year={2009}
}

@Article{briers09,
author="Briers, Mark
and Doucet, Arnaud
and Maskell, Simon",
title="Smoothing algorithms for state--space models",
journal="Annals of the Institute of Statistical Mathematics",
year="2009",
month="Jun",
day="09",
volume="62",
number="1",
pages="61",
abstract="Two-filter smoothing is a principled approach for performing optimal smoothing in non-linear non-Gaussian state--space models where the smoothing distributions are computed through the combination of `forward' and `backward' time filters. The `forward' filter is the standard Bayesian filter but the `backward' filter, generally referred to as the backward information filter, is not a probability measure on the space of the hidden Markov process. In cases where the backward information filter can be computed in closed form, this technical point is not important. However, for general state--space models where there is no closed form expression, this prohibits the use of flexible numerical techniques such as Sequential Monte Carlo (SMC) to approximate the two-filter smoothing formula. We propose here a generalised two-filter smoothing formula which only requires approximating probability distributions and applies to any state--space model, removing the need to make restrictive assumptions used in previous approaches to this problem. SMC algorithms are developed to implement this generalised recursion and we illustrate their performance on various problems.",
issn="1572-9052",
doi="10.1007/s10463-009-0236-2",
url="https://doi.org/10.1007/s10463-009-0236-2"
}

@article{fearnhead10,
  title={A sequential smoothing algorithm with linear computational cost},
  author={Fearnhead, Paul and Wyncoll, David and Tawn, Jonathan},
  journal={Biometrika},
  volume={97},
  number={2},
  pages={447--464},
  year={2010},
  publisher={Oxford University Press}
}

@article{del10,
  title={Forward smoothing using sequential Monte Carlo},
  author={Del Moral, Pierre and Doucet, Arnaud and Singh, Sumeetpal},
  journal={arXiv preprint arXiv:1012.5390},
  year={2010}
}

@article{malik11,
  title={Particle filters for continuous likelihood evaluation and maximisation},
  author={Malik, Sheheryar and Pitt, Michael K},
  journal={Journal of Econometrics},
  volume={165},
  number={2},
  pages={190--209},
  year={2011},
  publisher={Elsevier}
}

@article{poyiadjis11,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/29777165},
 abstract = {Particle methods are popular computational tools for Bayesian inference in nonlinear non-Gaussian state space models. For this class of models, we present two particle algorithms to compute the score vector and observed information matrix recursively. The first algorithm is implemented with computational complexity ð’ª(N) and the second with complexity ð’ª(NÂ²), where N is the number of particles. Although cheaper, the performance of the ð’ª(N) method degrades quickly, as it relies on the approximation of a sequence of probability distributions whose dimension increases linearly with time. In particular, even under strong mixing assumptions, the variance of the estimates computed with the ð’ª(N) method increases at least quadratically in time. The more expensive ð’ª(NÂ²) method relies on a nonstandard particle implementation and does not suffer from this rapid degradation. It is shown how both methods can be used to perform batch and recursive parameter estimation.},
 author = {George Poyiadjis and Arnaud Doucet and Sumeetpal S. Singh},
 journal = {Biometrika},
 number = {1},
 pages = {65--80},
 publisher = {Biometrika Trust},
 title = {Particle approximations of the score and observed information matrix in state space models with application to parameter estimation},
 volume = {98},
 year = {2011}
}

@article{schon11,
  title={System identification of nonlinear state-space models},
  author={Sch{\"o}n, Thomas B and Wills, Adrian and Ninness, Brett},
  journal={Automatica},
  volume={47},
  number={1},
  pages={39--49},
  year={2011},
  publisher={Elsevier}
}

@article{wood14,
author = {Simon Wood and Yannig Goude and Simon Shaw},
title = {Generalized additive models for large data sets},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
volume = {64},
number = {1},
year = {2014},
pages = {139-155},
keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
doi = {10.1111/rssc.12068},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssc.12068},
abstract = {Summary We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.}
}

@article{kantas15,
  title={On particle methods for parameter estimation in state-space models},
  author={Kantas, Nikolas and Doucet, Arnaud and Singh, Sumeetpal S and Maciejowski, Jan and Chopin, Nicolas and others},
  journal={Statistical science},
  volume={30},
  number={3},
  pages={328--351},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}
