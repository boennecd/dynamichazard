---
title: "Comparing UKF with EKF"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

  ## Set the width of the R-terminal to # characters
  options(width = 80, warn = -1)

# Derfine function to make small margin in pictures
# you can use the bool flag to set what to excute before, after chunk 
# or always
# If you do not whant to excute the code than do e.g. small.mar = F
knitr::knit_hooks$set(small.mar = function(before, options, envir){
  if (before){ par(
    mar = c(5,5,0.5,0.5), 
    tcl = -0.3, 
    mgp = c(2.5,.5,0), 
    oma = c(0,0,0,0),
    pch=16,
    cex=.6,
    cex.axis = 1,
    cex.lab = .8/.6,
    lwd= 1
  )}},
  my.options=function(before, options, envir){
    if(before){
      options(digits = 3)
    }})

## opts_chunk$set() can change the default global options in a document (e.g. put this in a code chunk
knitr::opts_chunk$set(fig.path='figures/',
               fig.align='center',
               fig.width=6, fig.height=4,
               out.width="0.7\\textwidth",
               size='tiny',                ## See R highligt package for possible values (https://cran.r-project.org/web/packages/highlight/highlight.pdf)
               small.mar = TRUE,
               my.options = TRUE,
               comment = "##", 
               warnings = T,
               errors = T
)
```

## Introduction
This note will compare the dynamic discrete model for the logistic model fitted with the Extended Kalman filter (EKF) versus with Unscented Kalman filter (UKF). We will use simulated data sets in order to compare the two


### Main findings
* The UKF seems to work 
* The UKF performs better in some cases then the EKF
* The current implementation of the UKF is not scalable and is sensitive to the initial values

### Questions
* Is it a good approach that I use to simulation data to compare with
* Your reflections of the results for both the EKF and the UKF
* What would you find interesting to look into
* Proceed by looking into other sigma points and/or other UKFs like square root-Unscented Kalman filter. Is it worth it? 
* Proceed by looking at the Poisson model instead and keep using the EKF and/or time constant effects
* Do all three but in what order

## Simulation
First, we will need to simulate the data. We start by simulating $\vec{ \beta }_t = \vec{\beta}_{t-1} + \vec{\eta}_t$ where $\vec{\eta}_t \sim N(\vec{0}, \mathbf{Q})$. We then model the death of individual $i$ in period $t$ by $\pi_{it} = \exp (\vec{\beta}^T \vec{x}_{it}) / (1 + \exp (\vec{\beta}^T \vec{x}_{it}))$. We update the co-variate vector for the $i$'th individual with gabs of $1 + z$ where $z \sim \text{Pois} (1)$. We let $x_{itk} \sim \text{Unif}(a,b)$ for given values $a$ and $b$

There is a function to do this in the `R` folder called `test_sim_func_logit`. You can skip down to 'Comparing with one sample' if you trust that I have made the simulation function in accordance with what I described above
```{r}
# We are currenlty in the vignettes folder
getwd()
source("../R./test_utils.R")
test_sim_func_logit
```

The `get_zyx_draw` functions are convenient functions that runs faster than the equivalent sampling function like `rnorm` when we simulate few outcomes many times:
```{r}
# We are currenlty in the vignettes folder
microbenchmark::microbenchmark(
  get_unif_draw(1),
  runif(1),
  times = 1e6
)

microbenchmark::microbenchmark(
  get_unif_draw(10),
  runif(10),
  times = 1e5
)
```

## Comparing with one sample
We simulate as follows:

```{r}
set.seed(20160904) # a good day
beta_start <- 1
n_vars <- 3
sims_args <- list(n_series = (n_series <- 1e3), 
                  n_vars = n_vars, 
                  t_0 = (t_0 <- 0), 
                  t_max = (t_max <- 5),
                  x_range = (x_range <- 1), 
                  x_mean = (x_mean <- .5), 
                  beta_start = beta_start,
                  intercept_start = (intercept_start <- 
                                       - (.5 * n_vars * beta_start + 2)),
                  re_draw = T,
                  sds = (sds <- c(sqrt(.2), rep(1, n_vars))))

sims <- do.call(test_sim_func_logit, sims_args)
sims$res <- as.data.frame(sims$res)
```

We have a total of `r n_series` series, with `r n_vars` parameters and an intercept. The simulated parameters are:
```{r}
sims$betas
matplot(seq_len(nrow(sims$betas)) - 1, sims$betas, type = "l", lty = 1, 
        lwd = c(2, rep(1, n_vars)), col = "black",
        xlab = "time", ylab = "True beta")
```

Further, we start at time `r t_0` and end at time `r t_max` giving us `r t_max - t_0` intervals. The $\vec{\beta}_0$ starts at (`r c(intercept_start, rep(beta_start, n_vars))`). We set the variances of state space variables to [`r sds`]. The variance of the intercept is lower to ensure that it does not wonder of too much. Thus, we end with a lower base line risk of dying with greater certainty Lastly, the co-variates are simulated to be uniformly distributed within [`r x_mean + c(-1, 1) * x_range / 2`]. The first 10 rows of the final data frame are: 
```{r}
head(sims$res, 10)
```

Next, we get the design matrix and risk set object for `ddhazard_fit_cpp_prelim` by using the the following functions from `benssurvutils`:
```{r}
design_mat <- dynamichazard::get_design_matrix(survival::Surv(tstart, tstop, event) ~ x1 + x2 + x3, sims$res)
risk_obj <- dynamichazard::get_risk_obj(design_mat$Y, by = 1, max_T = t_max, id = sims$res$id)
```

The last two lines update the failure flags and stop times. It is needed for the logic in `ddhazard_fit_cpp_prelim` to check in which period a row counts as a death or as a control. For example, say that we have row from time [3, 7.5] with a death at the end. The row is a control period [3, 4), [4, 5), [5, 6) and [6, 7) and a case in period [7, 8)

The final number of series that end with a death within time `r t_max` are:
```{r}
sum(design_mat$Y[, 3] & design_mat$Y[, 2] <= t_max)
```

### Fitting EKF
We start by fitting the EKF. We do it with the following call:

```{r}
arg_list <- list(X = design_mat$X, tstart = design_mat$Y[, 1],  
                 tstop = design_mat$Y[, 2],
                 a_0 = (a_0 <- c(-1, rep(0, n_vars))),
                 Q_0 = (Q_0 <- diag(1, ncol(design_mat$X))),
                 Q = (Q <- diag(1, ncol(design_mat$X))),
                 F_ = diag(1, ncol(design_mat$X)),
                 risk_obj = risk_obj,
                 eps = 10^-2, n_max = 10^4,
                 order_ = 1,
                 est_Q_0 = F,
                 verbose = T,
                 k = 0)

system.time(fit_EKF <- do.call(dynamichazard::ddhazard_fit_cpp_prelim, arg_list))
```

The `arg_list` is saved as we will need it when we estimate with the UKF. We start with $\vec{\beta}_0$ as [`r a_0`], the initial co-variance, $\mathbf{Q}_0$, as a diagonal matrix with elements [`r diag(Q_0)`] and the co-variance matrix in state space equation, $\mathbf{Q}$, as a diagonal matrix with elements [`r diag(Q)`]. Further, we keep $\mathbf{Q}_0$ fixed in each iteration by setting `est_Q_0 = F`. The convergence criteria is the relative norm of the change $\vert\vec{\beta}_0^{(k)} - \vec{\beta}_0^{(k - 1)}\vert/\vert\vec{\beta}_0^{(k - 1)} \vert$ where superscripts denotes the iteration number of the EM algorithm

### Fitting UKF
Next, we fit the same model with the UKF:
```{r}
arg_list$method = "UKF"

system.time(fit_UKF <- do.call(dynamichazard::ddhazard_fit_cpp_prelim, arg_list))
```

Note, the big difference in computation time. We will return to this later. We can compare the two in terms of mean square error of the state space vectors:
```{r}
MSE_func <- function(b, betas = sims$betas) 
  mean((b - betas)^2) 

MSE_func(fit_EKF$a_t_d_s)
MSE_func(fit_UKF$a_t_d_s)
```

Further, we can compare plots:
```{r}
f_plot <- function(UKF, EKF, betas = sims$betas){
  par(mfcol = c(2, 2))
  for(i in 1:4){
    b <- sims$betas[, i]
    b_UKF <- UKF$a_t_d_s[, i]
    b_EKF <- EKF$a_t_d_s[, i]
    plot((seq_len(t_max - t_0 + 1) - 1) ,sims$betas[, i], 
         ylim = range(b, b_UKF, b_EKF), type = "l",
         xlab = "time (t)", ylab = paste("beta", i - 1))
    lines((seq_len(t_max - t_0 + 1) - 1), b_EKF, col = "blue")
    lines((seq_len(t_max - t_0 + 1) - 1), b_UKF, col = "red")
  }
}

f_plot(UKF = fit_UKF, EKF = fit_EKF)
```

The blue line is the EKF and the red line is the UKF. Lastly, we can compare in-sample estimates versus actual outcomes. We call `dynamichazard::ddhazard` in order to use the S3 generic `predict` implemented for the returned class. This method was not used before to illustrate the difference in computation time (we avoid some computation that applies for both methods)
```{r}
arg_list <- list(
  formula = formula(survival::Surv(tstart, tstop, event) ~ x1 + x2 + x3),
  data = sims$res, by = 1, a_0 = a_0, Q_0 = Q_0, id = sims$res$id,
  Q = Q, n_max = 1e3, eps = 1e-2, max_T = t_max, est_Q_0 = F,
  k = 0)

fit_EKF_new <- do.call(dynamichazard::ddhazard, arg_list)

testthat::test_that("Estimates are equal", {
  testthat::expect_equal(fit_EKF$a_t_d_s,  fit_EKF_new$a_t_d_s, check.attributes = F)
})

# New object has more information
str(fit_EKF_new)
str(fit_EKF)

# Compute predicted chance of death and then the Brier score
brier_func <- function(fit, pred_dat = sims$res){
  # We have to set some of the stop times to t_max when predicitng
  pred_dat$tstop = pmin(pred_dat$tstop, t_max)
  
  pred <- predict(fit, new_data = pred_dat, 
                  tstart = "tstart", tstop = "tstop")
  c("Brier score" = mean((pred_dat$event - pred$fits)^2))
}
brier_func(fit_EKF_new)

# Do the same for UKF
arg_list$method <- "UKF"
fit_UKF_new <- do.call(dynamichazard::ddhazard, arg_list)
brier_func(fit_UKF_new)
```

Lastly, we can compare the fits with a logistic model where assume time constant parameters:

```{r}
static_glm <- 
  benssurvutils::static_glm(formula = arg_list$formula, data = arg_list$data, 
                            by = arg_list$by, use_risk_set_to_weight = T)
summary(static_glm)
```

The `use_risk_set_to_weight = T` adds weights to each row computed which is a count how many risk sets the row is in. We cannot use the fitted values directly as some of the observations appears in multiple bins which we have to account for. This is done below:

```{r}
n_bins_apperance <- rep(0, nrow(sims$res))
for(t in 1:t_max){
  is_in_bin <- sims$res$tstart <= t & t <= sims$res$tstop
  n_bins_apperance[is_in_bin] <- n_bins_apperance[is_in_bin] + 1
}

static_glm$pred <- 1 - (1- static_glm$fitted.values)^(n_bins_apperance)

mean((static_glm$pred - sims$res$event)^2) # compute brier score
```

## Comparing more than one simulation
We will simulate a number of times and compare the mean square error for the state space variables and the brier score (you can skip by the following output lines which are included 1) for debugging and 2) to illustrate that UKF fails due to the Cholesky decomposition):

```{r}
n_sim <- 100
outcome <- matrix(NA_real_, nrow = n_sim, ncol = 6, 
                  dimnames = list(NULL, c("EKF MSE", "EKF Brier", "EKF iters",
                                          "UKF MSE", "UKF Brier", "UKF iters")))

for(i in seq_len(n_sim)){
  sims <- do.call(test_sim_func_logit, sims_args)
  arg_list$data <- data.frame(sims$res)
  arg_list$id <- arg_list$data$id
  
  cat("Iteration", i, "has", sum(arg_list$data$event & arg_list$data$tstop <= t_max),
      "deaths\n")
  
  for(method in c("EKF", "UKF"))
    tryCatch({
      arg_list$method <- method
      fit <- do.call(dynamichazard::ddhazard, arg_list)
      outcome[i, paste(method, c("MSE", "Brier", "iters"))] <- 
        c(MSE_func(fit$a_t_d_s, betas = sims$betas), 
          brier_func(fit,pred_dat = arg_list$data),
          fit$n_iter)
    }, error = function(e){
      cat("Failed with method ", method, ". See above for the error message\n")
    })
}
```

The code above simulates `r n_sim` data sets and fit the model using the EKF and UKF. We save the mean square error, Brier score and the number of iterations. The number of iterations is saved to see if one methods exist much earlier than the other due to the convergence criteria. We can look at the outcome and see the mean difference between the EKF and UKF:

```{r}
outcome

# The number complete cases
sum(complete.cases(outcome))

# We also compute trimmed mean where we exclude the x / ... most extreme in 
# either end
trim_fac <- 2 / sum(complete.cases(outcome))

mean(outcome[, c("EKF MSE")] - outcome[, c("UKF MSE")], na.rm = T)
mean(outcome[, c("EKF MSE")] - outcome[, c("UKF MSE")], na.rm = T,
     trim = trim_fac) 
mean(outcome[, c("EKF Brier")] - outcome[, c("UKF Brier")], na.rm = T)
mean(outcome[, c("EKF Brier")] - outcome[, c("UKF Brier")], na.rm = T,
     trim = trim_fac)
```

## Improving UKF
### Computation time
The first issue is the computation time. The current implementation use the method described in Julier, Simon and Uhlmann. "New extension of the Kalman filter to nonlinear systems." AeroSense'97. International Society for Optics and Photonics, 1997. The majority of the computation time is spend computing $\mathbf{P}_{yy}^{-1}$ in every bin (i.e. every risk set). $\mathbf{P}_{yy}^{-1}$ correspondents to $\mathbb{E}\left((\vec{\pi}_{t} - \bar{\vec{\pi}}_{t})(\vec{\pi}_{t} - \bar{\vec{\pi}}_{t})^T\right)$ computed using the sigma points at given time point $t$. $\vec{\pi}$ denotes the probability of dying for the individuals in the risk at time $t$. The dimension of this matrix is equal to the size of the risk set. The current inversion is computed for a generic square matrix. Consequently, it does not scale well when the risk sets' size increase

A way to make the computation quicker is to exploit that:
$$\mathbf{P}_{yy} =\sum_{i = 0}^{2n} w_i \mathbf{H}(\vec{x}_i)
  + \sum_{i = 0}^{2n} w_i (\vec{y}_i -\bar{\vec{y}}) (\vec{y}_i -\bar{\vec{y}})^T$$

Where $\vec{x}_i$ is the $i$'th sigma point and we have a total $2n + 1$ sigma points. In our case, $\vec{x}_i$ will correspond to state space vector. $\mathbf{H}(\vec{x}_i)$ denotes the observable equation's variance matrix given sigma point $\vec{x}_i$. $w_i$ denotes the weight of the $i$'th sigma point. Further, the outcome predicted using the $i$'th sigma point at a given time $t$ for the $j$ individual is:
$$\vec{y}_{ij} = \vec{\pi}_{ij} = \exp(\vec{x}_i^T\vec{z}_{jt}) / (1 + \exp(\vec{x}_i^T\vec{z}_{jt}))$$

Where $\vec{z}_{jt}$ is the $j$'th individual's co-variate vector at time $t$. We are now able to re-write $\mathbf{P}_{yy}$ as:

$$\begin{aligned}
\bar{\vec{\pi}} &= \sum_{i = 0}^{2n} w_i \vec{\pi}_i\\ 
\mathbf{P}_{yy} &=\sum_{i = 0}^{2n} w_i \mathbf{H}(\vec{\pi}_i)
  + \sum_{i = 0}^{2n} w_i (\vec{\pi}_i -\bar{\vec{\pi}}) (\vec{\pi}_i -\bar{\vec{\pi}})^T \\
\mathbf{H}(\vec{\pi}_i)_{lk} &= \left\{ \begin{array}{cc} 
   \pi_{kl}(1 - \pi_{kl}) & k = l \\
   0 & \text{Otherwise}
   \end{array}\right.
\end{aligned}$$

Hence, $\mathbf{P}_{yy}$ can be written as positive definite diagonal matrix added by a sum of outer products of vector that hence have rank one. This allows us to use the recursive algorithm suggested in Miller. "On the inverse of the sum of matrices." Mathematics Magazine 54.2 (1981): 67-72. Details are omitted here but it should be much faster as inverting the initial matrix $\mathbf{H}(\vec{\pi}_i)$ is computationally easy since it is a diagonal matrix

Another problem is that we have to store a matrix of size $\mathbf{P}_{yy}$. Since a double precision requires variable requires $8$ bytes, this mean that if our risk set is size $10^5$ then we will need $8 \cdot 10^{2 \cdot 5}$ yielding 80GB of memory which is infeasible on most desktop machines. This is avoided with the EKF since we only work with matrices of dimension equal to the state space vectors dimension as we use Fahrmiers formulation

Lastly, there is the issue that we fail to compute the Cholesky decomposition for the sigma points. I checked a few of the instance  where UKF method fails and it is due to passed matrix $\mathbf{P}_{xx}$ is not positive definite in the first iteration of the EM-algorithm. The current implementation seems sensitive to the initial choice of $\mathbf{Q}_0$, $\mathbf{Q}$ and $\vec{\beta}_0$. Consequently, it may be beneficial to look into other formulation of the UKF and/or other choices of sigma points

## Caveat
It is not completely fair to compare the computation time of the EKF with UKF in the current implementation. The EKF method uses parallel computation for the score matrix which greatly improves performance. However, the current implementation will still scale better with the EKF as it does not have invert a dense matrix with dimension equal to the size of the risk set. The example below shows how we can easily scale the number of series:
```{r}
arg_list$method = "EKF"
for(n_series in 2^(14:20))
{
  cat("The number of simulated series are: ", n_series, ". The run time is:\n", sep = "")
  sims_args$n_series <- n_series
  
  sims <- do.call(test_sim_func_logit, sims_args)
  arg_list$data <- data.frame(sims$res)
  arg_list$id <- arg_list$data$id
  
  print(system.time(do.call(dynamichazard::ddhazard, arg_list)))
}
```


## Installation
You make skip this part if you not interested in running the above code or getting the above code to look it through. You can obtain the two needed package through github with the following calls:

```{r eval=FALSE}
devtools::install_github("boennecd/benssurvutils")
devtools::install_github("boennecd/dynamichazard")
```

A last note is that the `src/Makevars.win` checks if there is `C:\OpenBLAS` folder. If so, I assume that the structure is:
```
C:/OpenBLAS/
|--lib/
   |--libopenblas.a
|--include/
   |--cblas.h
   |--f77blas.h
   |--openblas_config.h
```

Thus, the code will be compiled with `openBLAS` instead of the `BLAS` library used to compile `R`. This will allow parts of the matrix operations to be run in parallel to. The difference is minor though for the EKF as the most computationally expensive part is computed in parallel regardless as I used the `std::thread` library for this part
