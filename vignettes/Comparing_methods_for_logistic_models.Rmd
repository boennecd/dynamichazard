---
title: "Comparing methods for logistic models"
author: "Benjamin Christoffersen"
date: "31 October 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(
  mySettings  = function(before, options, envir){
    if (before && options$mySettings){ 
      par(
        mar = c(10, 10, 4, 4),
        bty = "n",
        xaxs = "i",
        pch=16,
        cex= (cex <- .4),
        cex.axis = .8 / cex,
        cex.lab = .8 / cex,
        lwd= 1)
      options(digits = 3)
    }})

knitr::opts_chunk$set(echo = TRUE, mySettings=TRUE, fig.height=3.5, fig.width = 6)
```

# Intro

This note will compare the dynamic logistic model in `dynamichazard` with others methods within the package and methods from `timereg` and `mgcv`. The note will serve both as a comparisson with other methods and as an illustration of how to use the `ddhazard` function for the logistic model. We will use the `pbc2` dataset from the `survival` package. The motivation is that it is commonly used in survival analysis for illustrations. Consquently, we have expecation to the outcome of our analysis

The note is structured as follows: First, we cover the `pbc2` data set. Then we estimate two non-dynamic logistic regression models. This is followed by a fit using Generalized Aditive model by using the `gam` function in the `mgcv` package. Next, we will estimate a cox-model with time varying parameters using the `timecox` function in the `timereg` package. Finaly, we will end by illustrating the new methods in this package for time variying parameters in a logistic regression

# The pbc data set
The `pbc` data set consists of data is from the Mayo Clinic trial in primary biliary cirrhosis (TODO: add reference). The particular dataset is not of interest in this note. Rather, we use this dataset to compare with results previously found using analysing it. We will focus on a the subset of co-varaites used in Dynamic regression models for survival data (TODO: add reference and motivate these co-variates). The dataset can be created as follows: 

```{r}
# PBC data set from survival with time variying covariates
# See: https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf
library(survival)
temp <- subset(pbc, id <= 312, select=c(id, sex, time, status, edema, age))
pbc2 <- tmerge(temp, temp, id=id, death = event(time, status))
pbc2 <- tmerge(pbc2, pbcseq, id=id, albumin = tdc(day, albumin),
               protime = tdc(day, protime), bili = tdc(day, bili))
pbc2 <- pbc2[, c("id", "tstart", "tstop", "death", "sex", "edema", 
                 "age", "albumin", "protime", "bili")]
```

which is described in the vignette *Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model* in the `survival` package. The details of `tmerge` is not important in this scope but the code is useful if you want to reproduce the results in this note. The resulting data frame is structered as follows:

```{r}
head(pbc2)
```

The data set is in the "typical" start and stop time setup used for survival analysis. Each individual in the trail has one or more observations. The `id` column is used to identify the individual. The `tstart` column indicates when the row is valid from and the `tstop` column indicates when the row is valid to. The `death` column is the outcome and is 2 when the individual dies at `tstop`. The `sex`, `edema` and `age` are baseline variables while `albumin`, `protime` and `bili` are updated labratory values from follow ups with the patient. As an example, we can look indvidual `282`:

```{r}
(ex <- pbc2[pbc2$id == 282, ])
```

She (`sex` is `f`) has had four laboratory values messaured at time `r ex$tstart[1]`, `r ex$tstart[2]`, `r ex$tstart[3]` and `r ex$tstart[4]`. Further, she does not die as the `death` column is zero in the final row 

# Logistic model
We can start of with a simple logistic model where we completly ignore `tstart` and `tstop` variable using `glm`:

```{r}
glm_simple <- glm(death == 2 ~ age + edema + log(albumin) + log(protime) + 
                    log(bili), binomial, pbc2)

glm_simple$coefficients
```

We may though want to account for the fact that say the second the row of individual 282 has a length of `r (ex$tstop - ex$tstart)[2]` days (`r ex$tstop[2]` minus `r ex$tstart[2]`) while the fourth row has a length `r (ex$tstop - ex$tstart)[4]`. A way to incoporate this information is to bin the observations into periods of a given length. This is for instance what is done in [@tutz16] and [@shumway01]

Say that we set the bin interval lengths to 100 days. Then the first row in our example will yield three observation: one from 0 to 100, one from 100 to 200 and one from 200 to 300. That is, she survives from time 0 to time 100, survives from time 100 to time 200 etc. We can make the data frame with the `get_survival_case_Weigths_and_data` function in this package:

```{r}
library(dynamichazard)
pbc2_big_frame <- get_survival_case_Weigths_and_data(
  Surv(tstart, tstop, death == 2) ~ age + edema + log(albumin) + log(protime) +
    log(bili), data = pbc2, id = pbc2$id, by = 100, max_T = 3600, 
  use_weights = F)
```

The above use the `Surv` function on the left hand site of the formula. The `Surv` function needs a start time, a stop time and a outcome. The right hand site is as before. The `by` argument specifies the interval length (here 100 days) and the `max_T` specify the last time want to include. We will comment on `use_weights` argument shortly. As an example, we can look at individual `282` in the new data frame:

```{r}
pbc2_big_frame[pbc2_big_frame$id == 282, ]
```

Notice that three new columns have been added. `Y` which is the outcome, `t` which is the stop time in the bin and `weights` which is the weight to be used in a regression. We see that the first row in the initial data frame for individual `282` now has three rows. It may be clear to the reader that we could just use weights instead. This is what we get if we set `use_weights = T`:

```{r}
pbc2_small_frame <- get_survival_case_Weigths_and_data(
  Surv(tstart, tstop, death == 2) ~ age + edema + log(albumin) + log(protime) +
    log(bili), data = pbc2, id = pbc2$id, by = 100, max_T = 3600, 
  use_weights = T)
```

The new data rows for individual `282` looks as follows:

```{r}
pbc2_small_frame[pbc2_small_frame$id == 282, ]
```

A minor caveat is how to treat individuals who do die. For instance, take individual `268`:
```{r}
pbc2[pbc2$id == 268, ] # the orginal data
pbc2_small_frame[pbc2_small_frame$id == 268, ] # new data
```

Notice, that we have to add an additional row with weight `1` where `Y = 1` as it would be wrong to give a weight of `10` to a the row with `Y = 1`. She do survive for 11 bins and dies in the 12 bins

Finaly, we can fit the model with the `glm` function using either of the two data frames as follows:
```{r}
glm_fit_big <- glm(Y ~ age + edema + log(albumin) + log(protime) + 
                    log(bili), binomial, pbc2_big_frame)
glm_fit_small <- glm(Y ~ age + edema + log(albumin) + log(protime) + 
                      log(bili), binomial, pbc2_small_frame, 
                     weights = pbc2_small_frame$weights)
```

We can confirm that the two models give the same estimate:
```{r}
all.equal(glm_fit_big$coefficients, glm_fit_small$coefficients)
```

Further, the binning here do affact our estimates as shown below. In particular, it affacts the estimates for `edema` and `log(albumin)`. The standard errors from the simple fit are also printed. However, these standard errors do not account for the dependence as we use multiple observations from the same individual 

```{r}
rbind("glm with bins" = glm_fit_big$coefficients, 
      "glm without bins" = glm_simple$coefficients, 
      "Sds from simple fit" = 
        summary(glm_simple)[["coefficients"]][, "Std. Error"])
```

To end this section, you can skip making data frame with `get_survival_case_Weigths_and_data` by calling the `static_glm` function from `dynamichazard` package. For details, see the help file by calling `?static_glm`

# Generalized Additive Models using mgvc
The first method we will compare with is Generalized Additive Models (GAM) by using the `gam` function in the `mgcv` package. The model we fit is of the form: 

$$\begin{aligned}
\text{logit}(\pi_i) &= \vec{\gamma}_{\text{time}}\vec{f}_{\text{time}}(t_i)
    + \vec{\gamma}_{\text{age}}\vec{f}_{\text{time}}(t_i)a_i
    + \vec{\gamma}_{\text{ede}}\vec{f}_{\text{ede}}(t_i)e_i
    + \vec{\gamma}_{\text{alb}}\vec{f}_{\text{alb}}(t_i)\log al_{it} \\
    &\quad + \vec{\gamma}_{\text{pro}}\vec{f}_{\text{alb}}(t_i)\log p_{it}
    + \vec{\gamma}_{\text{biil}}\vec{f}_{\text{bili}}(t_i)\log b_{it}
\end{aligned}$$
    
where $\pi_{it}$ is the probability that the $i$'th individual dies of cancer, $t$ is the stop time of the bin and $a_i$, $e_i$, $al_{it}$, $p_{it}$ and $b_{it}$ are respecivly the $i$'th individuals age, edema, albumin, protime and bilirunbin. The extra subscript $t$ is added to refer to the level in the bin. $\vec{f}_{\cdot}$ is a basis function. We will use cubic regression splines with knots spread evenly through the covariate values. We fit the model with the following call:

```{r message=FALSE}
library(mgcv, quietly = T)
spline_fit <- gam(
  formula = Y ~ # cr is cubic basis with dim of k
    s(t, bs = "cr", k = 3, by = age) + 
    s(t, bs = "cr", k = 3, by = edema) + 
    s(t, bs = "cr", k = 5, by = log(albumin)) + 
    s(t, bs = "cr", k = 5, by = log(protime)) + 
    s(t, bs = "cr", k = 5, by = log(bili)),
  family = binomial, data = pbc2_big_frame,
  method = "GCV.Cp") # estimate smoothing parameters with generalized cross 
                     # validation  
                  
```    

The above estimates the GAM model where the likelihood is penalised by a quadratical penalty for each spline function. The tuning parameters is chosen by generalized cross validation. As suggested in the help menu in regard to  the number of knots `k`, *"... [the] exact choice of k is not generally critical: it should be chosen to be large enough that you are reasonably sure of having enough degrees of freedom to represent the underlying ‘truth’ reasonably well, but small enough to maintain reasonable computational efficiency"*. See `?gam` or `?choose.k` for details

Below are plots of the estimates. The effective degree of freedom is noted in the parentheses and is computed given the number knots and final tunning parameter for spline function. For instance, `s(t,1.84):...` mean that the effective degress of freedom for that term is `1.84`

```{r, fig.height = 6, mySettings=FALSE}
plot(spline_fit, scale = 0, page = 1, rug = F)
```

Further, we compare the result with static model. Recall that our static model had the following estimates:

```{r}
glm_fit_big$coefficients
```

These do seem to corespond with the plots. Further, the intercept in the spline model is:

```{r}
spline_fit$coefficients["(Intercept)"]
```

which again seems to match. The plot suggest that there may be time variying effects for `bili` particularly

# Time varying cox model from `timereg`
Another method we can try is a time varying effects Cox model. The motivation to look into Cox models is the similarty between the cloglog link function and logit function in discrete outcome. Hence, a Cox regression would be of interesting to compare with. We will start with the Cox model from the package `timereg` based on [@martinussen07]. The model we fit has an instantaneous hazard $\lambda(t)$ given by:

$$\lambda(t) = \lambda_0(t) \exp \left( \vec{x}\vec{\beta}(t) \right)$$

where each margin of $\vec{\beta}(t)$ is estimated recursively with an update equation that is simplified through a first order Taylor expansion and adding a smoothness through weighting the time changes with a uniform continuous kernel.  The `breslow` for the baseline is currently not supported. Instead the baseline is $\lambda_0(t)=\exp(\alpha_0(t))$ where $\alpha_0(t)$ is estimated in a similar to way to $\vec{\beta}(t)$. For details see [@martinussen07]. Below we estimate the model similar to the two previous:

```{r, message=FALSE, fig.height = 6, mySettings=FALSE}
library(timereg)
cox_fit<- timecox(Surv(tstart / 365, tstop / 365, death == 2) ~ age + edema +
                        log(albumin) + const(log(protime)) + log(bili), pbc2,
                  start.time=0, max.time = 3000 / 365, id = pbc2$id, bandwidth = 0.35)

par(mfcol = c(3, 2))
plot(cox_fit)
```

We set last observation time (`max.time`) lower than in the previous model as there were issues with converge. For the same reason we specify the effect of `log(protime)` to be constant (non time variying). We end the above code by plotting the cummulative coeffecients given by $B(t) = \int_0^t \beta(t) dt$. Thus a constant effect should be roughly linear. The `timecox` further provides two test for whether the coeffecient is time invariant or not:

```{r, message=FALSE, fig.height = 6, mySettings=FALSE}
summary(cox_fit)
```

The above test suggest that only `edema` might be "border line" time variying. Hence, an idea could be to look at the change in the cummalitve coeffecient within a period of say of length 1 year:

```{r, message=FALSE, warning=FALSE, fig.height = 6, mySettings=FALSE}
# The cum element contain the time and cumulative timevarying regression 
# coefficient  
head(cox_fit$cum)

# Compute differences
max_less_than = sapply(0:9, function(i) max(which(cox_fit$cum[, "time"] <= i)))
tmp <- cox_fit$cum[max_less_than, ]
diffs <- diff(tmp[, -1]) / diff(tmp[, "time"] )

par(mfcol = c(3, 2))
for(c_name in colnames(diffs))
  plot(diffs[, c_name], xlab = "time", ylab = c_name, type = "l")
```

which seems to be of minor use. Question: Include this or just make inferens based on the previous plot? What they do in the book is only to infer on the cumulative regression functions. One may be able to see that there is a slope change in the bili coeffecient

# ddhazard
## Estimation with Extended Kalman Filter
```{r, fig.height = 6, mySettings=FALSE}
library(dynamichazard)
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ age + edema +
                        log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(10, 6)), Q = diag(rep(0.01, 6)))

par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit, cov_index = i)
```

## Estimation with Unscented Kalman Filter
```{r, fig.height = 6, mySettings=FALSE}
dd_fit_UKF <- ddhazard(Surv(tstart, tstop, death == 2) ~ age +
                         edema + log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(1, 6)), Q = diag(rep(0.01, 6)),
                   control = list(method = "UKF", beta = 0, alpha = 5e-1))

par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit, cov_index = i)
```

## Estimation with fixed effects
```{r, fig.height = 6, mySettings=FALSE}
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ ddFixed(age) + edema +
                        ddFixed(log(albumin)) + ddFixed(log(protime)) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(10, 3)), Q = diag(rep(0.001, 3)), 
                   control = list(LR = 1, eps = 0.02), verbose = 5)

par(mfcol = c(2, 3))
for(i in 1:3)
  plot(dd_fit, cov_index = i)

dd_fit$fixed_effects
```
