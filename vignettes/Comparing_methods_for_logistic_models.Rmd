---
title: "Comparing methods for time varying logistic models"
author: "Benjamin Christoffersen"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{bm}
output: pdf_document
bibliography: pbc_data.bib
vignette: >
  %\VignetteIndexEntry{Comparing methods for time varying logistic models}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(
  mySettings  = function(before, options, envir){
    if (before && options$mySettings){ 
      par(
        mar = c(10, 10, 4, 4),
        bty = "n",
        xaxs = "i",
        pch=16,
        cex= (cex <- .4),
        cex.axis = .8 / cex,
        cex.lab = .8 / cex,
        lwd= 1)
      options(digits = 3, width = 80)
    }})

knitr::opts_chunk$set(echo = TRUE, mySettings=TRUE, fig.height=3.5, fig.width = 6,
                      message = FALSE)
```

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}

# Intro
This note will compare the dynamic logistic model in `dynamichazard` with others methods within the package and methods from the `timereg` and `mgcv` package. The note will serve both as a comparison with other packages and as an illustration of how to use the `ddhazard` function for the logistic model. We will use the `pbc2` dataset from the `survival` package. The motivation is that the `pbc2` data set is commonly used in survival analysis for illustrations. Consequently, we have expectation to the outcome of our analysis. It is suggested first to read the ddhazard vignette to get an introduction to the models and estimation methods in this package

The note is structured as follows: First, we cover the `pbc2` data set. Then we estimate two non-dynamic logistic regression models using `glm`. This is followed by a fit using a Generalized Additive model with the `gam` function in the `mgcv` package. Next, we will estimate a cox-model with time varying parameters using the `timecox` function in the `timereg` package. Finally, we will end by illustrating the new methods in this package for time varying parameters in a logistic regression

You can install the version of the library used to make this vignettes from github with the `devtools` library as follows:

```{r echo=FALSE}
current_sha <- httr::content(
  httr::GET("https://api.github.com/repos/boennecd/dynamichazard/git/refs/heads/master")
  )$object$sha

stopifnot(length(current_sha) > 0 && class(current_sha) == "character")

current_version <- paste0("boennecd/dynamichazard@", current_sha)
```

```{r}
current_version
```

```{r eval=FALSE}
devtools::install_github(current_version)
```

# The pbc data set
The `pbc` data set consists of data is from the Mayo Clinic trial in primary biliary cirrhosis. The particular dataset is not of interest in this note. Rather, we use this dataset to compare with results previously found analyzing the data set. We will focus on a the subset of co-variates used in [@martinussen07]. The dataset can be created as follows: 

```{r}
# PBC data set from survival with time variying covariates
# See: https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf
library(survival)
temp <- subset(pbc, id <= 312, select=c(id, sex, time, status, edema, age))
pbc2 <- tmerge(temp, temp, id=id, death = event(time, status))
pbc2 <- tmerge(pbc2, pbcseq, id=id, albumin = tdc(day, albumin),
               protime = tdc(day, protime), bili = tdc(day, bili))
pbc2 <- pbc2[, c("id", "tstart", "tstop", "death", "sex", "edema", 
                 "age", "albumin", "protime", "bili")]
```

which is described in the vignette *Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model* in the `survival` package. The details of `tmerge` is not important in this scope. However, the code is useful if you want to reproduce the results in this note. The resulting data frame is structured as follows:

```{r}
head(pbc2)
```

The data set is in the "typical" start and stop time format used in survival analysis. Each individual in the trail has one or more observations. The `id` column is used to identify the individual. The `tstart` column indicates when the row is valid from and the `tstop` column indicates when the row is valid to. The `death` column is the outcome which is 2 when the individual dies at `tstop`. The `sex`, `edema` and `age` are baseline variables while `albumin`, `protime` and `bili` are updated laboratory values from follow ups with the patient. As an example, we can look individual `282`:

```{r}
(ex <- pbc2[pbc2$id == 282, ])
```

She (`sex` is `f`) has had four laboratory values measured at time `r ex$tstart[1]`, `r ex$tstart[2]`, `r ex$tstart[3]` and `r ex$tstart[4]`. Further, she does not die as the `death` column is zero in the final row 

# Logistic model
We can start of with a simple logistic model where we ignore `tstart` and `tstop` variable using `glm`:

```{r}
glm_simple <- glm(death == 2 ~ age + edema + log(albumin) + log(protime) + 
                    log(bili), binomial, pbc2)

glm_simple$coefficients
```

Though, we want to account for the fact that say the second the row of individual 282 has a length of `r (ex$tstop - ex$tstart)[2]` days (`r ex$tstop[2]` minus `r ex$tstart[2]`) while the fourth row has a length `r (ex$tstop - ex$tstart)[4]` days. A way to incorporate this information is to bin the observations into periods of a given length. Example of such binning is found in [@tutz16] and [@shumway01]

Say that we set the bin interval lengths to 100 days. Then the first row for `id = 282`  will yield three observation: one from 0 to 100, one from 100 to 200 and one from 200 to 300 (since we know that she survives beyond time 300). That is, she survives from time 0 to time 100, survives from time 100 to time 200 etc. We can make such a data frame with the `get_survival_case_weigths_and_data` function in this package:

```{r}
library(dynamichazard)
pbc2_big_frame <- get_survival_case_weigths_and_data(
  Surv(tstart, tstop, death == 2) ~ age + edema + log(albumin) + log(protime) +
    log(bili), data = pbc2, id = pbc2$id, by = 100, max_T = 3600, 
  use_weights = F)
```

The above use the `Surv` function on the left hand site of the formula. The `Surv` function needs a start time, a stop time and a outcome. The right hand site is as before. The `by` argument specifies the interval length (here 100 days) and the `max_T` specify the last time we want to include. We will comment on `use_weights` argument shortly. As an example, we can look at individual `282` in the new data frame:

```{r}
pbc2_big_frame[pbc2_big_frame$id == 282, ]
```

Notice that three new columns have been added. `Y` which is the outcome, `t` which is the stop time in the bin and `weights` which is the weight to be used in a regression. We see that the first row in the initial data frame for individual `282` has three rows now. It may be clear to the reader that we could just use weights instead. This is what we get if we set `use_weights = T`:

```{r}
pbc2_small_frame <- get_survival_case_weigths_and_data(
  Surv(tstart, tstop, death == 2) ~ age + edema + log(albumin) + log(protime) +
    log(bili), data = pbc2, id = pbc2$id, by = 100, max_T = 3600, 
  use_weights = T)
```

The new data rows for individual `282` looks as follows:

```{r}
pbc2_small_frame[pbc2_small_frame$id == 282, ]
```

A minor caveat is how to treat individuals who do die. For instance, take individual `268`:
```{r}
pbc2[pbc2$id == 268, ] # the orginal data
pbc2_small_frame[pbc2_small_frame$id == 268, ] # new data
```

Notice, that we have to add an additional row with weight `1` where `Y = 1` as it would be wrong to give a weight of `10` to a the row with `Y = 1`. She do survive for 11 bins and dies in the 12th bin

Finally, we can fit the model with the `glm` function using either of the two data frames as follows:
```{r}
glm_fit_big <- glm(Y ~ age + edema + log(albumin) + log(protime) + 
                    log(bili), binomial, pbc2_big_frame)
glm_fit_small <- glm(Y ~ age + edema + log(albumin) + log(protime) + 
                      log(bili), binomial, pbc2_small_frame, 
                     weights = pbc2_small_frame$weights) # <-- we use weights
```

We can confirm that the two models give the same estimate:
```{r}
all.equal(glm_fit_big$coefficients, glm_fit_small$coefficients)
```

Further, the binning do affect our estimates as shown below. In particular, it affects the estimates for `edema` and `log(albumin)`. The standard errors from the simple fit are also printed. However, these standard errors do not account for the dependence as we use multiple observations from the same individual 

```{r}
rbind("glm with bins" = glm_fit_big$coefficients, 
      "glm without bins" = glm_simple$coefficients, 
      "Sds from simple fit" = 
        summary(glm_simple)[["coefficients"]][, "Std. Error"])
```

To end this section, you can skip making data frame with `get_survival_case_weigths_and_data` by calling the `static_glm` function from `dynamichazard` package. For details, see the help file by calling `?static_glm`

# Generalized Additive Models using mgvc
The first method we will compare with is Generalized Additive Models (GAM) by using the `gam` function in the `mgcv` package. The model we fit is of the form: 

$$\begin{aligned}
\text{logit}(\pi_i) &= \vec{\gamma}_{\text{time}}\vec{f}_{\text{time}}(t_i)
    + \vec{\gamma}_{\text{age}}\vec{f}_{\text{time}}(t_i)a_i
    + \vec{\gamma}_{\text{ede}}\vec{f}_{\text{ede}}(t_i)e_i
    + \vec{\gamma}_{\text{alb}}\vec{f}_{\text{alb}}(t_i)\log al_{it} \\
    &\quad + \vec{\gamma}_{\text{pro}}\vec{f}_{\text{alb}}(t_i)\log p_{it}
    + \vec{\gamma}_{\text{biil}}\vec{f}_{\text{bili}}(t_i)\log b_{it}
\end{aligned}$$
    
where $\pi_{it}$ is the probability that the $i$'th individual dies of cancer, $t$ is the stop time of the bin and $a_i$, $e_i$, $al_{it}$, $p_{it}$ and $b_{it}$ are respectively the $i$'th individual's age, edema, albumin, protime and bilirunbin. The extra subscript $t$ is added to refer to the level of the covariate in the bin at time $t$. $\vec{f}_{\cdot}$ is a basis function. We will use cubic regression splines with knots spread evenly through the covariate values. We fit the model with the following call:

```{r message=FALSE}
library(mgcv, quietly = T)
spline_fit <- gam(
  formula = Y ~ 
    # cr is cubic basis with dim of k
    s(t, bs = "cr", k = 3, by = age) + 
    s(t, bs = "cr", k = 3, by = edema) + 
    s(t, bs = "cr", k = 5, by = log(albumin)) + 
    s(t, bs = "cr", k = 5, by = log(protime)) + 
    s(t, bs = "cr", k = 5, by = log(bili)),
  family = binomial, data = pbc2_big_frame,
  method = "GCV.Cp") # estimate smoothing parameters with generalized cross 
                     # validation  
                  
```    

The above estimates the GAM model where the likelihood is penalized by a quadratical penalty for each of the spline functions. The tuning parameters is chosen by generalized cross validation. As suggested in the help menu in regard to  the number of knots `k`, *"... [the] exact choice of k is not generally critical: it should be chosen to be large enough that you are reasonably sure of having enough degrees of freedom to represent the underlying ‘truth’ reasonably well, but small enough to maintain reasonable computational efficiency"*. See `?gam` or `?choose.k` for details

Below are plots of the estimates. The effective degree of freedom is noted in the parentheses on the y axis and is computed given the number knots and final tuning parameter for spline function. For instance, `s(t,1.84):...` means that the effective degrees of freedom for that term is `1.84`

```{r, fig.height = 4.5, mySettings=FALSE}
plot(spline_fit, scale = 0, page = 1, rug = F)
```

Further, we compare the result with static model. Recall that our static model had the following estimates:

```{r}
glm_fit_big$coefficients
```

These do seem to correspond with the plots. Further, the intercept in the spline model is:

```{r}
spline_fit$coefficients["(Intercept)"]
```

which again seems to match. The plot suggest that there may be time varying effects for `bili` particularly

# Time varying cox model from `timereg`
Another method we can try is a time varying effects Cox model. The motivation to compare with a Cox models is the similarity between the cloglog link function and logit function in discrete time. Hence, a Cox regression would be of interesting to compare with as continuous model. We will use the Cox model from the package `timereg` based on [@martinussen07]. The model we fit has an instantaneous hazard $\lambda(t)$ given by:

$$\lambda(t) = \lambda_0(t) \exp \left( \vec{x}\vec{\beta}(t) \right)$$

where each margin of $\vec{\beta}(t)$ is estimated recursively with an update equation. The update equation is simplified through a first order Taylor expansion on the score function and by adding a smoothness by using weighting depending on time changes with a uniform continuous kernel. Thus, it differs from other alternatives in `R` that use splines for the time varying effects (like the `cox.ph` in the `mgcv` package). The baseline is $\lambda_0(t)=\exp(\alpha_0(t))$ where $\alpha_0(t)$ is estimated in a similar to way to $\vec{\beta}(t)$. For details see [@martinussen07] in chapter 6. Below we estimate the model similar to the previous:

```{r, message=FALSE, fig.height = 7, mySettings=FALSE}
library(timereg)
cox_fit<- timecox(Surv(tstart / 365, tstop / 365, death == 2) ~ age + edema +
                        log(albumin) + const(log(protime)) + log(bili), pbc2,
                  start.time=0, 
                  max.time = 3000 / 365, # <-- decreased
                  id = pbc2$id, bandwidth = 0.35)

par(mfcol = c(3, 2))
plot(cox_fit)
```

We set the last observation time (`max.time`) lower than in the previous model as there are issues with converge if we do not. For the same reason we specify the effect of `log(protime)` to be constant (non-time varying). We end the above code by plotting the cumulative coefficients given by $B(t) = \int_0^t \beta(t) dt$. Thus, a constant effect should be roughly linear. The `timecox` packages provides two test for whether the coefficient is time invariant or not:

```{r, message=FALSE, fig.height = 6, mySettings=FALSE}
summary(cox_fit)
```

The above test suggest that only `edema` might be "border line" time varying <!-- A further idea could be to look at the change in the cumulative coefficient within a period of say of length 1 year to get an idea of the coefficient. This is done below: -->

<!--  ```{r, message=FALSE, warning=FALSE, fig.height = 6, mySettings=FALSE}
# The cum element contains the time and cumulative timevarying regression 
# coefficients
head(cox_fit$cum)

# Compute differences
max_less_than = sapply(0:9, function(i) max(which(cox_fit$cum[, "time"] <= i)))
tmp <- cox_fit$cum[max_less_than, ]

# scale by time
diffs <- diff(tmp[, -1]) / diff(tmp[, "time"] )

par(mfcol = c(3, 2))
for(c_name in colnames(diffs))
  plot(diffs[, c_name], xlab = "time", ylab = c_name, type = "l")
``` -->

<!-- which seems to be of minor use. Question: Include this or just make draw conclusions based on the previous plot? What they do in the book is only to infer on the cumulative regression functions -->
  
# Dynamic hazard model
In this section, we will cover the dynamic hazard model with the logistic link function that is implemented in this package. Firstly, we will briefly cover the model. Secondly, we will turn to different ways of designing the model and fitting the model 

The idea is that we discretize the outcomes into $1,2,\dots,d$ bins (or intervals if you prefer this notion). In each bin, we observe whether the individual dies or survives. The state space model we are applying is of the form: 
$$\begin{array}{ll}
    	\vec{y}_t = \vec{z}_t(\vec{\alpha}_t) + \vec{\epsilon}_t \qquad & 
         	\vec{\epsilon}_t \sim (\vec{0}, \mathbf{H}_t(\vec{\alpha}_t))  \\
    	\vec{\alpha}_{t + 1} = \mathbf{F}\vec{\alpha}_t + \mathbf{R}\vec{\eta}_t \qquad & 
         	\vec{\eta}_t \sim N(\vec{0}, \mathbf{Q}) \\
	\end{array}
	, \qquad t = 1,\dots, d$$
	
where $y_{it} \in \{0,1\}$ is an indicator for whether the $i$'th individual dies in interval $t$. $\cdots \sim (a, b)$ denotes a random variable with mean (or mean vector) $a$ and variance (or co-variance matrix) $b$. It needs not to be normally distributed. $z_{it}(\vec{\alpha}_t) = h(\vec{\alpha}_t\vec{x}_{it})$ is the non-linear map from state space variables to mean outcomes where $h$ is the link function. We use the logit model model in this note. Thus, $h(x) = \text{logit}^{-1}(x)$. The current implementation supports $\mathbf{F}$ and $\mathbf{R}$ such that we have first and second order random walk for the state equation

We need to estimate the starting value $\vec{\alpha}_0$ and co-variance matrix $\mathbf{Q}$. This is done with an EM-algorithm. The E-step either use the Extended Kalman filter (EKF) or Unscented Kalman filter (UKF). Both a followed by a smoother. The result is smoothed estimates of $\vec{\alpha}_1,\dots, \vec{\alpha}_d$, smoothed co-variance matrix and smoothed correlation matrices that we need for the M-step to update $\vec{\alpha}_0$ and $\mathbf{Q}$. Further, we assume that $\mathbf{Q}_0 = \kappa \mathbf{I}$ is fixed to a large value $\kappa$ in the examples. For more details see the ddhazard vignette

## Estimation with Extended Kalman Filter
We start by estimating the model using the EKF where we let all coefficients follow a first random walk:

```{r, fig.height = 4.5, mySettings=FALSE}
library(dynamichazard)
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ age + edema +
                        log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(10, 6)), Q = diag(rep(0.001, 6)))

par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit, cov_index = i)
```

We start by estimating the model. The arguments `Q_0` and `Q` corresponds to the co-variance matrix at time zero and the co-variance matrix in the state equation. The latter values is only used in the first iteration of the EM algorithm after which we update $\mathbf{Q}$

Next, we plot the coefficient. Notice that the coefficient are close the estimates we saw previously for the GAM model. A key thing to notice is that `Q_0` is not scaled by the time difference but `Q` is. Hence, `Q` should not change much if we alter the length of the interval. We can illustrate this by fitting a model with a shorter bin lengths:

```{r, warning=FALSE}
# First diagonal of the the current co-variance matrix estimate
(old_mat <- dd_fit$Q) * 1e4 # scale for neater print

# Then we refit the model
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ age + edema +
                        log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 50, max_T = 3600, # <-- by is changed!
                   Q_0 = diag(rep(10, 6)), Q = diag(rep(0.001, 6)))

# We compute the relative difference
diag((old_mat - dd_fit$Q) / old_mat)

```

Apart from the age variable, the rest are fairly comparable. Plot of the estimated coefficient match the previous plots

```{r, fig.height = 4.5, mySettings=FALSE}
par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit, cov_index = i)
```

## Extra iteration in the correction step
Another idea is to take extra iterations in the correction step. The motivation is that this step has the form of a Newton Rapshon algorithm as pointed out in [@fahrmeir92] and [@fahrmeir94]. For more details see the ddhazard vignette. Below, we estimate the model with potentially extra steps in the correction step. 

```{r, fig.height = 4.5, mySettings=FALSE}
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ age + edema +
                        log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600,
                   a_0 = glm_fit_big$coefficients,
                   control = list(NR_eps = 0.1), # <-- epsilon in correction step
                   Q_0 = diag(rep(1, 6)), # <-- decreased Q_0
                   Q = diag(rep(0.001, 6)))

# Plot result
par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit, cov_index = i)
```

First, we run the code with the `NR_eps` element of the list passed to the `control` argument set to something that is finite. The value is the threshold for the relative change of in the state vector in correction step of the EKF. Notice that we decrease `Q_0`. The algorithm diverges if we do not

We end the code above by creating plots of the new estimates. The curves seems to be less smooth. One of the motivation to use extra iterations is that experimental simulation have shown decrease predicted mean square error for the state vector (see the vignette 'Simulation study with logit model')

## Estimation with Unscented Kalman Filter
Another option is to perform the E-step using an unscented Kalman filter. This is done below. We start by setting the initial co-variance matrix $\mat{Q}$ large:
```{r, fig.height = 4.5, mySettings=FALSE}
dd_fit_UKF <- ddhazard(Surv(tstart, tstop, death == 2) ~ age +
                         edema + log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(1, 6)), Q = diag(rep(0.01, 6)),
                   control = list(method = "UKF", beta = 0, alpha = 1,
                                  eps = 0.1, n_max = 1e4))

par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit_UKF, cov_index = i)
```

Clearly, the plots of the estimates are not what we expected. The reason is that $\mat{Q}_0$ is quite large. Recall that it is used to form the sigma points in the first iteration. Hence, we mainly get estimates that are either zero or one when $\mat{Q}_0$ is a diagonal matrix with large entries. You can run the code below to see how the algorithm progress:

```{r, eval=FALSE}
# Not run
tmp_file <- file("pick_some_file_name.txt")
sink(tmp_file)
dd_fit_UKF <- ddhazard(Surv(tstart, tstop, death == 2) ~ age +
                         edema + log(albumin) + log(protime) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600, 
                   Q_0 = diag(rep(1, 6)), Q = diag(rep(0.01, 6)),
                   control = 
                     list(method = "UKF", beta = 0, alpha = 1, 
                          debug = T)) # <-- prints information in each iteration
sink()
close(tmp_file)
```

It will print quite a lot of information and hence it is recommended to use `sink` to write the output to a file. The main take away is that the conditional co-variance matrices accumulate in each iteration will the state vectors does not move. This motivates us to pick $\mat{Q}$ and $\mat{Q}_0$ more carefully. Our estimates from the EKF was:

```{r}
diag(dd_fit$Q)
```

which could motivate us to make the following call: 

```{r, fig.height = 4.5, mySettings=FALSE}
dd_fit_UKF <- ddhazard(
  Surv(tstart, tstop, death == 2) ~ age +
    edema + log(albumin) + log(protime) + log(bili), pbc2,
  id = pbc2$id, by = 100, max_T = 3600, 
  Q_0 = diag(c(0.001, 0.00001, rep(0.001, 4))) * 100, # <-- decreased
  Q = diag(rep(0.0001, 6)),                           # <-- decreased
  control = 
    list(method = "UKF", beta = 0, alpha = 1, eps = 0.01))

par(mfcol = c(2, 3))
for(i in 1:6)
  plot(dd_fit_UKF, cov_index = i)
```

This is comparable to the fits from the EKF and GAM model. The main point here is:

1. The UKF may work for certain data set. It may require "more care"" for some data sets
2. The algorithm is sensitive to the choice of $\mat{Q}$ and $\mat{Q}_0$. Further, there is dependence on hyper parameters $\alpha$, $\kappa$ and $\beta$ which we have not explored
3. The output you get by setting `debug` in the list passed to the `control` argument can be useful. Combine this with `sink` because the output may be long
4. The UKF has shown better performs than the EKF previously. Examples includes [@romanenko04], [@kandepu08], [@julier04], [@wan00] and chapter 11 of [@durbin12]

## Estimation with fixed effects
We may want to fit a model where we assume that some of the co-variates are fixed. For instance, we may want to fit a model where `age`, the intercept and `log(albumin)` are fixed. This is done below:
```{r, warning=FALSE}
dd_fit <- ddhazard(
  Surv(tstart, tstop, death == 2) ~ -1 + ddFixed(rep(1, length(age))) + 
    ddFixed(age) + ddFixed(log(albumin)) + edema +ddFixed(log(protime)) + log(bili), 
  pbc2, id = pbc2$id, by = 100, max_T = 3600, 
  Q_0 = diag(rep(1, 2)), Q = diag(rep(0.0001, 2)), control = list(eps = 0.02))
```


```{r, fig.height = 3.5, mySettings=FALSE}
par(mfcol = c(1, 2))
for(i in 1:2)
  plot(dd_fit, cov_index = i)
```

The estimates seems similar although the confidence bounds are slightly narrower. Moreover, the fixed effects are in agreement with the previous fits (they are printed below):

```{r, fig.height = 6}
dd_fit$fixed_effects
```

We can also try to both fix some of terms and potentially take extra iterations in the correction step. This is done below:

```{r, fig.height = 3.5, mySettings=FALSE}
dd_fit <- ddhazard(
  Surv(tstart, tstop, death == 2) ~ -1 + ddFixed(rep(1, length(age))) + 
    ddFixed(age) + ddFixed(log(albumin)) + edema +ddFixed(log(protime)) + log(bili), 
  pbc2, id = pbc2$id, by = 100, max_T = 3600, 
  Q_0 = diag(rep(1, 2)), Q = diag(rep(0.0001, 2)), 
  control = list(eps = 0.02, NR_eps = 0.1))

par(mfcol = c(1, 2))
for(i in 1:2)
  plot(dd_fit, cov_index = i)
```

We see that the curve is less smooth similar to what we saw previously

## Second order random walk

We will end by fitting a second order random walk to model where only `log(bili)` effect is time varying. The motivation is that the second order random walk tend to diverge more easily especially for small data sets. Further, the previous models seems to suggest that it is the only covariate where we may have time varying coefficient. We fit the model below:

```{r}
dd_fit <- ddhazard(Surv(tstart, tstop, death == 2) ~ -1 + 
                     ddFixed(rep(1, length(age))) + ddFixed(age) + 
                     ddFixed(edema) + ddFixed(log(albumin)) + 
                     ddFixed(log(protime)) + log(bili), pbc2,
                   id = pbc2$id, by = 100, max_T = 3600,
                   order = 2,             # <-- second order
                   Q_0 = diag(c(10, 10)), # <-- needs more elements
                   Q = diag(c(0.001, 0))) # <-- same

plot(dd_fit, cov_index = 1)

dd_fit$fixed_effects
```

We plot the curve after the fit. We see that the estimate is more smooth as expected with a second order random walk. Further, we can confirm that the fixed effects are comparable with our previous fits


<!-- ## Residuals and predict
Question: Is this note too long now or should I also show how to compute residuals and predict outcomes? What about doing a K-fold cross validation here? -->

# Summary
We have estimated a model using Generalized additive models with the `mgcv` package and a time varying Cox model with the `timereg` package. Further, we have illustrated how the `ddhazard` function in the `dynamichazard` package can be used. In particular, we have used the logit link function and showed how to estimate with fixed effects, taking extra Newton Raphson steps in the correction step, fitting second order random walks and using the Unscented Kalman filter in the E-step. All the fits have been comparable with the Generalized Additive model

An unaddressed question is why you should this package. Some of the advantageous of the state space model here are among other: 

1. You can extrapolate beyond the last observation time
2. The implementation is linear in time complexity with the number of observations. Further, the Extended Kalman filter computes the correction step in parallel. Consequently, it is very fast even for large number observations
3. All estimation is made in `c++` with use of the `Armadillo` library. Thus, an optimized version of `Blas` or `Lapack` can decrease the computation
4. It is an alternative to spline models. This relates to 1. The main tools for including time varying coefficient in `R` seems to apply of splines. Thus, this is an alternative estimation method


