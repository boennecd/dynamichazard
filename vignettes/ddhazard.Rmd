---
title: "ddhazard"
output: 
  pdf_document: 
    fig_caption: yes
author: "Benjamin Christoffersen"
header-includes:
   - \usepackage{bm}
bibliography: ddhazard_bibliography.bib
---

```{r setup, include=FALSE}
options(digist = 4)
knitr::opts_chunk$set(echo = TRUE)
```

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%
\newcommand{\Lparen}[1]{\left( #1\right)} 
\newcommand{\Lbrack}[1]{\left[ #1\right]} 
\newcommand{\Lbrace}[1]{\left \{ #1\right \}} 
\newcommand{\Lceil}[1]{\left \lceil #1\right \rceil}
\newcommand{\Lfloor}[1]{\left \lfloor #1\right \rfloor}
%
\newcommand{\propp}[1]{P\Lparen{#1}}
\newcommand{\proppCond}[2]{P\Lparen{\left. #1 \right\vert  #2}}
%
\newcommand{\expecp}[1]{E\Lparen{#1}}
\newcommand{\expecpCond}[2]{E\Lparen{\left. #1  \right\vert  #2}}
%
\newcommand{\varp}[1]{\textrm{Var}\Lparen{#1}}
\newcommand{\varpCond}[2]{\textrm{Var} \Lparen{\left. #1 \right\vert  #2}}
%
\newcommand{\corpCond}[2]{\textrm{Cor} \Lparen{\left. #1 \right\vert  #2}}
%
\newcommand{\covp}[1]{\textrm{Cov} \Lparen{#1}}
%
\newcommand{\emNotee}[3]{#1_{\left. #2 \right\vert #3}}
\newcommand{\emNote}[4]{#1_{\left. #2 \right\vert #3}^{(#4)}} 
%
\newcommand{\ukfNote}[2]{\mat{P}_{\vec{#1}, \vec{#2}}}
\newcommand{\ukfNotee}[3]{\mat{P}_{\vec{#1}_{#3}, \vec{#2}_{#3}}}
%
\newcommand{\diag}[1]{\text{diag}{\Lparen{#1}}}
\newcommand{\wvec}[1]{\widehat{\vec{#1}}}
\newcommand{\wmat}[1]{\widehat{\mat{#1}}}
%
\newcommand{\deter}[1]{\left| #1 \right|}
%
\newcommand{\MyInd}[2]{\Lparen{#1}_{#2}}

# Intro
This note will cover the `ddhazard` function in `dynamichazard` library. You can install the the library through github for the `devtools` library as follows:

```{r eval=FALSE}
devtools::install_github("boennecd/dynamichazard")
```

The `ddhazard` function estimates a binary regression model where the parameters are assumed to time-variant and follow a pre-defined random walk. The function is implemented such that: 

1) The time complexity of the computation is linear in the number of observations
2) The dimension of the observation equation (this will be defined soon) can vary through time
3) It is fast due and scalable due the `C++` implementation which uses `Armadillo` library and use of multithreading through the standard library `thread` 

We will briefly introduce the in model in the following paragraphs. Let $\vec{x}_{it}$ denote the co-variate vector for individual $i$ at time $t$ and let $Y_{it}$ be the random variable for whether the $i$'th individual dies within time $(t-1, t]$. Denote the parameters at time $t$ by $\vec{\alpha}_t$. For given parameters at time $t$ the probability of death is:

$$\begin{aligned}
  \vec{y}_{it} = (y_{i1}, \dots, y_{it})^T, 
  \quad \mat{X}_{t} = \Lparen{\vec{x}_{i1}^T, \dots, \vec{x}_{it}^T}^T \\
  \proppCond{Y_{it} = 1}{\vec{y}_{i,t-1}, \mat{X}_{t}, \vec{\alpha}_t} = h(\vec{\alpha}_t^T \vec{x}_{it})
\end{aligned}$$

where $h$ is the link function. For example, this could be the logistic function such that $H(\eta) = \exp(\eta) / (1 + \exp(\eta))$.

The `ddhazard` function estimates models in the state space form:
$$\begin{array}{ll}
  \vec{y}_t = \vec{z}_t(\vec{\alpha}_t) + \vec{\epsilon}_t \qquad & 
  \vec{\epsilon}_t \sim (\vec{0}, \mat{H}_t(\vec{\alpha}_t))  \\
  \vec{\alpha}_{t + 1} = \mat{F}\vec{\alpha}_t + \mat{R}\vec{\eta}_t \qquad & 
  \vec{\eta}_t \sim N(\vec{0}, \mat{Q}) \\
\end{array} , \qquad t = 1,\dots, n$$

$\vec{y}_t$ is the binary outcomes and the associated equation is the *observational equation*. $\sim (a,b)$ denotes a random variable(s) with mean (vector) $a$ and variance (co-variance matrix) b. It needs not be a normal distribution. $\vec{\alpha}_t$ is the state vector with the corresponding *state equation*

The mean $\vec{z}_t(\vec{\alpha}_t)$ and variance $\mat{H}(\vec{\alpha}_t)$ are state dependent with:
$$\begin{aligned}
  z_{it}(\vec{\alpha}_t)=\expecpCond{Y_{it}}{\vec{\alpha}_t} = h(\vec{\alpha}_t^T \vec{x}_{it}) \\
  H_{ijt}(\vec{\alpha}_t) = \left\{\begin{matrix}
    \varpCond{Y_{it}}{\vec{\alpha}_t} & i = j \\ 0 & \textrm{otherwise}
  \end{matrix}\right.
\end{aligned}$$

The state equation is implemented with a 1. and 2. order random walk. For the first order random walk $\mat{F} = \mat{R} = \mat{I}_m$ where $m$ is the number of regression parameters and $\mat{I}_m$ is the identity matrix with dimension $m$. As for the second order random walk, we have:
$$\mat{F} = \begin{pmatrix} 
  2\mat{I}_m & - \mat{I}_m \\ \mat{0}_m & \mat{I}_m
\end{pmatrix},  \qquad 
\mat{R} = \begin{pmatrix} \mat{I}_m \\ \mat{0}_m \end{pmatrix}$$

where $\mat{0}_m$ is a $m\times m$ matrix with zeros in all entries. The models are estimated by an Extended Kalman filter (EKF) or an Unscented Kalman filter (UKF). The method is chosen by passing a list to the `control` argument of `ddhazard` with `list(method = "EKF", ...)` or `list(method = "UKF", ...)` respectively

Either methods require a vector $\vec{\alpha}_0$, co-variance matrix $\mat{Q}$ and co-variance matrix $\mat{Q}_0$ to start the filters. They can be estimated with an EM-algorithm as suggested in  [@fahrmeir94]. [@fahrmeir94] states that $\mat{Q}_0$ can be estimated to. Though, it does not seem possible and doing as [@fahrmeir94] will make all the elements tend towards zero. Hence, the default is not to estimate the co-variance matrix $\mat{Q}_0$ and only estimate the state vector $\vec{\alpha}_0$ and co-variance matrix $\mat{Q}$. You can estimate $\mat{Q}_0$ in the way [@fahrmeir94] describes by setting the `est_Q_0` element of the `control` to `TRUE` (`list(est_Q_0 = T, ...)`)

A key thing to notice is that the `Q` argument for $\mat{Q}$ is scaled by the length of the bins. That is, say that we call `ddhazard` with `by = 10` and pass `Q = diag(rep(1, 4))` if we have four co-variates. Then the $\mat{Q}$ in the in the state equation is $\mat{Q} = 10 \cdot \text{diag}((1, 1, 1, 1))$ where $\text{diag}(\cdot)$ is a function that takes a vector and return a diagonal matrix with the vector in the diagonal. The motivation for this behavior is that you can alter the `by` argument and get comparable estimates of $\mat{Q}$. Further, it will also be usefull if unequal bin length are implemented later. As a last comment in this context, `Q_0` is not scaled and thus will exactly match $\mat{Q}_0$ in the estimation

The rest of this note is structured as follows. The section 'Example of usage' will show how to quickly fit a model. This is followed by the section 'EM algorithm' where the EM algorithm will be explained. The sections 'Extended Kalman Filter' and 'Unscented Kalman Filter' respectively covers the EKF and UKF used in the E-step of the EM algorithm. Finally, we end with the sections 'Logistic model' and 'Exponential model' which cover the models

# Example of usage
Question: Should I include a a quick example here or leave examples to a separate vignettes? 

# EM algorithm
An EM algorithm is used to estimate the initial state space vector $\vec{\alpha}_0$ and the co-variance matrix $\mat{Q}$. Optionally $\mat{Q}_0$ is also estimated if `control = list(est_Q_0 = T, ...)`. We will need a short hand for the conditional means and co-variances to ease the notation. Define

$$\emNotee{\vec{a}}{t}{k} = \expecpCond{\vec{\alpha}_t}{\mat{Y}_k},  \qquad
\emNotee{\mat{V}}{t}{k} = \expecpCond{\mat{V}_t}{\mat{Y}_k}, \qquad  \mat{Y}_k = (\vec{y}_{1k}, \dots, \vec{y}_{sk}) $$

for the conditional mean and co-variance matrix where $s$ is the number of observations. Notice that the letter 'a' is used for estimates while alpha is used for the unknown state. Further, these can both be filter estimates in the case where $k \leq t$ and smoothed estimates when $k > t$. We suppress the dependence of the co-variates ($\vec{x}_{it}$) here to simplify the notation

The initial values for $\vec{\alpha}_0$, $\mat{Q}$ and $\mat{Q}_0$ can be set by passing a vector to `a_0` argument of `ddhazard` for $\vec{\alpha}_0$ and matrices to `Q_0` and `Q` argument of `ddhazard` for respectively $\mat{Q}_0$ and $\mat{Q}$ 

## E-step
The outcome of the E-step is smoothed estimates:
$$\emNote{\vec{a}}{i}{T}{k},  \quad 
  \emNote{\mat{V}}{i}{T}{k}, \quad 
  \mat{B}_j^{(k)} = \emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}\emNotee{\mat{V}}{t}{t - 1}^{-1},  \quad 
  i=0,1,\dots,T\wedge j = 1,2,\dots,T$$
  
where $T$ is the end of the last period we observe and superscripts $\cdot^{(k)}$ is used to distinguish the estimates from each iteration of the EM-algorithm. The required input to start the E-step is an initial mean vector $\widehat{\vec{a}}_0^{(k-1)}$ and co-variance matrices $\widehat{\mat{Q}}_0^{(k - 1)}$ and $\widehat{\mat{Q}}^{(k - 1)}$. Given these input, we compute the following estimates either by using the EKF or UKF:
$$\emNotee{\vec{a}}{j}{j-1}, \quad 
  \emNotee{\vec{a}}{i}{i}, \quad 
  \emNotee{\mat{V}}{j}{j - 1}, \quad 
  \emNotee{\mat{V}}{i}{i}, \quad i =0,1,\dots,T\wedge j=1,2,\dots,T$$
  
Then the estimates are smoothed by computing:
$$\begin{aligned}
  & \mat{B}_t^{(k)} = \emNotee{\mat{V}}{t - 1}{t - 1} \mat{F} \emNotee{\mat{V}}{t}{t - 1}^{-1} \\
  & \emNote{\vec{a}}{t - 1}{d}{k} = \emNotee{\vec{a}}{t - 1}{t - 1} + \mat{B}_t (
    \emNote{\vec{a}}{t}{d}{k} - \emNotee{\vec{a}}{t}{t - 1}) \\
  & \emNote{\mat{V}}{t - 1}{d}{k} = \emNotee{\mat{V}}{t - 1}{t - 1} + \mat{B}_t (
    \emNote{\mat{V}}{t}{d}{k} - \emNotee{\mat{V}}{t}{t - 1}) \mat{B}_t^T
\end{aligned} \qquad t = T,T-1,\dots, 1$$

## M-step
The M-step updates the mean $\widehat{\vec{a}}_0^{(k - 1)}$ and co-variance matrices $\widehat{\mat{Q}}_0^{(k - 1)}$ and $\widehat{\mat{Q}}_0^{(k - 1)}$ (the latter being optional). These are computed by:
$$\begin{aligned}
\widehat{\vec{\alpha}}_0^{(k)} &= \emNote{\vec{a}}{0}{d}{k}, \qquad
    \widehat{\mat{Q}}_0^{(k)} = \emNote{\mat{V}}{0}{d}{k} \\
  \widehat{\mat{Q}}^{(k)} &= \frac{1}{d}	\sum_{t = 1}^d \mat{R}^T\left( 
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^T \right. \\
    &\hspace{57pt} + \emNote{\mat{V}}{t}{d}{k} - \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} - 
      \left( \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} \right) ^T + 
      \mat{F}\emNote{\mat{V}}{t - 1}{d}{k}\mat{F}^T
      \left. \vphantom{\left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^T} \right)\mat{R}
\end{aligned}$$

We test the relative norm of the change in the initial state vector to check for convergence. The convergence criteria is given by $\rVert \widehat{\vec{\alpha}}_0^{(k)} - \widehat{\vec{\alpha}}_0^{(k - 1)} \lVert / (\rVert \widehat{\vec{\alpha}}_0^{(k - 1)} \lVert + \delta) < \epsilon$  where $\delta$ is a small number and $\epsilon$ is a pre-specified threshold. You can select $\epsilon$ by changing setting the `eps` element of the list passed to the `control` argument of `ddhazard` (e.g. `list(eps = 0.01, ...)`)

# Kalman Filter
The standard Kalman filter is carried out by recursively doing two steps. This also applies for the EKF and UKF. Thus, this paragraph is included to introduce general notions. The first step is in the Kalman Filter is the *filter step* where we estimate $\emNotee{\vec{a}}{t}{t - 1}$ and $\emNotee{\mat{V}}{t}{t - 1}$ based on $\emNotee{\vec{a}}{t - 1}{t - 1}$ and $\emNotee{\mat{V}}{t - 1}{t - 1}$. Secondly, we carny out the *correction step* where we estimate $\emNotee{\vec{a}}{t}{t}$ and $\emNotee{\mat{V}}{t}{t}$ based on $\emNotee{\vec{a}}{t}{t - 1}$ and $\emNotee{\mat{V}}{t}{t - 1}$ and the observations. We then repeat the process until $t=T$

# Extended Kalman Filter
The idea for the Extended Kalman filter is to replace the observational equation with a first order Taylor expansion. This approximated model can then be estimated with a regular Kalman Filter. The EKF presented here is originally described in [@fahrmeir94] and [@fahrmeir92] 

The formulation in [@fahrmeir94] differs from the standard Kalman Filter by re-writing the correction step using the Woodbury matrix identity. This has two computational advantages. The first one is that the time complexity is $O(p)$ instead of $O(p^3)$ where $p$ denotes the dimension of the observation equation. Secondly, we do not have store an intermediate $p\times p$ matrix

The EKF starts with filter step where we compute:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}, \quad \\
  \emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^T + \mat{R}\mat{Q}\mat{R}^T
\end{aligned}$$
	  
Secondly, we perform the correction step by:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1} + \emNotee{\mat{V}}{t}{t}\vec{u}_t (\emNotee{\vec{a}}{t}{t - 1})\\
  \emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\emNotee{\vec{a}}{t}{t - 1})\right)^{-1}
\end{aligned}$$

where $\vec{u}_t (\emNotee{\vec{a}}{t}{t - 1})$ and $\mat{U}_t (\emNotee{\vec{a}}{t}{t - 1})$ are given by:

$$\begin{aligned}
  & \vec{u}_t (\vec{\alpha}_t) = \sum_{i \in R_t} \vec{u}_{it} (\vec{\alpha}_t) \nonumber \\
  & \vec{u}_{it} (\vec{\alpha}_t)= \left. \vec{z}_{it} \frac{\partial h(\eta)/ \partial \eta}{H_{iit}(\vec{\alpha}_t)} \Lparen{y_{it} - h(\eta)} \right\vert_{\eta = \vec{z}_{it}^T \vec{\alpha}_t} \\
	& \mat{U}_t (\vec{\alpha}_t) = \sum_{i \in R_t} \mat{U}_{it} (\vec{\alpha}_t) \nonumber \\
	& \mat{U}_{it} (\vec{\alpha}_t) = \left. \vec{z}_{it} \vec{z}_{it}^T 
		 \frac{\left( \partial h(\eta)/ \partial \eta \right)^2}{H_{iit}(\vec{\alpha}_t)} \right\vert_{\eta = \vec{z}_{it}^T \vec{\alpha}_t}
\end{aligned}$$

$R_t$ is the set of indices of individuals who are add risk at time $t$. It is commonly referred to as the risk set. Thus, the dimension the observational equation can vary as individual dies or are right censored

## Divergence
Initial testing shows that the EKF has issues with divergence for some data set. The cause of divergence seems to be overstepping in the correction step where we update $\emNotee{\vec{a}}{t}{t}$. In particular, the signs of the margins of $\emNotee{\vec{a}}{t}{t}$ tends to alter between $t-1, t, t+1$ etc. and the absolute values tends to increase when the algorithm diverges.  The following section describes a solution to this issue

[@fahrmeir92] mentions that the filter step can be viewed as a single Fischer Scoring step (and hence a step in a Newton Raphson method). This motivates:

1) To take multiple steps if $\emNotee{\vec{a}}{t}{t}$ is far from $\emNotee{\vec{a}}{t}{t-1}$
2) Introduce a learning rate

Simulated datasets show that the learning rate solves the issues with divergence. Let $l>0$ denote the learning rate and $\epsilon_\text{NR}$ denote the tolerance of the for the scoring step. We then set $\vec{a} = \emNotee{\vec{a}}{t}{t-1}$ and compute:

$$\begin{aligned}
  &\emNotee{\vec{a}}{t}{t} = \vec{a} + l \cdot \emNotee{\mat{V}}{t}{t}\vec{u}_t (\vec{a})\\
  &\emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\vec{a})\right)^{-1} \\
  &\text{if } \rVert \emNotee{\vec{a}}{t}{t} - \vec{a} \lVert / (\rVert \vec{a} \lVert + \delta) < \epsilon_{\text{NR}} 
    \text{ then exit}\\
  &\text{else set } \vec{a} = \emNotee{\vec{a}}{t}{t} \text{ and repeat} 
\end{aligned}$$

where $\delta$ is small like $10^{-9}$. The defaults are $l = 1$ and $\epsilon_{\text{NR}} = \infty$. Selecting $l < 1$ in case of divergence seems to help. Further, while [@fahrmeir92] does not observe improvements with multiple repetitions, we find improvements in terms of mean square error of the state vector by taking multiple steps (for example by setting $\epsilon_{\text{NR}} = 10^{-2}$ or lower)

$l$ and $\epsilon_{\text{NR}}$ are set by respectively setting the elements `LR` and `NR_eps` of the list passed to `control` argument of `ddhazard`. By default, `LR = 1` and `NR_eps = NULL` which yields a learning rate of 1 and a single Fischer scoring step (which is the same as $\epsilon_{\text{NR}=\infty}$. These arguments can be altered by setting `control = list(LR = .75, NR_eps = 0.001)` for a learning rate of 0.75 and a threshold in the Fischer Scoring of $10^{-3}$

## Parallel BLAS or LAPACK
All the computations use objects from the `Armadillo` library. Thus, an optimized version LAPACK and BLAS can speed up the computation

A multithreaded version of LAPACK or BLAS can cause issues with performance. The majority of the computation time is spend in the correction step of the EKF when have to compute $\vec{u}_t (\vec{\alpha}_t)$ and $\mat{U}_t (\vec{\alpha}_t)$ when the number of regression parameter is low and we have a lot of observations. For this reason, this part of the code is computed in parallel with the standard library 'thread'. The reduction in computation time can mitigated if multithreaded version of LAPLACK or BLAS is used as the code already use multithreading

A very specific solution to the issues is implemented for windows users who compiles with openBLAS. the `src/Makevars.win` checks if there is `C:\OpenBLAS` folder. If so, we assume that the structure is:
```
C:/OpenBLAS/
|--lib/
   |--libopenblas.a
|--include/
   |--cblas.h
   |--f77blas.h
```

The code will be compiled with this `openBLAS` instead of the `BLAS` library used to compile `R`. This will allow parts of the matrix operations to be run in parallel to `openBLAS` for multithreading. The number of threads openBLAS will use is set to 1 before the part that use `thread` is run and reset after the this part is completed

## Recommandations
In general, choosing $\mat{Q}_t = k\mat{I}$ with a large value $k$ and Choose large $\mat{Q} = d\mat{I}$ for $d$ small seems to work for a lot of data set. The exact value of $k$ and $d$ seems not to be too important. For instance, one dataset yielded almost the same result in terms of mean square error when $k \in [1, 10^3]$ and $d \in [10^{-6},10^{-1}]$


# Uncented Kalman Filter
The UKF selects state vectors called *sigma point* with given *sigma weigths* such that to match the moments of  observational equation. The motivation to use the UKF in place of the EKF as we avoid linerization error in the EKF and choose hyperparameters to approximate moments of the observational equation. [@julier97] introduces a UKF that approximate the first two moments and up to fourth moment in certain settings. [@julier04] further develops the UKF and extend to what is later referred to as *the Scaled Unscented Transformation*. We will cover the the Scaled Unscented Transformation.

## Usual UKF formulation
We start by introducing a common notation used in the UKF. Let:
$$\ukfNotee{a}{b}{t}= \expecpCond{(\vec{a}_t - \overline{\vec{a}}_t)(\vec{b}_t - \overline{\vec{b}}_t)^T}{\mat{Y}_t}$$

$\ukfNote{\cdot}{\cdot}$ is useful short hand for the expected correlation matrices and expected co-variance matrix. Further, notice that $\mat{P}_{\vec{\alpha}_t, \vec{\alpha}_t} = \emNotee{\mat{V}}{t}{t}$. The UKF method proceeds as follows: We are given estimates $\emNotee{\vec{a}}{t - 1}{t - 1}$ and $\emNotee{\mat{V}}{t - 1}{t - 1}$. We then select $2m + 1$ *sigma points* (where $m$ is the dimension of the state equation) denoted by $\wvec{a}_0, \wvec{a}_1, \dots, \wvec{a}_{2m + 1}$ according to:

$$\begin{aligned}
  \wvec{a}_0 &= \mat{F}\emNotee{\vec{a}}{t-1}{t-1} \\
  \wvec{a}_{i} &= \mat{F}\left(\emNotee{\vec{a}}{t-1}{t-1} + \sqrt{m + \lambda} \left(\sqrt{\emNotee{\mat{V}}{t - 1}{t - 1}}\right)_i\right) \\
  \wvec{a}_{i + m} &= \mat{F}\left(\emNotee{\vec{a}}{t-1}{t - 1} - \sqrt{m + \lambda} \left(\sqrt{\emNotee{\mat{V}}{t - 1}{t - 1}}\right)_i\right)
\end{aligned} \qquad i = 1,2,\dots, n$$

where $\left(\sqrt{\emNotee{\mat{V}}{t - 1}{t - 1}}\right)_i$ is the $i$'th column of the lower triangular matrix of the Cholesky decomposition of $\emNotee{\mat{V}}{t - 1}{t - 1}$. We assign the following weights to each sigma point (we will cover selection of the hyperparameters $\alpha$, $\beta$ and $\kappa$ shortly):
$$\begin{aligned} 
  W_0^{(m)} &= \frac{\lambda}{m + \lambda} \\
  W_0^{(c)} &= \frac{\lambda}{m + \lambda} + 1 - \alpha^2 + \beta \\
  W_i^{(m)} &= W_0^{(c)} = \frac{1}{2(m+\lambda)}, \qquad i = 1,\dots, 2m \\
  \lambda &= \alpha^2 (m + \kappa) - m
\end{aligned}$$

Let $\vec{W}^{(j)} = (W_0^{(j)}, \dots, W_{2m}^{(j)})^T$ and $\wmat{A} = (\wvec{a}_0,\dots,\wvec{a}_{2m})$ The filter step given the sigma points and sigma weights is:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t-1} &= \sum_{i = 0}^{2m} W_i^{(m)} \wvec{a}_i \\
  \Delta\wmat{A} &= \wmat{A} - \emNotee{\vec{a}}{t}{t-1} \vec{1}^T \\
  \emNotee{\mat{V}}{t}{t-1} &= \mat{R}\mat{Q}\mat{R}^T + \sum_{i=0}^{2m} W_i^{(c)}
    (\mat{F}\wvec{a}_i - \emNotee{\vec{a}}{t}{t-1})(\wvec{a}_i - \emNotee{\vec{a}}{t}{t-1})^T \\
  & = \mat{R}\mat{Q}\mat{R}^T + \Delta\wmat{A}\diag{\vec{W}^{(c)}}\Delta\wmat{A}^T
\end{aligned}$$

where $\diag{\cdot}$ returns a diagonal matrix with the passed vectors values in the diagonal and $\vec{1}$ is a vector with one in all the entries. We then proceed to the correction step. We start by defining the following intermediates: 

$$\begin{aligned}
  \wvec{y}_i &= \vec{z}_t \left(\wvec{a}_i \right), \qquad i = 0,1,\dots, 2m \\
  \wmat{Y} &= (\wvec{y}_0, \dots, \wvec{y}_{2m}) \\
  \overline{\vec{y}} &= \sum_{i = 0}^{2m} W_i^{(m)} \vec{y}_i, \qquad
  \Delta\wmat{Y} = \wmat{Y} - \overline{\vec{y}} \vec{1}^T, \qquad 
  \wmat{H} = \sum_{i=0}^m W_i^{(c)}\mat{H}_t(\wmat{a}_i) \\
  \ukfNotee{y}{y}{t} &= \sum_{i=0}^m W_i^{(c)} \left(
    (\wvec{y}_i - \overline{\vec{y}})(\wvec{y}_i - \overline{\vec{y}})^T + \wmat{H}\right) \\
  &= \Delta\wmat{Y}\diag{\vec{W}^{(c)}}\Delta\wmat{Y}^T + \wmat{H} \\
  \ukfNotee{x}{y}{t} &= \sum_{i=0}^m W_i^{(c)} 
    (\wvec{a}_i - \emNotee{\vec{a}}{t}{t-1})(\wvec{y}_i - \overline{\vec{y}})^T \\
  &= \Delta\wmat{A}\diag{\vec{W}^{(c)}}\Delta\wmat{Y}^T
\end{aligned}$$

The final correction step is then:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t} &= \emNotee{\vec{a}}{t}{t - 1} + \ukfNotee{x}{y}{t}\ukfNotee{y}{y}{t}^{-1}(\vec{y}_t - \overline{\vec{y}}) \\
  \emNotee{\mat{V}}{t}{t} &= \emNotee{\mat{V}}{t}{t} - \ukfNotee{x}{y}{t}\ukfNotee{y}{y}{t}^{-1}\ukfNotee{x}{y}{t}^T
\end{aligned}$$

## Re-writting
The above formulation has the draw back that we have to invert $\ukfNotee{y}{y}{t}^{-1}$ which is infeasible when the number observation is moderately large (say greater than 1000). We can re-write the above using the Woodbury matrix identity to get algorithm $O(\vert R_t \vert)$ instead of $O(\vert R_t \vert^3)$ where $R_t$ is the indices at risk in the $i$'th interval

The proposed correction step can be computed as:
$$\begin{aligned}
  \tilde{\vec{y}} &= \Delta \wmat{Y}^T \widehat{\mat{H}}^{-1}(\vec{y}_t - \overline{\vec{y}}) \\ 
    \mat{G} &= \Delta\wmat{Y}^T\widehat{\mat{H}}^{-1}\Delta\wmat{Y} \\
  \vec{c} &= \tilde{\vec{y}} - \mat{G}\left( \diag{\vec{W}^{(m)}}^{-1} + \mat{G}\right)^{-1} \tilde{\vec{y}} \\ 
    \mat{L} &= \mat{G} - \mat{G}\left( \diag{\vec{W}^{(c)}}^{-1} + \mat{G}\right)^{-1} \mat{G} \\
  \emNotee{\vec{a}}{t}{t} &= \emNotee{\vec{a}}{t}{t - 1} + \Delta\wmat{X}\diag{\vec{W}^{(c)}}\vec{c} \\
  \emNotee{\mat{V}}{t}{t} &= \emNotee{\mat{V}}{t}{t - 1} - 
    \Delta\wmat{X}\diag{\vec{W}^{(c)}}\mat{L}\diag{\vec{W}^{(c)}}\Delta\wmat{X}^T
\end{aligned}$$

where $\tilde{\vec{y}}$, $\mat{G}$, $\mat{L}$ and $\vec{c}$ are intermediates. The above algorithm is $O(\vert R_t \vert)$ since $\widehat{\mat{H}}$ is a diagonal matrix and all products involves at worst multiplication $m\times \vert R_t \vert$ or $\vert R_t \vert \times m$ matrices

## Selecting hyperparameters
We still need to select the hyperparameters $\kappa$, $\alpha$ and $\beta$. We will cover these in the given order. $\kappa$ is usually set to $\kappa = 0$ or $\kappa = 3 - m$ which [@julier97] state is a "*useful heuristic*" when the state equation is Gaussian and $\alpha = 1$. The default is $\kappa = m/\alpha^2 - m$ and can be altered by setting the list element `kappa` in the list passed as the `control` argument to `ddhazard`. For example, `control = list(kappa = 1, ...)` yields $\kappa = 1$. The default is set to ensure that $W_0^{(m)} = 0$ such that all weights are positive. Hence, we are guaranteed that $\emNotee{\mat{V}}{t}{t-1}$ and $\ukfNotee{y}{y}{t}$ are positive semi-definite. This follows since both are sum of outer products with positive weights and that $\widehat{\mat{H}}$ is a diagonal matrix with positive entries. Notice though that this means that $\alpha$ only affects $W_0^{(c)}$ which simplifies to $W_0^{(c)} = 1 - \alpha^2$. See [@julier02] for more details


$0<\alpha \leq 1$ controls the spread of the sigma points. Notice that $\lambda + m \rightarrow 0^+$, $w_0^{(c)},w_0^{(m)}\rightarrow -\infty$ and $w_i^{(c)}, w_i^{(m)} \rightarrow \infty$ ($i > 0$) as $\alpha \rightarrow 0^+$ with $\kappa = 0$. Thus, the lower the value of $\alpha$, the lower the spread but the higher the absolute weights. It is generally suggested to choose $\alpha$ small. The arguments hereof are provided in for example [@gustafsson12] and [@julier04]. For this reason, the default is $\alpha = 0.1$. The parameter can be altered through the `alpha` element of the list passed to the argument `control` of `ddhazard`.

Lastly, $\beta$ is a correction term to match the fourth-order term in the Taylor series expansion of the covariance of the observational equation. [@julier04] show in the appendix that the optimal value with a Gaussian state equation is $\beta = 2$. This is the default. It can be altered through the `beta` element of list passed to the argument `control` of `ddhazard`.

## Selecting starting values
Experience with different data sets and the UKF is that the method is sensitive to the starting values of $\mat{Q}$ and $\mat{Q}_0$ (where the latter may be fixed). The reason for divergence can be illustrated by the effect of $\mat{Q}_0$. We start the filter by setting $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$. Say that we set $\mat{Q}_0 = k \mat{I}_m$ and $\vec{a}_0 = \vec{0}$. Then the $i$'th column of the Cholesky decomposition $\emNotee{\mat{V}}{0}{0}$ is a vector with $\sqrt{k}$ in the $i$'th entry and zero in the rest of the entries. Suppose that we set $k$ large. Then the linear predictors computed with the $l < m +1$ sigma point is $k x_{j1l}$ where $x_{j1l}$ is the $l$'th entry of individuals $j$'s co-variate vector at time $1$. This can be potentially quite large in absolute terms if $x_{kjt}$ is moderately different from zero. This seems to lead to divergence in some cases. For instance in the logistic model where we end with either zero or one estimates for the outcome for most or all sigma points  

$\mat{Q}$ has a similar effect although it is harder to illustrate with a small example as it occurs as an intermediate in the UKF. Question is then how to select $\mat{Q}$ and $\mat{Q}_0$. At this point, I can suggest to pick at diagonal matrix for $\mat{Q}_0$ with a "*somewhat*" large values and $\mat{Q}_0$ to a diagonal matrix with small values. This is based on experience. Though, what is "*somewhat*" large and what is small dependent on the data set

# Fixed effects
This section will motivate the fixed effects (non time varying) and cover how they are estimated. We start with the latter. The likelihood we are working with can be written as follows by application of the markovian property of the model:
$$\begin{split}
	\proppCond{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}}{\vec{y}_{t},\dots,\vec{y}_T} &\propto 
		L\Lparen{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}} \\
	 & = p(\vec{\alpha}_0)\prod_{t = 1}^d \proppCond{\vec{\alpha}_t}{\vec{\alpha}_{t-1}}	
		\prod_{i \in \mathcal{R}_t} \proppCond{y_{it}}{\vec{\alpha}_t}
\end{split}$$

This is what we approximately find the expectation of in the E-step of the EM-algorithm where $\mat{Q}$, $\mat{Q}_0$ and $\vec{\alpha}_0$ are assumed fixed. Denote the entries of the latter two matrices and vector by $\vec{\theta}$. Then the M-step proceeds by maximizing $\vec{\theta}$ conditional on the estimates in the E-step. Expanding the log likelihood yields:  

$$\begin{aligned}
	\mathcal{L}\Lparen{\vec{\theta}} =    
		\log L \Lparen{\vec{\theta}} = & 
		- \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^T \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^T \mat{Q}^{-1} \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\ 
	&  - \frac{1}{2} \deter{\mat{Q}_0} - \frac{1}{2d} \deter{\mat{Q}} \\
	&  - \sum_{t = 1}^d \sum_{i \in R_t} l_{it}({\vec{\alpha}_t})
\end{aligned}$$

$$l_{it}(\vec{\alpha}_t) = y_{it} \log h(\vec{x}_{it}^T \vec{\alpha}_t) + (1 - y_{it}) 
	\log \Lparen{1 - h(\vec{x}_{it}^T \vec{\alpha}_t)}$$
	
Notice that the entries in $\vec{\theta}$ only appears in the first three lines while the fourth line only depends on the estimate from the E-step. Suppose now that we assume that some of the effects are fixed such that we replace the linear predictor $\vec{x}_{it}^T\vec{\alpha}_t$ by $\vec{x}_{it}^T\vec{\alpha}_t + \widetilde{\vec{x}}_{it}^T\vec{\gamma}$ where $\vec{\gamma}$ is the fixed effects and $\widetilde{\vec{x}}_{it}$ are the corresponding co-variates. The new definition of $l_{it}$ is:

$$l_{it}(\vec{\alpha}_t) = y_{it} \log h(\vec{z}_{it}^T \vec{\alpha}_t + \widetilde{\vec{x}}_{it}^T\vec{\gamma}) + (1 - y_{it}) 
	\log \Lparen{1 - h(\vec{x}_{it}^T \vec{\alpha}_t + \widetilde{\vec{x}}_{it}^T\vec{\gamma})}$$
	
Say that we fix $\vec{\gamma}$ doing the E-step and estimate $\vec{\gamma}$ doing the M-step. Then we observe that: 

1. $\widetilde{\vec{x}}_{it}^T\vec{\gamma}$ acts like offsets doing the E-step. Thus, we only need to add these offsets in the UKF or EKF
2. $\vec{\gamma}$ is estimated separately from $\vec{\theta}$ in the M-step. Thus, no changes are needed in the update formulas for $\mat{Q}$, $\mat{Q}_0$ and $\vec{\alpha}_0$
3. $\vec{x}_{it}^T \vec{a}_t$ acts like offsets in the M-step when we estimate $\vec{\gamma}$
4. The optimization of $\vec{\gamma}$ in the M-step is a generalized linear model for distributions from the exponential family 

## Implementation
Point number 4 above implies that we can use a typical Newton Raphson algorithm to compute an updated estimate of $\vec{\gamma}$ when we are using a distribution from the exponential family. This can be solved by a QR decomposition as done in `glm`. However, point 3 implies that every observation will have a different offset in every bin the observation is in. Thus, we can end with a large design matrix

To overcome the memory issue this package use the same Fortran function that `bigglm` function in the `biglm` package uses. The Fortran function recursively performs a QR update for each row in the design matrix. Hence, we do not need to store the entire design matrix at any given point. The Fortran code is described in [@miller1992] and written by the author. It is an updated version of the algorithm described in [@gentleman1972] which has a time complexity of $O(\vert\vec{\gamma}\vert^2)$ for the QR-update of each row in the design matrix

Surely, other methods to solve the QR problem or fit a generalized linear model could be used that does not require us to store the entire design matrix and are faster and/or more stable. An example could be the algorithm described in [@hammarling08]. The current method is used since it has shown to work well in the `bigglm` function and as we assume that few parameters will be fixed. Thus, the $O(\vert\vec{\gamma}\vert^2)$ cost of doing the M-step should not be an issue

Fixed terms can be estimated by wrapping the co-variates in the formula of `ddhazard` in the `ddFixed` function. As an example, `ddhazard(Surv(tstart, tstop, y) ~ x1 + ddFixed(x2), ...)` will fit a model where `x1` is time varying and `x2` is not. The M-step recursively updates the $\vec{\gamma}$ starting with the previous estimated value. The estimation stops when $\rVert\vec{\gamma}^{(k)} - \vec{\gamma}^{(k - 1)}\lVert / (\rVert\vec{\gamma}^{(k - 1)}\lVert + \delta) < \epsilon$ where superscript denotes the estimation number, $\epsilon$ is the tolerance and $\delta$ is a small number. $\epsilon$ can be changed by setting  `eps_fixed_parems` element of the list passed to the `control` argument of `ddhazard`. 

The estimation will stop if the criteria given by $\epsilon$ is not meet within a given number of iterations. The maximum number of iteration can be set by setting the `max_it_fixed_parems` element of the `control` argument to `ddhazard`. The user will be warned if the criteria is not meet within `max_it_fixed_parems` iterations. The estimation though continues with the latest estimate of $\vec{\gamma}$ in the E-step 

## Why use fixed effects
Question: Should I motivate the use of fixed effects or is this obvious?

# Logistic model
The logistic model is fitted with by setting `model = "logit"` in the call to `ddhazard`. The link function $h$ is defined as inverse logit function $h(\eta) = \exp(\eta) / (1 + \exp(\eta))$. The following paragraphs will cover the a few comments to the EKF and UKF implementation. This is followed by some comments about the loss of information due to binning which motivates the exponential model

## EKF
From a numerical point of view, each individual computation are fairly stable with EKF method for the logistic model. The main reason is that:
$$\left. \partial h(\eta)/ \partial \eta \right\vert_{\eta = \vec{z}_{it}^T \vec{\alpha}_t} = H_{iit}(\vec{\alpha}_t)$$

Given the variance $\mat{H}_t(\vec{\alpha}_t)$ and expected mean $h(\eta)$ are bound this means that all the terms are on a reasonably stable for all values of the linear predictor $\vec{\alpha}_t^T\vec{x}_{it}$

## UKF
This is not the case for the UKF method and the logistic model. We scale by $\wmat{H}^{-1}$ when we compute $\mat{G}$ and $\tilde{\vec{y}}$ which will approach Infinity as linear predictor get large in absolute value. For this reason, we truncate the linear predictor $\eta_{it} = \vec{\alpha}_t^T\vec{x}_{it}$ such that the variance cannot be less than some pre-specified quantity $\delta$. Effectively this means that we set:
$$\begin{aligned}
  &h(\eta)(1-h(\eta) \geq \delta, \qquad \delta \in (0, 1/4) \\
  \Leftrightarrow \quad & \log \left( \frac{1 - 2\delta - \sqrt{(1-4\delta)}}{2\delta}\right)
    \leq \eta \leq 
    \log \left( \frac{1 - 2\delta +\sqrt{(1-4\delta)}}{2\delta}\right)
\end{aligned}$$

In terms this implies that:
$$\frac{1 - 2 \delta - \sqrt{1-4 \delta }}{1 - \sqrt{1-4 \delta}}
  \leq h(\eta) \leq \frac{1 - 2 \delta + \sqrt{1-4 \delta }}{1 + \sqrt{1-4 \delta}}$$
  
The current implementation uses $\delta = 10^{-4}$ 

## Fixed effects
Fixed effects are estimated by a logistic model where each individuals' observation will get a an observation in the new model with an individual offset for each bin the observation is in. Say for instance that we want to fit: 
```{r, echo=FALSE}
set.seed(1010101012)
data_frame <- 
  data.frame(id = c(1, 1, 1, 2, 2),
             tstop = c(0, 2, 3, 0, 2), tstart = c(2, 3, 4, 2, 4), y = c(0, 0, 1, 0, 0), 
             x1 = round(rnorm(5), 2), x2 = round(rnorm(5), 2))
```

```{r}
# The data we use
head(data_frame)
```

```{r, eval=FALSE}
# The fit
fit <- ddhazard(Surv(tstart, tstop, y) ~ x1 + ddFixed(x2), data_frame, 
                by = 1, # bin lengths are 1
                id = data_frame$id)
```

The model is with one fixed effect for `x2` and a time varying effect for `x1`. The individual with `id = 1` will yield 4 observation in the M-step: two for the first row, one for the second row and one for the third row. The first row will have two observations as it crosses two bins. Each observation in the M-step will have a separate offset given by $\emNotee{\vec{a}}{t}{d}$ times the value of `x1` where $t$ match the bin number. For example, the third rows' observation will have an offset of $\emNotee{\vec{a}}{4}{d}$ times `r data_frame$x1[3]`

## Starting value
QUESTION: Should I write about the static model that can be used for $\vec{\alpha}_0$?

## Binning
```{r binning_fig, echo=FALSE, results="hide", fig.cap = "Illustration of binning. See the text for explanation", fig.height=3}
par(cex = .8, mar = c(1, 4, 1, 2))
plot(c(0, 4), c(0, 1), type="n", xlab="", ylab="", axes = F)

abline(v = c(0.5, 1.5, 2.5), lty = 2)

n_series = 7
y_pos = seq(0, 1, length.out = n_series + 2)[-c(1, n_series +2)]

captioner::captioner()("binning_fig")

set.seed(1992)
x_vals_and_point_codes = list(
  cbind(0:4 + c(.1, rep(0, 4)),
        rep(4, 5)),
  cbind(c(0.1, 1, 1.5 + runif(1)),
        c(4, 4, 16)),
  cbind(c(0.1, 1, 2, 2 + runif(1, min = 0, max = .25)),
        c(4, 4, 4, 16)),
  cbind(c(0.1, runif(1)), c(4, 16)),
  cbind(c(0.1, 1, 2, 2.5 + runif(1)),
        c(4, 4, 4, 16)),
  cbind(c(2, 2.33),
        c(4, 16)), 
  cbind(c(0.1, .75),
        c(4, 4)))

x_vals_and_point_codes = x_vals_and_point_codes[
  c(n_series, sample(n_series - 1, n_series - 1, replace = F))]

for(i in seq_along(x_vals_and_point_codes)){
  vals = x_vals_and_point_codes[[i]]
  y = y_pos[i]
  
  xs = vals[, 1]
  n_xs = length(xs)
  
  # add lines
  segments(xs[-n_xs], rep(y, n_xs - 1),
           xs[-1], rep(y, n_xs - 1))
  
  # Add point
  points(xs, rep(y, n_xs), pch = vals[, 2])
  
  # Add numbers
  text(xs, rep(y + .05, n_xs), as.character(0:(n_xs -1)))
}

# add letters
text(rep(0, n_series), rev(y_pos), letters[1:n_series], cex = par()$cex * 1.5)
```

This section will illustrate how binning is performed for the logistic model and how this can lead to loss of information. It is elementary but included to stress this point and motivate the exponential model. We will use `r tolower(captioner::captioner()("binning_fig", display = "cite"))` as the illustration. Each horizontal line represent an individual. A cross represents when the co-variate values change for the individual and a circle represents the death of an individual. Lines that ends with a cross are right censored

The vertical dashed lines represents the bin borders. The first vertical from the left is where we start our model, the second vertical line is where the first bin ends and the second bin starts and the third vertical line is where the second bin ends. Thus, only have two bins in this example

We can now cover how the individuals (horizontal lines) are used in the estimation:

a. Is a control in both bins. We use the co-variates from 0 in the first bin and the co-variates from 1 in the second bin
b. Is not included in any of the bins. We do not know the co-variates values at the start of the second bin so we cannot include him
c. Is a control in the first bin with the co-variates from 0. He will count as a death in the second bin with the co-variates from 1
d. Acts like a. 
e. Is a death in the first bin with co-variates from 0
f. Is a control in the first bin with the co-variates from 0. He is a death in the second bin with the co-variates from 1
g. Is not included in any bins. We do not know if he survived the entire period of the first bin and thus we cannot include him

The example illustrates that: 

1. We loose  information about co-variates that are updated within bins. For instance, a., c., d. and f. all use the co-variates from 0 for the entire period of the first bin despite that the co-variates change at 1. Moreover, we never use the information at 2 from a., d. and f.
2. We loose information when we have right censoring. For instance, g. is not included at all since we only know that survives parts of the first bin
3. We loose information for observation that only occurs within bins as is the case for b.

The above motivates the exponential model that will be covered in the next sections

# Exponential model
The following section introduce the exponential model. The exponential model has a tuple for each observation. The tuples contains an indicator for whether the individual dies and a right truncated time variable for the observed survival time. We start by describing the assumption of the exponential model. Then we turn to the indicator followed by the right truncated time variable. Finally, a few comment is added to the EKF implementation.


## Assumptions
We make the following assumption in the exponential model: 

1. Parameters (i.e. state variables $\vec{\alpha}_1,\dots, \vec{\alpha}_T$) change at time $1, 2, \dots , T$
2. The individuals co-variates change at discrete times
3. We have exponential distributed arrival times within bins conditional on the parameters and co-variates

These assumption means that we have piecewise constant exponential distributed arrival times. The instantaneous hazard change when either the individual co-variates change or the parameters change. We make the following definitions to formalize the above. Let $\vec{x}_{ij}$ denote the $i$'th individuals $j$'th co-variate vector. For each individual we observe $j = 1, 2, \dots , l_i$ co-variate vectors. Each co-variate vector is valid in a period $(t_{i,j-1}, t_{i,j}]$. This definition differs from the previous definition of $\vec{x}_{ij}$ where the subscript $j$ referred to the bin number. Let $T_i$ denote the random death time of the $i$'th individual. Lastly, let $y_{ij} = 1_{\{T_i \in (t_{i,j - 1}, t_{i,j}]\}}$ be the indicator for whether the $i$'th individual dies in period $(t_{i,j - 1}, t_{i,j}]$

The likelihood of observing a death for the $i$'th individual in the last period $(t_{i,l_i - 1}, t_{i,l_i}]$ is: 

$$\begin{aligned}
  \proppCond{Y_{il_i} = 1}{\vec{\alpha}_0,\vec{\alpha}_1,\dots} =& \proppCond{Y_{il_i} = 1}{Y_{i, l_i -1} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{5pt} \cdot \proppCond{Y_{i, l_i -1} = 0}{Y_{i, l_i -2} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{75pt} \vdots \\
  &\hspace{5pt} \cdot \proppCond{Y_{i, 2} = 0}{Y_{i, 1} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots}
    \proppCond{Y_{i, 1} = 0}{\vec{\alpha}_0,\vec{\alpha}_1,\dots}
\end{aligned}$$

We can use the memory less property of the exponential distribution to conclude that each of factor and the right hand site have:

$$\begin{aligned}
\proppCond{Y_{i,s} = 1}{Y_{i, s - 1} = 0\wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} &= 1 - \prod_{z = \Lfloor{t_{i,s-1}} + 1 }^{\Lceil{t_{i,s}} } \exp\Lparen{ - \exp(\vec{x}_{is}^T \vec{\alpha}_{z})
    \Lparen{\min\{ z, t_{i,s} \} - \max\{ z - 1, t_{i,s-1} \} }}
\end{aligned}$$
    
where $\Lfloor{\cdot }$ is the floor function and $\Lceil{\cdot}$ is the ceiling function. In order to get this into the state space model notation we further have to separate $Y_{i,s}$ if $(t_{i,s-1}, t_{i,s}]$ crosses one or more bins. Take for example $(0.5, 2.5]$. Here we add three binary observations: one with time period of $(0.5,1]$, one with $(1, 2]$ and one with $(2, 2.5]$. Notice that this also implies that an individual who has different co-variate vectors within in a bin will have more than one observation in that bin. For example, an individual with a co-variate vector for $(0, 0.5]$ and a co-variate vector for $(0.5, 1]$ will yield two observation to the observation equation in the first bin

Computing the conditional mean, $h$, can done as follows. Assume for simplicity of notation that the observation $Y_{i,s}$ is inside an interval. In other words $\Lceil{t_{i,s}} - 1 = \Lfloor{t_{i,s-1}}$. This could be an observation we have introduced because the initial interval crossed a bin. The conditional mean is:

$$\begin{aligned}
  \tilde{z}(\vec{\alpha}) &= \expecpCond{Y_{i, s}}{Y_{i, s - 1} = 0 \wedge \vec{\alpha}_{\Lceil{t_{i,s}} } = \vec{\alpha}, \vec{\alpha}_{\Lceil{t_{i,s}} - 1}, \dots , \vec{\alpha}_0 } \\
   &= \left. h(\eta) \right|_{\eta = \vec{x}_{is}^T\vec{\alpha} } \\
   &= \left. 1 - \exp\Lparen{ - \exp(\eta) \Lparen{t_{i,s} - t_{i,s-1} }}\right|_{\eta = \vec{x}_{is}^T\vec{\alpha} }
\end{aligned}$$

where the tilde is added to stress that $\tilde{z}(\vec{\alpha}_{\Lceil{t_{i,s}}})$ is a margin of the mean in the observational equation $\vec{z}_{\Lceil{t_{i,s}}}$ where we do not provide a subscript for the elements index. The variance for the diagonal in $\mat{H}_{\Lceil{t_{i,s}}}(\vec{\alpha}_{\Lceil{t_{i,s}}})$ is given by $h(\eta)(1 - h(\eta))$ as $Y_{is}$ is binary. 

Right censoring is easily handled if we assume independent censoring. In this case the '$\min$' condition in $\proppCond{Y_{i,s} = 1}{Y_{i, s - 1} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots}$ is valid for the last $t_{il_i}$ even if $t_{il_i} < \Lceil{t_{il_i}}$. We only know that the individual survives up to $t_{il_i}$ after which he was censored by an independent mechanism which we can condition on. However, the same logic does not apply when $t_{il_i} < \Lceil{t_{il_i}}$ and it is due to a death ($T_i = t_{il_i}$). We cannot condition on $T_i = t_{il_i}$ as we did with independent censoring because then $\proppCond{Y_{i,l_i} = 1}{T_i = t_{il_i}} = 1$

What we do instead is that we round $t_{il_i}$ up when we compute $h$ (the conditional mean) and when we compute the conditional variance in the case of a death. This means that we loose the information of the time of the event. However, we incorporate this information with the addition described in the next section

## Truncated observations time
We start by defining the truncated observation time $\Delta_{is}$:
$$\begin{aligned}
  &\Delta_{is} = (T_i - t_{i,s-1}) + \Lparen{t_{i,s} - T_i}1_{\{T_i \geq t_{i,s}\}} \\
  &T_i \geq t_{i,s-1}\Rightarrow\Delta_{is} \in [0,t_{i,s} - t_{i,s-1}]
\end{aligned}$$

The proposed and implemented model is to let every observation yield a tuple $(Y_{i,s}, \Delta_{i,s})$. The reason to include the tuple will be given and the end of this section. First, we will need to find the conditional mean and variance of $\Delta_{i,s}$ in order to use $\Delta_{i,s}$ in the observational equation. We start by recursively conditioning to get:
$$\begin{aligned}
  \proppCond{T_i = t_{i,l_i}}{\vec{\alpha}_0,\vec{\alpha}_1,\dots} =&
    \proppCond{\Delta_{i,l_i} = t_{i,l_i} - t_{i,l_i -1}}{\Delta_{i,l_i - 1} = t_{i,l_i - 1} - t_{i,l_i - 2} \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{5pt} \cdot \proppCond{\Delta_{i,l_i - 1} = t_{i,l_i - 1} - t_{i,l_i - 2} }
    {\Delta_{i,l_i - 2} = t_{i,l_i - 2} - t_{i,l_i - 3} \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{140pt} \vdots \\
  &\hspace{5pt} \cdot \proppCond{\Delta_{i,2} = t_{i,2} - t_{i,1} }
    {\Delta_{i,1} = t_{i,1} \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots}
    \proppCond{\Delta_{i,1} = t_{i,1} }{\vec{\alpha}_0,\vec{\alpha}_1,\dots}
\end{aligned}$$

The conditional probabilities simplifies in a similar manner as for the binary $Y_{i,s}$ due to the memoryless property of the exponential distribution. Thus, computing the conditional mean, $h$, can done as follows. Again, assume for simplicity of notation that the observation $(t_{i, s-1}, t_{i,s}]$ is inside an interval (see the previous section). Then:

$$\begin{aligned}
  \tilde{z}(\vec{\alpha}) &= \expecpCond{\Delta_{i,s}}
    {\Delta_{i,s - 1} = t_{i,s - 1} - t_{i,s - 2} \wedge \vec{\alpha}_{\Lceil{t_{i,s}} } = \vec{\alpha}, \vec{\alpha}_{\Lceil{t_{i,s}} - 1}, \dots , \vec{\alpha}_0  } \\
  &= \left. h(\eta) \right|_{\eta = \vec{x}_{is}^T\vec{\alpha} } \\
  &= (t_{i,s} - t_{i,s - 1}) P(\tilde{T} \geq t_{i,s} - t_{i,s - 1}) + \int_0^{t_{i,s} - t_{i,s - 1}} r f_{\tilde{T}}(r) \mathrm{d} r
\end{aligned}$$

where $\tilde{T} \sim \text{Exp}\Lparen{\exp(\vec{x}_{is}^T\vec{\alpha})}$ and $f_{\tilde{T}}$ is the density function of $\tilde{T}$. Set $\lambda = \exp(\vec{x}_{is}^T\vec{\alpha})$ and $\delta =t_{i,s} - t_{i,s - 1}$ . The resulting conditional mean is:

$$\tilde{z}(\vec{\alpha}) = \frac{1 - \exp \Lparen{- \lambda \delta} }{\lambda}$$

Moreover, we can show that the variance is:
$$\begin{aligned}
  \hspace{50pt}&\hspace{-50pt}\varpCond{\Delta_{i,s}}
    {\Delta_{i,s - 1} = t_{i,s - 1} - t_{i,s - 2} \wedge \vec{\alpha}_{\Lceil{t_{i,s}} } = \vec{\alpha}, \vec{\alpha}_{\Lceil{t_{i,s}} - 1}, \dots , \vec{\alpha}_0  } \\
  &= \frac{1 - \exp\Lparen{-2\delta\lambda} - 2 \lambda \delta \exp \Lparen{-\delta\lambda}}{\lambda^2}
\end{aligned}$$

Right censoring is treated the same way as for $Y_{i,s}$. That is, we can have $t_{i,l_i} < \Lceil{t_{i,l_i}}$ in the case censoring. Further, we still round up in the case where $T_i = t_{i,l_i}$ (in the case of a death). The individual could have survived $\Lceil{t_{i,l_i}}$ but only survived $t_{i,l_i}$. Denote the predicted mean $\Delta_{i,l_i}$ by $\widehat{\Delta}_{i,l_i}$. We can have $\widehat{\Delta}_{i,l_i} \geq \Delta_{i,l_i}$ in the case of death. This will affect the correction step in both the UKF end EKF when we compute $\emNotee{\vec{a}}{t}{t}$

## Why not use $\Delta_{i,s}$ solely
The reason also to use the binary $Y_{i,t}$ is that we cannot distinguish between deaths when times are rounded. For example, say that the observed times are rounded up to multiplies of $0.5$ (i.e. $0$, $0.5$, $1$, $1.5$ etc.). We cannot distinguish between the death of an individual, change in co-variate vector or change of parameters if $t_{i,s}$ is a whole number. That is, $\Delta_{i,s} = 1$ either because $T_i = 1$ or because $t_{i,s} = 1$ since we have new co-variate vector $\vec{x}_{i,s+1}$ or(/and) because the parameters changed from $\vec{\alpha}_1$ to $\vec{\alpha}_2$

## Covariance 
$Y_{i,t}$ and $\Delta_{i,s} = 1$ are correlated and we have to account for that. This will affect the score vector and information matrix ($\vec{u}_t(\emNotee{\vec{a}}{t}{t-1})$ and $\mat{U}_t(\emNotee{\vec{a}}{t}{t-1})$) in the EKF. Further, it will affect the covariance matrix $\widehat{\mat{H}}$ in the UKF. First though, we have to derive the covariance. We start by defining the following variables to ease the notation:
$$\delta = t_{i,s}-t_{i,s-1}, \qquad \lambda = \exp \Lparen{\vec{x}_{is}^T \vec{\alpha}_{\lceil t_{i,s} \rceil}}, \qquad
  Z \sim \text{Exp}\Lparen{\lambda}$$

Thus, we have the following relation (conditional on having survied up the this point):
$$Y_{i,s} \sim 1_{\Lbrace{Z\in[0,\delta\}}}, \qquad \Delta_{i,s} \sim Z + (\delta - Z) 1_{\Lbrace{Z\in[\delta,\infty\}}}$$

where $\sim$ denotes "similary destributed" and $1_{\Lbrace{\cdot}}$ is one if the condition in the braces are satisfied and zero otherwise. Hence, we can find the covariance by: 
$$\begin{aligned}
\covp{1_{\Lbrace{Z\in[0,\delta\}}}, Z + (\delta - Z) 1_{\Lbrace{Z\in[\delta,\infty\}}}}
  =\hspace{-150pt}& \\ 
  &\covp{1_{\Lbrace{Z\in[0,\delta\}}}, Z}
  + \delta\covp{1_{\Lbrace{Z\in[0,\delta\}}},1_{\Lbrace{Z\in[\delta,\infty\}}}} 
  - \covp{1_{\Lbrace{Z\in[0,\delta\}}}, Z 1_{\Lbrace{Z\in[\delta,\infty\}}}} =\\
  &-\exp (-\lambda\delta)\delta 
    + \delta \Lparen{- \Lparen{1 - \exp(-\lambda\delta)} \exp(-\lambda\delta)} 
    - \frac{- \Lparen{1 - \exp(-\lambda\delta)} \exp(-\lambda\delta)(1+\delta\lambda)}{\lambda} = \\
  &  - \frac{\exp (-2\lambda\delta)\Lparen{1 + \lambda\delta\exp(\lambda\delta)-\exp(\lambda\delta)}}{\lambda}
\end{aligned}$$

Next, we turn to the EKF. Define:
$$\varp{Y_{i,s}} = \sigma_{Y_{i,s}}^2, \qquad \varp{\Delta_{i,s}} = \sigma_{\Delta_{i,s}}^2,  \qquad
  \covp{Y_{i,s}, \Delta_{i,s}} = \rho_{i,s} \sigma_{Y_{i,s}} \sigma_{\Delta_{i,s}} = \xi_{i,s}$$
  
Say that we are in looking at a given bin and order the observation equation such that:
$$\vec{y}_t = \Lparen{y_{1,i_{1,1}}, y_{1,i_{1,2}}, \dots y_{m,i_{m,k}}, \Delta_{1,i_{1,1}}, \Delta_{1,i_{1,2}}, \dots, \Delta_{m,i_{m,k}}}^T$$

where $i_{j,i}$ is an index function that returns the index of the $j$'th individuals $i$'th observation in this bin. Although this is tedious, it is included to stress that any given individual could have none, one or multiple enteries in this bin. Further, notice that $\vec{y}_t$ refers to the whole state vector while $y_{i,j}$ refers to the indicator for whetehr invidiual $i$ dies at his $j$'th observations. The co-variance matrix $\mat{H}(\alpha)$ is then of the following structure due to the ordering of the elements of $\vec{y}_t$:

$$\mat{H}(\vec{\alpha}_t) =\begin{pmatrix}
  \diag{\sigma_{Y_{1,i_{1,1}}}^2, \sigma_{Y_{1,i_{1,2}}}^2,\dots, \sigma_{Y_{m,i_{m,k}}}^2}
    & \diag{\xi_{1,i_{1,1}}, \xi_{1,i_{1,2}},\dots, \xi_{m,i_{m,k}}} \\
  \diag{\xi_{1,i_{1,1}}, \xi_{1,i_{1,2}},\dots, \xi_{m,i_{m,k}}} 
    & \diag{\sigma_{\Delta_{1,i_{1,1}}}^2, \sigma_{\Delta_{1,i_{1,2}}}^2,\dots, \sigma_{\Delta_{m,i_{m,k}}}^2}
\end{pmatrix}$$

where all the entries implicitly depends on the state vector $\vec{\alpha}_t$. In order to update the EKF we have to go back to the forumlas for the score vector and information matrix. They are:
$$\begin{array}{c}
  \dot{\vec{z}}_t(\alpha_t) = \left. \frac{\partial\vec{z}_t(\vec{\eta})}{\partial \vec{\eta}} \right\vert_{\vec{\eta}= \vec{\alpha}_t}\vspace{10pt} \\
  \vec{u}_t(\vec{\alpha}_t) = \dot{\vec{z}}_t(\alpha_t) \mat{H}(\vec{\alpha}_t)^{-1}\Lparen{\vec{y}_t - \vec{z}_t(\vec{\alpha}_t)}, \qquad
\mat{U}_t(\vec{\alpha}_t) = \dot{\vec{z}}_t(\alpha_t) \mat{H}(\vec{\alpha}_t)^{-1} \dot{\vec{z}}_t(\alpha_t)^T
\end{array}$$

where we get the simplier expression we saw previously when $\mat{H}(\vec{\alpha}_t)$ is a diagonal matrix. In the case with off-diagonal terms we can use the idenity: 
$$\begin{bmatrix} \mathbf{A} & \mathbf{C}^T \\ \mathbf{C} & \mathbf{D} \end{bmatrix}^{-1} = \begin{bmatrix} (\mathbf{A}-\mathbf{C}^T\mathbf{D}^{-1}\mathbf{C})^{-1} & -\mathbf{A}^{-1}\mathbf{C}^T(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{C}^T)^{-1} \\ -(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{C}^T)^{-1}\mathbf{CA}^{-1} & (\mathbf{D}-\mathbf{CA}^{-1}\mathbf{C}^T)^{-1} \end{bmatrix}$$
We can find a closed form solution since the three matrices are diagonal matricies. Define:
$$\mat{K}(\vec{\alpha}_t) = \mat{H}(\vec{\alpha}_t)^{-1} = \begin{pmatrix}
  \mat{K}_{11} & \mat{K}_{12} \\
  \mat{K}_{12} & \mat{K}_{22}
\end{pmatrix}$$

Then:
$$\begin{aligned}
&\mat{K}_{11} = \diag{\frac{\exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(-2 \delta _{s,j} \lambda _{s,j} \exp\Lparen{\delta
   _{s,j} \lambda _{s,j}}+\exp\Lparen{2 \delta _{s,j} \lambda _{s,j}}-1\right)}{-\exp\Lparen{\delta _{s,j}
   \lambda _{s,j}} \left(\delta _{s,j}^2 \lambda _{s,j}^2+2\right)+\exp\Lparen{2 \delta _{s,j}
   \lambda _{s,j}}+1}} \\
&\mat{K}_{22} = \diag{\frac{\lambda _{s,j}^2 \exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(\exp\Lparen{\delta _{s,j} \lambda
   _{s,j}}-1\right)}{-\exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(\delta _{s,j}^2 \lambda
   _{s,j}^2+2\right)+\exp\Lparen{2 \delta _{s,j} \lambda _{s,j}}+1}} \\
&\mat{K}_{12} = \diag{\frac{\lambda _{s,j} \exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(\exp\Lparen{\delta _{s,j} \lambda _{s,j}}
   \left(\delta _{s,j} \lambda _{s,j}-1\right)+1\right)}{-\exp\Lparen{\delta _{s,j} \lambda _{s,j}}
   \left(\delta _{s,j}^2 \lambda _{s,j}^2+2\right)+\exp\Lparen{2 \delta _{s,j} \lambda _{s,j}}+1}}
\end{aligned}$$

where we have omitted the exact entries of the subscript. The subscripts follow such that the pair match the order for the observation vector ($(1,i_{1,1}),(1,i_{1,2}),\dots,(m,i_{m,k})$). The score and information matrix can than be computed as follows. Suppose we have $r$ observations such that the observation equation has dimension $2r$. Next, denote the binaries of $\vec{y}_t$ by $y_1,y_2,\dots,y_r$ and denote the truncated times by $\Delta_1,\Delta_2,\dots,\Delta_r$. The score vector is then:

$$\begin{aligned}
\vec{u}_t(\vec{\alpha}_t) = \sum_{i=1}^r 
  &\vec{x}_i\Lparen{h^y_i}'(\eta_i)\Lparen{\MyInd{\mat{K}_{11}}{ii} + \MyInd{\mat{K}_{12}}{ii}}(y_i - h^y_i(\eta_i)) \\
  &\hspace{4pt}+\vec{x}_i\Lparen{h^\Delta_i}'(\eta_i)\Lparen{\MyInd{\mat{K}_{22}}{ii} + \MyInd{\mat{K}_{12}}{ii}}(\Delta_i - h^\Delta_i(\eta_i))
\end{aligned}$$

where $\vec{x}_i$ is the $i$'th observation's covariate vector, $\eta_i = \vec{x}_i^T\vec{\alpha}_t$ is the linear predictor and $\Lparen{h^\Delta_i}'$ and $\Lparen{h^y_i}'$ are the first deriatives of the link function w.r.t. the linear predictor for the truncated waiting time and the binary outcome. Similarly, $h^\Delta_i$ and $h^y_i$ are the link functions the truncated waiting time and the binary outcome. We need the subscript because each observation can be add risk for different amount of time in the bin we are focusing on. Lastly, $(\cdot)_{ij}$ is the $(i,j)$'th entry of the matrix in the parenthsis. Finally, the infomration matrix can be computed by:

$$\begin{aligned}
\mat{U}_t(\vec{\alpha}_t) = \sum_{i=1}^r 
  &\vec{x}_i\Lparen{\Lparen{\Lparen{h^y_i}'(\eta_i)}^2\MyInd{\mat{K}_{11}}{ii} +
    \Lparen{h^y_i}'(\eta_i)\cdot\Lparen{h^\Delta_i}'(\eta_i)\MyInd{\mat{K}_{12}}{ii}}\vec{x}_i^T\\
  & \hspace{4pt}+\vec{x}_i\Lparen{\Lparen{h^y_i}'(\eta_i)\cdot\Lparen{h^\Delta_i}'(\eta_i)\MyInd{\mat{K}_{12}}{ii} +
    \Lparen{\Lparen{h^\Delta_i}'(\eta_i)}^2\MyInd{\mat{K}_{22}}{ii}}\vec{x}_i^T
\end{aligned}$$

We could implement the above. I have done that for the EKF. There is and issue though with divergence. The entries of $\mat{K}$ can become numerically large. In particular, we can show that: 

$$\mat{K} = \begin{pmatrix}
  \diag{\frac{1}{\sigma_{Y_1}^2(1 - \rho_1^2)}, \dots, \frac{1}{\sigma_{Y_r}^2(1 - \rho_r^2)}} &
    \diag{\frac{\rho_1}{\sigma_{Y_1}\sigma_{\Delta_1}(\rho_1^2 - 1)}, \dots, \frac{\rho_r}{\sigma_{Y_r}\sigma_{\Delta_r}(\rho_r^2 - 1)}} \\
  \diag{\frac{\rho_1}{\sigma_{Y_1}\sigma_{\Delta_1}(\rho_1^2 - 1)}, \dots, \frac{\rho_r}{\sigma_{Y_r}\sigma_{\Delta_r}(\rho_r^2 - 1)}} &
    \diag{\frac{1}{\sigma_{\Delta_1}^2(1 - \rho_1^2)}, \dots, \frac{1}{\sigma_{\Delta_r}^2(1 - \rho_r^2)}}
\end{pmatrix}$$

We can compute some numerical examples to illustrate why this is an issue. This is done below. `eta` refers to the linear predictor, `delta` refers to at the at risk length and `Delta` refers to the outcome:

```{r}
grid <- expand.grid(delta = c(.1, 1, 2, 3), eta = c(-2, -1, 0, 1))

grid$`Link Y` <- 1 - exp(- exp(grid$eta) * grid$delta)
grid$`Var Y` <- grid$`Link Y` * (1 - grid$`Link Y`)
grid$`Deriv link Y` <- exp(- exp(grid$eta) * grid$delta) * exp(grid$eta) * grid$delta


grid$`Link Delta` <- (1 - exp(- exp(grid$eta) * grid$delta)) / exp(grid$eta)
grid$`Var Delta` <- (1 - exp(- 2 * exp(grid$eta) * grid$delta) 
                     - 2 * exp(grid$eta) * grid$delta 
                     * exp(- exp(grid$eta) * grid$delta)) / exp(2 * grid$eta)
grid$`Deriv link Delta` <- exp(- exp(grid$eta) * grid$delta) * (
  1 - exp(exp(grid$eta) * grid$delta) + exp(grid$eta) * grid$delta) / 
    exp(grid$eta)

grid$Covariance <- - exp(- 2 * exp(grid$eta) * grid$delta) * 
  (1 + exp(grid$eta) * grid$delta * exp(exp(grid$eta) * grid$delta)
   - exp(exp(grid$eta) * grid$delta)) / exp(grid$eta)

grid$Correlation <- grid$Covariance / sqrt(grid$`Var Delta` * grid$`Var Y`)

grid$K11 <- 1 / (grid$`Var Y` * (1 - grid$Correlation^2))
grid$K22 <- 1 / (grid$`Var Delta` * (1 - grid$Correlation^2))
grid$K12 <- grid$Correlation / (sqrt(grid$`Var Y` * grid$`Var Delta`) * 
                   (grid$Correlation^2 - 1))

knitr::kable(grid[, 1:8], digits = 3)
```
```{r}
knitr::kable(grid[, -(3:8)], digits = 3)
```

Where we see that certain combinations of $(\delta, \eta)$ cause $\mat{K}$ are large. We will illustrate why this may be an issue in the following paragraphs. Notice that the deriatives of the two link functions have opposing signs while all the entries of $\mat{K}$ are positive. Thus, there is a canceling effect in the computation of $\mat{U}_t(\vec{\alpha}_t)$. Hence, $\emNotee{\mat{V}}{t}{t}$ seems not to be an issue in the sense that it "stays around" sensible values before divergence

The update from $\emNotee{\vec{a}}{t}{t-1}$ to $\emNotee{\vec{a}}{t}{t}$ seems to be cause divergence. Re-call that:

$$\emNotee{\vec{a}}{t}{t} = \vec{a} + l \cdot \emNotee{\mat{V}}{t}{t}\vec{u}_t (\vec{a})$$
```{r, echo=FALSE}
ex <- 3
```

Supose now that we have somewhat rare events, the linear predictor $\eta$ is `r grid$eta[ex]`, $\delta$ is `r grid$delta[ex]`  and the indvidual with $\Delta = 2$. The latter can occour often if times are reported discritly (in say days or months). Then the factor from the residual in the term  with $(y_i - h_i^y(\eta_i))$ is the product of `r grid[ex, "Deriv link Y"]` ($\Lparen{h_i^y(\eta_i)}'$), (`r grid[ex, "K11"]` plus `r grid[ex, "K12"]`) ($\MyInd{\mat{K}_{11}}{ii} + \MyInd{\mat{K}_{12}}{ii}$) and ($y_i$ minus `r grid[ex, "Link Y"]` ($h_i^y(\eta_i)$)). The result is `r grid[ex, "Deriv link Y"] * (grid[ex, "K11"] + grid[ex, "K12"]) * (1 - grid[ex, "Link Y"])` if it is a death and `r grid[ex, "Deriv link Y"] * (grid[ex, "K11"] + grid[ex, "K12"]) * (0 - grid[ex, "Deriv link Y"])` otherwise

The factor for the residual time is the product of `r grid[ex, "Deriv link Delta"]`, (`r grid[ex, "K22"]` plus `r grid[ex, "K12"]`) and (2 minus `r grid[ex, "Link Delta"]`). The result is `r grid[ex, "Deriv link Delta"] * (grid[ex, "K22"] + grid[ex, "K12"]) * (2 - grid[ex, "Link Delta"])`. Meanwhile the factor for the information matrix $\mat{U}_t(\vec{\alpha})_t$ is `r grid[ex, "Deriv link Y"]^2 * grid[ex, "K11"] + 2 * grid[ex, "Deriv link Y"] * grid[ex, "Deriv link Delta"] * grid[ex, "K12"] + grid[ex, "Deriv link Delta"]^2 * grid[ex, "K22"]` ($\Lparen{\Lparen{h^y_i}'(\eta_i)}^2\MyInd{\mat{K}_{11}}{ii} + 2\cdot\Lparen{h^y_i}'(\eta_i)\cdot\Lparen{h^\Delta_i}'(\eta_i)\MyInd{\mat{K}_{12}}{ii} + \Lparen{\Lparen{h^\Delta_i}'(\eta_i)}^2\MyInd{\mat{K}_{22}}{ii}$). Recalling that:

$$\emNotee{\mat{V}}{t}{t} = \left(\emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\emNotee{\vec{a}}{t}{t - 1})\right)^{-1}$$
Depending on the co-varaite vectors $\vec{x}_i$ and covariance matrix $\emNotee{\mat{V}}{t}{t - 1}$ this can imply that $\vec{u}_t(\vec{\alpha}_t)$ becomes large compared to $\emNotee{\mat{V}}{t}{t }$ which causes a large step in the score direction. One solution is to set the learning rate $l$ (very) low. Though, I have to stress that this has to be set it at a very low level 

A solution may be to make the two variables uncorrelated. For example, we can set:
$$\begin{aligned}
&\begin{pmatrix} y_i \\ \Delta_i + c y_i \end{pmatrix} = 
  \underbrace{\begin{pmatrix} 1 & 0 \\ c & 1 \end{pmatrix}}_C \begin{pmatrix} y_i \\ \Delta_i \end{pmatrix} \\
\Leftrightarrow\hspace{4pt}& 
  \covp{\begin{pmatrix} y_i \\ \Delta_i + c y_i \end{pmatrix}} = C 
    \begin{pmatrix}\sigma_{Y_i}^2 & \xi_i \\ \xi_i  & \sigma_{\Delta_i}^2\end{pmatrix} C^T
\end{aligned}$$

and get zero correlation by selecting $c$ such that:
$$\begin{aligned}
& c \sigma_{Y_i}^2 + \xi_i = 0 \\
\Leftrightarrow\hspace{4pt}& 
  c = -\frac{\xi_i}{\sigma_{Y_i}^2} = -\frac{\exp\Lparen{-\eta _i} \left(\delta _i \left(-\exp\Lparen{\eta
   _i}\right)-\exp\Lparen{\delta _i \left(-\exp\Lparen{\eta _i}\right)}+1\right)}{1-\exp\Lparen{\delta _i
   \left(-\exp\Lparen{\eta _i}\right)}}
\end{aligned}$$

Though, this requires that we know the $\vec{\alpha}_t$ in order to compute $\eta_i$. Question: Would it be ok to plug in the current estimate of $\emNotee{\vec{a}}{t}{t - 1}$? Do you think this is a good idea?

## EKF
The EKF method for the exponential model has shown to have issues with divergence. The cause seem to be overstepping in the correction step. Hence, we can set the learning rate below 1. For example, you can do so by passing `list(LR = .5, ...)` to the `control` argument of `ddhazard` to gain a learning rate of $0.5$

The direct formulas for the conditional mean, conditional variance and derivatives of the conditional mean are numerically unstable for certain combinations of interval lengths ($t_{i,s} - t_{i, s - 1}$) and the linear predictor ($\eta = \vec{x}_{is}^T \vec{\alpha}_{\Lceil{t_{i,s}}}$). In particular they are subject to cancellation. This can cause both high relative errors and even wrong sign in some of the cases. Thus, Lauren series or Tylor series are used for critical points. This requires no action for user but is stressed for readers who worry after seeing both the conditional mean and conditional variance of $\Delta_{i,s}$

## UKF
TODO: WRITE ONCE QUESTION ABOUT CORRELATION IS SOLVED

## Fixed effects
Fixed effects in the M-step are estimated using a Poisson model with an offset equal to the logarithm of the time observed in each bin plus the estimated offset from time varying effects. That is, we use that if an arrival time $T$ is exponential distributed with rate $\lambda$ then having an outcome at at time $t$ is Poisson distributed $Y\sim\text{Poisson}(\lambda t)$. For example, say that we fit the following model:

```{r}
# The data we use
head(data_frame)
```

```{r, eval=FALSE}
# The fit
fit <- ddhazard(Surv(tstart, tstop, y) ~ x1 + ddFixed(x2), data_frame, 
                by = 1, # bin lengths are 1
                id = data_frame$id, model = "exponential")
```

Take the individual with  `id = 1`. As in the logistic model, he will yield four observations in the M-step. Each will have an offset of $\log (1) = 0$ because the interval length is $1$ plus $\emNotee{\vec{a}}{t}{d}$ times the value of `x1`. Say instead that the data frame was:

```{r, echo=FALSE}
data_frame_new <- data_frame
data_frame_new[1, 2:3] <- c(.5, 2)
data_frame_new <- rbind(c(1, 0, .5, 0, .43, .33),
                        data_frame_new)
```

```{r}
head(data_frame_new)
```

Then individual 1 would yield five observation. The first row would only have an offset $\log 0.5$ plus $\emNotee{\vec{a}}{1}{d}$ times `r data_frame_new$x1[1]`. The second row will yield two observations: one with an offset of $\log 0.5$ plus $\emNotee{\vec{a}}{1}{d}$ times `r data_frame_new$x1[2]` and the other with an offset $\log 1$ plus $\emNotee{\vec{a}}{2}{d}$ times `r data_frame_new$x1[2]`. Note that this is not the case with the logistic model as we have the same binning issue as described in the Binning section

## Starting value
QUESTION: Should I write about the static model that can be used for $\vec{\alpha}_0$?

# Further tasks and ideas
The last section will cover further task and ideas. Please, let me know what you think. Is it relevant, got ideas to the question I pose and how would you priorities?

## Comparisson of other package
Look further at the vignette I have made where I compare with other packages. The goal of the vignette is to: 

1. Show how the API for package works
2. Show how these modal compares with other models
3. Illustrate the pros and cons of the model I have implemented compared to other models 

## Confidence bounds
How do we construct confidence bounds both for the state vectors and for the predicted values?

## Diagnostics
I am thinking of making a second vignettes with diagnostic. It will contain raw residuals, Pearson residuals, non-standardized and standardized state space error. The motivation to keep it in a separate vignette is to include more examples and keep it as more of an example of how to use the package

Further, I need to look into tests the effects are time varying or not. One idea is to test entries in $\mat{Q}$. Though, this involves tests on the boundary of the parameter space. Another idea is to make an F-test if we can estimate a model where only some of the parameters are fixed. This thread here suggest the idea when make all the parameter time invariant [http://stats.stackexchange.com/a/161917](http://stats.stackexchange.com/a/161917)

## Other state equations
We can replace the state equation with other models then a given order random walk. For example, we can replace it with a stationary process: 
$$\vec{\alpha}_t = \vec{\mu} + \mat{F} \vec{\alpha}_{t - 1} + \mat{R}\vec{\eta}_t$$

where we require $\mat{F}$ is such that the process is stable. $\mat{F}$ and $\vec{\mu}$ can be estimated in the M-step with closed form solutions when the noise is Gaussian. Both the UKF end EKF can be adapted to this setting. Further, we can change the distribution of $\vec{\eta}$ or change make a non-linear dependence between  $\vec{\alpha}_t$ and $\vec{\alpha}_{t - 1}$

# References

