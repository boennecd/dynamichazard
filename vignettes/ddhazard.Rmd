---
title: "ddhazard"
output: 
  pdf_document: 
    fig_caption: yes
author: "Benjamin Christoffersen"
header-includes:
   - \usepackage{bm}
bibliography: ddhazard_bibliography.bib
---

```{r setup, include=FALSE}
options(digist = 4)
knitr::opts_chunk$set(echo = TRUE)
```

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%
\newcommand{\Lparen}[1]{\left( #1\right)} 
\newcommand{\Lbrack}[1]{\left[ #1\right]} 
\newcommand{\Lbrace}[1]{\left \{ #1\right \}} 
\newcommand{\Lceil}[1]{\left \lceil #1\right \rceil}
\newcommand{\Lfloor}[1]{\left \lfloor #1\right \rfloor}
%
\newcommand{\propp}[1]{P\Lparen{#1}}
\newcommand{\proppCond}[2]{P\Lparen{\left. #1 \right\vert  #2}}
%
\newcommand{\expecp}[1]{E\Lparen{#1}}
\newcommand{\expecpCond}[2]{E\Lparen{\left. #1  \right\vert  #2}}
%
\newcommand{\varp}[1]{\textrm{Var}\Lparen{#1}}
\newcommand{\varpCond}[2]{\textrm{Var} \Lparen{\left. #1 \right\vert  #2}}
%
\newcommand{\corpCond}[2]{\textrm{Cor} \Lparen{\left. #1 \right\vert  #2}}
%
\newcommand{\covp}[1]{\textrm{Cov} \Lparen{#1}}
%
\newcommand{\emNotee}[3]{#1_{\left. #2 \right\vert #3}}
\newcommand{\emNote}[4]{#1_{\left. #2 \right\vert #3}^{(#4)}} 
%
\newcommand{\ukfNote}[2]{\mat{P}_{\vec{#1}, \vec{#2}}}
\newcommand{\ukfNotee}[3]{\mat{P}_{\vec{#1}_{#3}, \vec{#2}_{#3}}}
%
\newcommand{\diag}[1]{\text{diag}{\Lparen{#1}}}
\newcommand{\wvec}[1]{\widehat{\vec{#1}}}
\newcommand{\wmat}[1]{\widehat{\mat{#1}}}
%
\newcommand{\deter}[1]{\left| #1 \right|}
%
\newcommand{\MyInd}[2]{\Lparen{#1}_{#2}}

# Intro
This note will cover the `ddhazard` function in `dynamichazard` library. You can install the the library through github for the `devtools` library as follows:

```{r eval=FALSE}
devtools::install_github("boennecd/dynamichazard")
```

The `ddhazard` function estimates a binary regression model where the parameters are assumed to time-variant and follow a pre-defined random walk. The function is implemented such that: 

1) The time complexity of the computation is linear in the number of observations
2) The dimension of the observation equation (this will be defined soon) can vary through time
3) It is fast due and scalable due the `C++` implementation which uses `Armadillo` library and use of multithreading through the standard library `thread` 

We will briefly introduce the in model in the following paragraphs. Let $\vec{x}_{it}$ denote the co-variate vector for individual $i$ at time $t$ and let $Y_{it}$ be the random variable for whether the $i$'th individual dies within time $(t-1, t]$. Denote the parameters at time $t$ by $\vec{\alpha}_t$. For given parameters at time $t$ the probability of death is:

$$\begin{aligned}
  \vec{y}_{it} = (y_{i1}, \dots, y_{it})^T, 
  \quad \mat{X}_{t} = \Lparen{\vec{x}_{i1}^T, \dots, \vec{x}_{it}^T}^T \\
  \proppCond{Y_{it} = 1}{\vec{y}_{i,t-1}, \mat{X}_{t}, \vec{\alpha}_t} = h(\vec{\alpha}_t^T \vec{x}_{it})
\end{aligned}$$

where $h$ is the link function. For example, this could be the logistic function such that $H(\eta) = \exp(\eta) / (1 + \exp(\eta))$.

The `ddhazard` function estimates models in the state space form:
$$\begin{array}{ll}
  \vec{y}_t = \vec{z}_t(\vec{\alpha}_t) + \vec{\epsilon}_t \qquad & 
  \vec{\epsilon}_t \sim (\vec{0}, \mat{H}_t(\vec{\alpha}_t))  \\
  \vec{\alpha}_{t + 1} = \mat{F}\vec{\alpha}_t + \mat{R}\vec{\eta}_t \qquad & 
  \vec{\eta}_t \sim N(\vec{0}, \mat{Q}) \\
\end{array} , \qquad t = 1,\dots, n$$

$\vec{y}_t$ is the binary outcomes and the associated equation is the *observational equation*. $\sim (a,b)$ denotes a random variable(s) with mean (vector) $a$ and variance (co-variance matrix) b. It needs not be a normal distribution. $\vec{\alpha}_t$ is the state vector with the corresponding *state equation*

The mean $\vec{z}_t(\vec{\alpha}_t)$ and variance $\mat{H}(\vec{\alpha}_t)$ are state dependent with:
$$\begin{aligned}
  z_{it}(\vec{\alpha}_t)=\expecpCond{Y_{it}}{\vec{\alpha}_t} = h(\vec{\alpha}_t^T \vec{x}_{it}) \\
  H_{ijt}(\vec{\alpha}_t) = \left\{\begin{matrix}
    \varpCond{Y_{it}}{\vec{\alpha}_t} & i = j \\ 0 & \textrm{otherwise}
  \end{matrix}\right.
\end{aligned}$$

The state equation is implemented with a 1. and 2. order random walk. For the first order random walk $\mat{F} = \mat{R} = \mat{I}_m$ where $m$ is the number of regression parameters and $\mat{I}_m$ is the identity matrix with dimension $m$. As for the second order random walk, we have:
$$\mat{F} = \begin{pmatrix} 
  2\mat{I}_m & - \mat{I}_m \\ \mat{0}_m & \mat{I}_m
\end{pmatrix},  \qquad 
\mat{R} = \begin{pmatrix} \mat{I}_m \\ \mat{0}_m \end{pmatrix}$$

where $\mat{0}_m$ is a $m\times m$ matrix with zeros in all entries. The models are estimated by an Extended Kalman filter (EKF) or an Unscented Kalman filter (UKF). The method is chosen by passing a list to the `control` argument of `ddhazard` with `list(method = "EKF", ...)` or `list(method = "UKF", ...)` respectively

Either methods require a vector $\vec{\alpha}_0$, co-variance matrix $\mat{Q}$ and co-variance matrix $\mat{Q}_0$ to start the filters. They can be estimated with an EM-algorithm as suggested in  [@fahrmeir94]. [@fahrmeir94] states that $\mat{Q}_0$ can be estimated to. Though, it does not seem possible and doing as [@fahrmeir94] will make all the elements tend towards zero. Hence, the default is not to estimate the co-variance matrix $\mat{Q}_0$ and only estimate the state vector $\vec{\alpha}_0$ and co-variance matrix $\mat{Q}$. You can estimate $\mat{Q}_0$ in the way [@fahrmeir94] describes by setting the `est_Q_0` element of the `control` to `TRUE` (`list(est_Q_0 = T, ...)`)

A key thing to notice is that the `Q` argument for $\mat{Q}$ is scaled by the length of the bins. That is, say that we call `ddhazard` with `by = 10` and pass `Q = diag(rep(1, 4))` if we have four co-variates. Then the $\mat{Q}$ in the in the state equation is $\mat{Q} = 10 \cdot \text{diag}((1, 1, 1, 1))$ where $\text{diag}(\cdot)$ is a function that takes a vector and return a diagonal matrix with the vector in the diagonal. The motivation for this behavior is that you can alter the `by` argument and get comparable estimates of $\mat{Q}$. Further, it will also be usefull if unequal bin length are implemented later. As a last comment in this context, `Q_0` is not scaled and thus will exactly match $\mat{Q}_0$ in the estimation

The rest of this note is structured as follows. The section 'Example of usage' will show how to quickly fit a model. This is followed by the section 'EM algorithm' where the EM algorithm will be explained. The sections 'Extended Kalman Filter' and 'Unscented Kalman Filter' respectively covers the EKF and UKF used in the E-step of the EM algorithm. Finally, we end with the sections 'Logistic model' and 'Exponential model' which cover the models

# Example of usage
Question: Should I include a a quick example here or leave examples to a separate vignettes? 

# EM algorithm
An EM algorithm is used to estimate the initial state space vector $\vec{\alpha}_0$ and the co-variance matrix $\mat{Q}$. Optionally $\mat{Q}_0$ is also estimated if `control = list(est_Q_0 = T, ...)`. We will need a short hand for the conditional means and co-variances to ease the notation. Define

$$\emNotee{\vec{a}}{t}{k} = \expecpCond{\vec{\alpha}_t}{\mat{Y}_k},  \qquad
\emNotee{\mat{V}}{t}{k} = \expecpCond{\mat{V}_t}{\mat{Y}_k}, \qquad  \mat{Y}_k = (\vec{y}_{1k}, \dots, \vec{y}_{sk}) $$

for the conditional mean and co-variance matrix where $s$ is the number of observations. Notice that the letter 'a' is used for estimates while alpha is used for the unknown state. Further, these can both be filter estimates in the case where $k \leq t$ and smoothed estimates when $k > t$. We suppress the dependence of the co-variates ($\vec{x}_{it}$) here to simplify the notation

The initial values for $\vec{\alpha}_0$, $\mat{Q}$ and $\mat{Q}_0$ can be set by passing a vector to `a_0` argument of `ddhazard` for $\vec{\alpha}_0$ and matrices to `Q_0` and `Q` argument of `ddhazard` for respectively $\mat{Q}_0$ and $\mat{Q}$ 

## E-step
The outcome of the E-step is smoothed estimates:
$$\emNote{\vec{a}}{i}{T}{k},  \quad 
  \emNote{\mat{V}}{i}{T}{k}, \quad 
  \mat{B}_j^{(k)} = \emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}\emNotee{\mat{V}}{t}{t - 1}^{-1},  \quad 
  i=0,1,\dots,T\wedge j = 1,2,\dots,T$$
  
where $T$ is the end of the last period we observe and superscripts $\cdot^{(k)}$ is used to distinguish the estimates from each iteration of the EM-algorithm. The required input to start the E-step is an initial mean vector $\widehat{\vec{a}}_0^{(k-1)}$ and co-variance matrices $\widehat{\mat{Q}}_0^{(k - 1)}$ and $\widehat{\mat{Q}}^{(k - 1)}$. Given these input, we compute the following estimates either by using the EKF or UKF:
$$\emNotee{\vec{a}}{j}{j-1}, \quad 
  \emNotee{\vec{a}}{i}{i}, \quad 
  \emNotee{\mat{V}}{j}{j - 1}, \quad 
  \emNotee{\mat{V}}{i}{i}, \quad i =0,1,\dots,T\wedge j=1,2,\dots,T$$
  
Then the estimates are smoothed by computing:
$$\begin{aligned}
  & \mat{B}_t^{(k)} = \emNotee{\mat{V}}{t - 1}{t - 1} \mat{F} \emNotee{\mat{V}}{t}{t - 1}^{-1} \\
  & \emNote{\vec{a}}{t - 1}{d}{k} = \emNotee{\vec{a}}{t - 1}{t - 1} + \mat{B}_t (
    \emNote{\vec{a}}{t}{d}{k} - \emNotee{\vec{a}}{t}{t - 1}) \\
  & \emNote{\mat{V}}{t - 1}{d}{k} = \emNotee{\mat{V}}{t - 1}{t - 1} + \mat{B}_t (
    \emNote{\mat{V}}{t}{d}{k} - \emNotee{\mat{V}}{t}{t - 1}) \mat{B}_t^T
\end{aligned} \qquad t = T,T-1,\dots, 1$$

## M-step
The M-step updates the mean $\widehat{\vec{a}}_0^{(k - 1)}$ and co-variance matrices $\widehat{\mat{Q}}_0^{(k - 1)}$ and $\widehat{\mat{Q}}_0^{(k - 1)}$ (the latter being optional). These are computed by:
$$\begin{aligned}
\widehat{\vec{\alpha}}_0^{(k)} &= \emNote{\vec{a}}{0}{d}{k}, \qquad
    \widehat{\mat{Q}}_0^{(k)} = \emNote{\mat{V}}{0}{d}{k} \\
  \widehat{\mat{Q}}^{(k)} &= \frac{1}{d}	\sum_{t = 1}^d \mat{R}^T\left( 
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^T \right. \\
    &\hspace{57pt} + \emNote{\mat{V}}{t}{d}{k} - \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} - 
      \left( \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} \right) ^T + 
      \mat{F}\emNote{\mat{V}}{t - 1}{d}{k}\mat{F}^T
      \left. \vphantom{\left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^T} \right)\mat{R}
\end{aligned}$$

We test the relative norm of the change in the initial state vector to check for convergence. The convergence criteria is given by $\rVert \widehat{\vec{\alpha}}_0^{(k)} - \widehat{\vec{\alpha}}_0^{(k - 1)} \lVert / (\rVert \widehat{\vec{\alpha}}_0^{(k - 1)} \lVert + \delta) < \epsilon$  where $\delta$ is a small number and $\epsilon$ is a pre-specified threshold. You can select $\epsilon$ by changing setting the `eps` element of the list passed to the `control` argument of `ddhazard` (e.g. `list(eps = 0.01, ...)`)

# Kalman Filter
The standard Kalman filter is carried out by recursively doing two steps. This also applies for the EKF and UKF. Thus, this paragraph is included to introduce general notions. The first step is in the Kalman Filter is the *filter step* where we estimate $\emNotee{\vec{a}}{t}{t - 1}$ and $\emNotee{\mat{V}}{t}{t - 1}$ based on $\emNotee{\vec{a}}{t - 1}{t - 1}$ and $\emNotee{\mat{V}}{t - 1}{t - 1}$. Secondly, we carny out the *correction step* where we estimate $\emNotee{\vec{a}}{t}{t}$ and $\emNotee{\mat{V}}{t}{t}$ based on $\emNotee{\vec{a}}{t}{t - 1}$ and $\emNotee{\mat{V}}{t}{t - 1}$ and the observations. We then repeat the process until $t=T$

# Extended Kalman Filter
The idea for the Extended Kalman filter is to replace the observational equation with a first order Taylor expansion. This approximated model can then be estimated with a regular Kalman Filter. The EKF presented here is originally described in [@fahrmeir94] and [@fahrmeir92] 

The formulation in [@fahrmeir94] differs from the standard Kalman Filter by re-writing the correction step using the Woodbury matrix identity. This has two computational advantages. The first one is that the time complexity is $O(p)$ instead of $O(p^3)$ where $p$ denotes the dimension of the observation equation. Secondly, we do not have store an intermediate $p\times p$ matrix

The EKF starts with filter step where we compute:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}, \quad \\
  \emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^T + \mat{R}\mat{Q}\mat{R}^T
\end{aligned}$$
	  
Secondly, we perform the correction step by:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1} + \emNotee{\mat{V}}{t}{t}\vec{u}_t (\emNotee{\vec{a}}{t}{t - 1})\\
  \emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\emNotee{\vec{a}}{t}{t - 1})\right)^{-1}
\end{aligned}$$

where $\vec{u}_t (\emNotee{\vec{a}}{t}{t - 1})$ and $\mat{U}_t (\emNotee{\vec{a}}{t}{t - 1})$ are given by:

$$\begin{aligned}
  & \vec{u}_t (\vec{\alpha}_t) = \sum_{i \in R_t} \vec{u}_{it} (\vec{\alpha}_t) \nonumber \\
  & \vec{u}_{it} (\vec{\alpha}_t)= \left. \vec{z}_{it} \frac{\partial h(\eta)/ \partial \eta}{H_{iit}(\vec{\alpha}_t)} \Lparen{y_{it} - h(\eta)} \right\vert_{\eta = \vec{z}_{it}^T \vec{\alpha}_t} \\
	& \mat{U}_t (\vec{\alpha}_t) = \sum_{i \in R_t} \mat{U}_{it} (\vec{\alpha}_t) \nonumber \\
	& \mat{U}_{it} (\vec{\alpha}_t) = \left. \vec{z}_{it} \vec{z}_{it}^T 
		 \frac{\left( \partial h(\eta)/ \partial \eta \right)^2}{H_{iit}(\vec{\alpha}_t)} \right\vert_{\eta = \vec{z}_{it}^T \vec{\alpha}_t}
\end{aligned}$$

$R_t$ is the set of indices of individuals who are add risk at time $t$. It is commonly referred to as the risk set. Thus, the dimension the observational equation can vary as individual dies or are right censored

## Divergence
Initial testing shows that the EKF has issues with divergence for some data set. The cause of divergence seems to be overstepping in the correction step where we update $\emNotee{\vec{a}}{t}{t}$. In particular, the signs of the margins of $\emNotee{\vec{a}}{t}{t}$ tends to alter between $t-1, t, t+1$ etc. and the absolute values tends to increase when the algorithm diverges.  The following section describes a solution to this issue

[@fahrmeir92] mentions that the filter step can be viewed as a single Fischer Scoring step (and hence a step in a Newton Raphson method). This motivates:

1) To take multiple steps if $\emNotee{\vec{a}}{t}{t}$ is far from $\emNotee{\vec{a}}{t}{t-1}$
2) Introduce a learning rate

Simulated datasets show that the learning rate solves the issues with divergence. Let $l>0$ denote the learning rate and $\epsilon_\text{NR}$ denote the tolerance of the for the scoring step. We then set $\vec{a} = \emNotee{\vec{a}}{t}{t-1}$ and compute:

$$\begin{aligned}
  &\emNotee{\vec{a}}{t}{t} = \vec{a} + l \cdot \emNotee{\mat{V}}{t}{t}\vec{u}_t (\vec{a})\\
  &\emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\vec{a})\right)^{-1} \\
  &\text{if } \rVert \emNotee{\vec{a}}{t}{t} - \vec{a} \lVert / (\rVert \vec{a} \lVert + \delta) < \epsilon_{\text{NR}} 
    \text{ then exit}\\
  &\text{else set } \vec{a} = \emNotee{\vec{a}}{t}{t} \text{ and repeat} 
\end{aligned}$$

where $\delta$ is small like $10^{-9}$. The defaults are $l = 1$ and $\epsilon_{\text{NR}} = \infty$. Selecting $l < 1$ in case of divergence seems to help. Further, while [@fahrmeir92] does not observe improvements with multiple repetitions, we find improvements in terms of mean square error of the state vector by taking multiple steps (for example by setting $\epsilon_{\text{NR}} = 10^{-2}$ or lower)

$l$ and $\epsilon_{\text{NR}}$ are set by respectively setting the elements `LR` and `NR_eps` of the list passed to `control` argument of `ddhazard`. By default, `LR = 1` and `NR_eps = NULL` which yields a learning rate of 1 and a single Fischer scoring step (which is the same as $\epsilon_{\text{NR}=\infty}$. These arguments can be altered by setting `control = list(LR = .75, NR_eps = 0.001)` for a learning rate of 0.75 and a threshold in the Fischer Scoring of $10^{-3}$

## Parallel BLAS or LAPACK
All the computations use objects from the `Armadillo` library. Thus, an optimized version LAPACK and BLAS can speed up the computation

A multithreaded version of LAPACK or BLAS can cause issues with performance. The majority of the computation time is spend in the correction step of the EKF when have to compute $\vec{u}_t (\vec{\alpha}_t)$ and $\mat{U}_t (\vec{\alpha}_t)$ when the number of regression parameter is low and we have a lot of observations. For this reason, this part of the code is computed in parallel with the standard library 'thread'. The reduction in computation time can mitigated if multithreaded version of LAPLACK or BLAS is used as the code already use multithreading

A very specific solution to the issues is implemented for windows users who compiles with openBLAS. the `src/Makevars.win` checks if there is `C:\OpenBLAS` folder. If so, we assume that the structure is:
```
C:/OpenBLAS/
|--lib/
   |--libopenblas.a
|--include/
   |--cblas.h
   |--f77blas.h
```

The code will be compiled with this `openBLAS` instead of the `BLAS` library used to compile `R`. This will allow parts of the matrix operations to be run in parallel to `openBLAS` for multithreading. The number of threads openBLAS will use is set to 1 before the part that use `thread` is run and reset after the this part is completed

## Recommandations
In general, choosing $\mat{Q}_t = k\mat{I}$ with a large value $k$ and Choose large $\mat{Q} = d\mat{I}$ for $d$ small seems to work for a lot of data set. The exact value of $k$ and $d$ seems not to be too important. For instance, one dataset yielded almost the same result in terms of mean square error when $k \in [1, 10^3]$ and $d \in [10^{-6},10^{-1}]$


# Uncented Kalman Filter
The UKF selects state vectors called *sigma point* with given *sigma weigths* such that to match the moments of  observational equation. The motivation to use the UKF in place of the EKF as we avoid linerization error in the EKF and choose hyperparameters to approximate moments of the observational equation. [@julier97] introduces a UKF that approximate the first two moments and up to fourth moment in certain settings. [@julier04] further develops the UKF and extend to what is later referred to as *the Scaled Unscented Transformation*. We will cover the the Scaled Unscented Transformation.

## Usual UKF formulation
We start by introducing a common notation used in the UKF. Let:
$$\ukfNotee{a}{b}{t}= \expecpCond{(\vec{a}_t - \overline{\vec{a}}_t)(\vec{b}_t - \overline{\vec{b}}_t)^T}{\mat{Y}_t}$$

$\ukfNote{\cdot}{\cdot}$ is useful short hand for the expected correlation matrices and expected co-variance matrix. Further, notice that $\mat{P}_{\vec{\alpha}_t, \vec{\alpha}_t} = \emNotee{\mat{V}}{t}{t}$. The UKF method proceeds as follows: We are given estimates $\emNotee{\vec{a}}{t - 1}{t - 1}$ and $\emNotee{\mat{V}}{t - 1}{t - 1}$. We then select $2m + 1$ *sigma points* (where $m$ is the dimension of the state equation) denoted by $\wvec{a}_0, \wvec{a}_1, \dots, \wvec{a}_{2m + 1}$ according to:

$$\begin{aligned}
  \wvec{a}_0 &= \mat{F}\emNotee{\vec{a}}{t-1}{t-1} \\
  \wvec{a}_{i} &= \mat{F}\left(\emNotee{\vec{a}}{t-1}{t-1} + \sqrt{m + \lambda} \left(\sqrt{\emNotee{\mat{V}}{t - 1}{t - 1}}\right)_i\right) \\
  \wvec{a}_{i + m} &= \mat{F}\left(\emNotee{\vec{a}}{t-1}{t - 1} - \sqrt{m + \lambda} \left(\sqrt{\emNotee{\mat{V}}{t - 1}{t - 1}}\right)_i\right)
\end{aligned} \qquad i = 1,2,\dots, n$$

where $\left(\sqrt{\emNotee{\mat{V}}{t - 1}{t - 1}}\right)_i$ is the $i$'th column of the lower triangular matrix of the Cholesky decomposition of $\emNotee{\mat{V}}{t - 1}{t - 1}$. We assign the following weights to each sigma point (we will cover selection of the hyperparameters $\alpha$, $\beta$ and $\kappa$ shortly):
$$\begin{aligned} 
  W_0^{(m)} &= \frac{\lambda}{m + \lambda} \\
  W_0^{(c)} &= \frac{\lambda}{m + \lambda} + 1 - \alpha^2 + \beta \\
  W_i^{(m)} &= W_0^{(c)} = \frac{1}{2(m+\lambda)}, \qquad i = 1,\dots, 2m \\
  \lambda &= \alpha^2 (m + \kappa) - m
\end{aligned}$$

Let $\vec{W}^{(j)} = (W_0^{(j)}, \dots, W_{2m}^{(j)})^T$ and $\wmat{A} = (\wvec{a}_0,\dots,\wvec{a}_{2m})$ The filter step given the sigma points and sigma weights is:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t-1} &= \sum_{i = 0}^{2m} W_i^{(m)} \wvec{a}_i \\
  \Delta\wmat{A} &= \wmat{A} - \emNotee{\vec{a}}{t}{t-1} \vec{1}^T \\
  \emNotee{\mat{V}}{t}{t-1} &= \mat{R}\mat{Q}\mat{R}^T + \sum_{i=0}^{2m} W_i^{(c)}
    (\mat{F}\wvec{a}_i - \emNotee{\vec{a}}{t}{t-1})(\wvec{a}_i - \emNotee{\vec{a}}{t}{t-1})^T \\
  & = \mat{R}\mat{Q}\mat{R}^T + \Delta\wmat{A}\diag{\vec{W}^{(c)}}\Delta\wmat{A}^T
\end{aligned}$$

where $\diag{\cdot}$ returns a diagonal matrix with the passed vectors values in the diagonal and $\vec{1}$ is a vector with one in all the entries. We then proceed to the correction step. We start by defining the following intermediates: 

$$\begin{aligned}
  \wvec{y}_i &= \vec{z}_t \left(\wvec{a}_i \right), \qquad i = 0,1,\dots, 2m \\
  \wmat{Y} &= (\wvec{y}_0, \dots, \wvec{y}_{2m}) \\
  \overline{\vec{y}} &= \sum_{i = 0}^{2m} W_i^{(m)} \vec{y}_i, \qquad
  \Delta\wmat{Y} = \wmat{Y} - \overline{\vec{y}} \vec{1}^T, \qquad 
  \wmat{H} = \sum_{i=0}^m W_i^{(c)}\mat{H}_t(\wmat{a}_i) \\
  \ukfNotee{y}{y}{t} &= \sum_{i=0}^m W_i^{(c)} \left(
    (\wvec{y}_i - \overline{\vec{y}})(\wvec{y}_i - \overline{\vec{y}})^T + \wmat{H}\right) \\
  &= \Delta\wmat{Y}\diag{\vec{W}^{(c)}}\Delta\wmat{Y}^T + \wmat{H} \\
  \ukfNotee{x}{y}{t} &= \sum_{i=0}^m W_i^{(c)} 
    (\wvec{a}_i - \emNotee{\vec{a}}{t}{t-1})(\wvec{y}_i - \overline{\vec{y}})^T \\
  &= \Delta\wmat{A}\diag{\vec{W}^{(c)}}\Delta\wmat{Y}^T
\end{aligned}$$

The final correction step is then:
$$\begin{aligned}
  \emNotee{\vec{a}}{t}{t} &= \emNotee{\vec{a}}{t}{t - 1} + \ukfNotee{x}{y}{t}\ukfNotee{y}{y}{t}^{-1}(\vec{y}_t - \overline{\vec{y}}) \\
  \emNotee{\mat{V}}{t}{t} &= \emNotee{\mat{V}}{t}{t} - \ukfNotee{x}{y}{t}\ukfNotee{y}{y}{t}^{-1}\ukfNotee{x}{y}{t}^T
\end{aligned}$$

## Re-writting
The above formulation has the draw back that we have to invert $\ukfNotee{y}{y}{t}^{-1}$ which is infeasible when the number observation is moderately large (say greater than 1000). We can re-write the above using the Woodbury matrix identity to get algorithm $O(\vert R_t \vert)$ instead of $O(\vert R_t \vert^3)$ where $R_t$ is the indices at risk in the $i$'th interval

The proposed correction step can be computed as:
$$\begin{aligned}
  \tilde{\vec{y}} &= \Delta \wmat{Y}^T \widehat{\mat{H}}^{-1}(\vec{y}_t - \overline{\vec{y}}) \\ 
    \mat{G} &= \Delta\wmat{Y}^T\widehat{\mat{H}}^{-1}\Delta\wmat{Y} \\
  \vec{c} &= \tilde{\vec{y}} - \mat{G}\left( \diag{\vec{W}^{(m)}}^{-1} + \mat{G}\right)^{-1} \tilde{\vec{y}} \\ 
    \mat{L} &= \mat{G} - \mat{G}\left( \diag{\vec{W}^{(c)}}^{-1} + \mat{G}\right)^{-1} \mat{G} \\
  \emNotee{\vec{a}}{t}{t} &= \emNotee{\vec{a}}{t}{t - 1} + \Delta\wmat{X}\diag{\vec{W}^{(c)}}\vec{c} \\
  \emNotee{\mat{V}}{t}{t} &= \emNotee{\mat{V}}{t}{t - 1} - 
    \Delta\wmat{X}\diag{\vec{W}^{(c)}}\mat{L}\diag{\vec{W}^{(c)}}\Delta\wmat{X}^T
\end{aligned}$$

where $\tilde{\vec{y}}$, $\mat{G}$, $\mat{L}$ and $\vec{c}$ are intermediates. The above algorithm is $O(\vert R_t \vert)$ since $\widehat{\mat{H}}$ is a diagonal matrix and all products involves at worst multiplication $m\times \vert R_t \vert$ or $\vert R_t \vert \times m$ matrices

## Selecting hyperparameters
We still need to select the hyperparameters $\kappa$, $\alpha$ and $\beta$. We will cover these in the given order. $\kappa$ is usually set to $\kappa = 0$ or $\kappa = 3 - m$ which [@julier97] state is a "*useful heuristic*" when the state equation is Gaussian and $\alpha = 1$. The default is $\kappa = m/\alpha^2 - m$ and can be altered by setting the list element `kappa` in the list passed as the `control` argument to `ddhazard`. For example, `control = list(kappa = 1, ...)` yields $\kappa = 1$. The default is set to ensure that $W_0^{(m)} = 0$ such that all weights are positive. Hence, we are guaranteed that $\emNotee{\mat{V}}{t}{t-1}$ and $\ukfNotee{y}{y}{t}$ are positive semi-definite. This follows since both are sum of outer products with positive weights and that $\widehat{\mat{H}}$ is a diagonal matrix with positive entries. Notice though that this means that $\alpha$ only affects $W_0^{(c)}$ which simplifies to $W_0^{(c)} = 1 - \alpha^2$. See [@julier02] for more details


$0<\alpha \leq 1$ controls the spread of the sigma points. Notice that $\lambda + m \rightarrow 0^+$, $w_0^{(c)},w_0^{(m)}\rightarrow -\infty$ and $w_i^{(c)}, w_i^{(m)} \rightarrow \infty$ ($i > 0$) as $\alpha \rightarrow 0^+$ with $\kappa = 0$. Thus, the lower the value of $\alpha$, the lower the spread but the higher the absolute weights. It is generally suggested to choose $\alpha$ small. The arguments hereof are provided in for example [@gustafsson12] and [@julier04]. For this reason, the default is $\alpha = 0.1$. The parameter can be altered through the `alpha` element of the list passed to the argument `control` of `ddhazard`.

Lastly, $\beta$ is a correction term to match the fourth-order term in the Taylor series expansion of the covariance of the observational equation. [@julier04] show in the appendix that the optimal value with a Gaussian state equation is $\beta = 2$. This is the default. It can be altered through the `beta` element of list passed to the argument `control` of `ddhazard`.

## Selecting starting values
Experience with different data sets and the UKF is that the method is sensitive to the starting values of $\mat{Q}$ and $\mat{Q}_0$ (where the latter may be fixed). The reason for divergence can be illustrated by the effect of $\mat{Q}_0$. We start the filter by setting $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$. Say that we set $\mat{Q}_0 = k \mat{I}_m$ and $\vec{a}_0 = \vec{0}$. Then the $i$'th column of the Cholesky decomposition $\emNotee{\mat{V}}{0}{0}$ is a vector with $\sqrt{k}$ in the $i$'th entry and zero in the rest of the entries. Suppose that we set $k$ large. Then the linear predictors computed with the $l < m +1$ sigma point is $k x_{j1l}$ where $x_{j1l}$ is the $l$'th entry of individuals $j$'s co-variate vector at time $1$. This can be potentially quite large in absolute terms if $x_{kjt}$ is moderately different from zero. This seems to lead to divergence in some cases. For instance in the logistic model where we end with either zero or one estimates for the outcome for most or all sigma points  

$\mat{Q}$ has a similar effect although it is harder to illustrate with a small example as it occurs as an intermediate in the UKF. Question is then how to select $\mat{Q}$ and $\mat{Q}_0$. At this point, I can suggest to pick at diagonal matrix for $\mat{Q}_0$ with a "*somewhat*" large values and $\mat{Q}_0$ to a diagonal matrix with small values. This is based on experience. Though, what is "*somewhat*" large and what is small dependent on the data set

# Fixed effects
This section will motivate the fixed effects (non time varying) and cover how they are estimated. We start with the latter. The likelihood we are working with can be written as follows by application of the markovian property of the model:
$$\begin{split}
	\proppCond{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}}{\vec{y}_{t},\dots,\vec{y}_T} &\propto 
		L\Lparen{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}} \\
	 & = p(\vec{\alpha}_0)\prod_{t = 1}^d \proppCond{\vec{\alpha}_t}{\vec{\alpha}_{t-1}}	
		\prod_{i \in \mathcal{R}_t} \proppCond{y_{it}}{\vec{\alpha}_t}
\end{split}$$

This is what we approximately find the expectation of in the E-step of the EM-algorithm where $\mat{Q}$, $\mat{Q}_0$ and $\vec{\alpha}_0$ are assumed fixed. Denote the entries of the latter two matrices and vector by $\vec{\theta}$. Then the M-step proceeds by maximizing $\vec{\theta}$ conditional on the estimates in the E-step. Expanding the log likelihood yields:  

$$\begin{aligned}
	\mathcal{L}\Lparen{\vec{\theta}} =    
		\log L \Lparen{\vec{\theta}} = & 
		- \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^T \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^T \mat{Q}^{-1} \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\ 
	&  - \frac{1}{2} \deter{\mat{Q}_0} - \frac{1}{2d} \deter{\mat{Q}} \\
	&  - \sum_{t = 1}^d \sum_{i \in R_t} l_{it}({\vec{\alpha}_t})
\end{aligned}$$

$$l_{it}(\vec{\alpha}_t) = y_{it} \log h(\vec{x}_{it}^T \vec{\alpha}_t) + (1 - y_{it}) 
	\log \Lparen{1 - h(\vec{x}_{it}^T \vec{\alpha}_t)}$$
	
Notice that the entries in $\vec{\theta}$ only appears in the first three lines while the fourth line only depends on the estimate from the E-step. Suppose now that we assume that some of the effects are fixed such that we replace the linear predictor $\vec{x}_{it}^T\vec{\alpha}_t$ by $\vec{x}_{it}^T\vec{\alpha}_t + \widetilde{\vec{x}}_{it}^T\vec{\gamma}$ where $\vec{\gamma}$ is the fixed effects and $\widetilde{\vec{x}}_{it}$ are the corresponding co-variates. The new definition of $l_{it}$ is:

$$l_{it}(\vec{\alpha}_t) = y_{it} \log h(\vec{z}_{it}^T \vec{\alpha}_t + \widetilde{\vec{x}}_{it}^T\vec{\gamma}) + (1 - y_{it}) 
	\log \Lparen{1 - h(\vec{x}_{it}^T \vec{\alpha}_t + \widetilde{\vec{x}}_{it}^T\vec{\gamma})}$$
	
Say that we fix $\vec{\gamma}$ doing the E-step and estimate $\vec{\gamma}$ doing the M-step. Then we observe that: 

1. $\widetilde{\vec{x}}_{it}^T\vec{\gamma}$ acts like offsets doing the E-step. Thus, we only need to add these offsets in the UKF or EKF
2. $\vec{\gamma}$ is estimated separately from $\vec{\theta}$ in the M-step. Thus, no changes are needed in the update formulas for $\mat{Q}$, $\mat{Q}_0$ and $\vec{\alpha}_0$
3. $\vec{x}_{it}^T \vec{a}_t$ acts like offsets in the M-step when we estimate $\vec{\gamma}$
4. The optimization of $\vec{\gamma}$ in the M-step is a generalized linear model for distributions from the exponential family 

## Implementation
Point number 4 above implies that we can use a typical Newton Raphson algorithm to compute an updated estimate of $\vec{\gamma}$ when we are using a distribution from the exponential family. This can be solved by a QR decomposition as done in `glm`. However, point 3 implies that every observation will have a different offset in every bin the observation is in. Thus, we can end with a large design matrix

To overcome the memory issue this package use the same Fortran function that `bigglm` function in the `biglm` package uses. The Fortran function recursively performs a QR update for each row in the design matrix. Hence, we do not need to store the entire design matrix at any given point. The Fortran code is described in [@miller1992] and written by the author. It is an updated version of the algorithm described in [@gentleman1972] which has a time complexity of $O(\vert\vec{\gamma}\vert^2)$ for the QR-update of each row in the design matrix

Surely, other methods to solve the QR problem or fit a generalized linear model could be used that does not require us to store the entire design matrix and are faster and/or more stable. An example could be the algorithm described in [@hammarling08]. The current method is used since it has shown to work well in the `bigglm` function and as we assume that few parameters will be fixed. Thus, the $O(\vert\vec{\gamma}\vert^2)$ cost of doing the M-step should not be an issue

Fixed terms can be estimated by wrapping the co-variates in the formula of `ddhazard` in the `ddFixed` function. As an example, `ddhazard(Surv(tstart, tstop, y) ~ x1 + ddFixed(x2), ...)` will fit a model where `x1` is time varying and `x2` is not. The M-step recursively updates the $\vec{\gamma}$ starting with the previous estimated value. The estimation stops when $\rVert\vec{\gamma}^{(k)} - \vec{\gamma}^{(k - 1)}\lVert / (\rVert\vec{\gamma}^{(k - 1)}\lVert + \delta) < \epsilon$ where superscript denotes the estimation number, $\epsilon$ is the tolerance and $\delta$ is a small number. $\epsilon$ can be changed by setting  `eps_fixed_parems` element of the list passed to the `control` argument of `ddhazard`. 

The estimation will stop if the criteria given by $\epsilon$ is not meet within a given number of iterations. The maximum number of iteration can be set by setting the `max_it_fixed_parems` element of the `control` argument to `ddhazard`. The user will be warned if the criteria is not meet within `max_it_fixed_parems` iterations. The estimation though continues with the latest estimate of $\vec{\gamma}$ in the E-step 

## Why use fixed effects
Question: Should I motivate the use of fixed effects or is this obvious?

# Logistic model
The logistic model is fitted with by setting `model = "logit"` in the call to `ddhazard`. The link function $h$ is defined as inverse logit function $h(\eta) = \exp(\eta) / (1 + \exp(\eta))$. The following paragraphs will cover the a few comments to the EKF and UKF implementation. This is followed by some comments about the loss of information due to binning which motivates the exponential model

## EKF
From a numerical point of view, each individual computation are fairly stable with EKF method for the logistic model. The main reason is that:
$$\left. \partial h(\eta)/ \partial \eta \right\vert_{\eta = \vec{z}_{it}^T \vec{\alpha}_t} = H_{iit}(\vec{\alpha}_t)$$

Given the variance $\mat{H}_t(\vec{\alpha}_t)$ and expected mean $h(\eta)$ are bound this means that all the terms are on a reasonably stable for all values of the linear predictor $\vec{\alpha}_t^T\vec{x}_{it}$

## UKF
This is not the case for the UKF method and the logistic model. We scale by $\wmat{H}^{-1}$ when we compute $\mat{G}$ and $\tilde{\vec{y}}$ which will approach Infinity as linear predictor get large in absolute value. For this reason, we truncate the linear predictor $\eta_{it} = \vec{\alpha}_t^T\vec{x}_{it}$ such that the variance cannot be less than some pre-specified quantity $\delta$. Effectively this means that we set:
$$\begin{aligned}
  &h(\eta)(1-h(\eta) \geq \delta, \qquad \delta \in (0, 1/4) \\
  \Leftrightarrow \quad & \log \left( \frac{1 - 2\delta - \sqrt{(1-4\delta)}}{2\delta}\right)
    \leq \eta \leq 
    \log \left( \frac{1 - 2\delta +\sqrt{(1-4\delta)}}{2\delta}\right)
\end{aligned}$$

In terms this implies that:
$$\frac{1 - 2 \delta - \sqrt{1-4 \delta }}{1 - \sqrt{1-4 \delta}}
  \leq h(\eta) \leq \frac{1 - 2 \delta + \sqrt{1-4 \delta }}{1 + \sqrt{1-4 \delta}}$$
  
The current implementation uses $\delta = 10^{-4}$ 

## Fixed effects
Fixed effects are estimated by a logistic model where each individuals' observation will get a an observation in the new model with an individual offset for each bin the observation is in. Say for instance that we want to fit: 
```{r, echo=FALSE}
set.seed(1010101012)
data_frame <- 
  data.frame(id = c(1, 1, 1, 2, 2),
             tstop = c(0, 2, 3, 0, 2), tstart = c(2, 3, 4, 2, 4), y = c(0, 0, 1, 0, 0), 
             x1 = round(rnorm(5), 2), x2 = round(rnorm(5), 2))
```

```{r}
# The data we use
head(data_frame)
```

```{r, eval=FALSE}
# The fit
fit <- ddhazard(Surv(tstart, tstop, y) ~ x1 + ddFixed(x2), data_frame, 
                by = 1, # bin lengths are 1
                id = data_frame$id)
```

The model is with one fixed effect for `x2` and a time varying effect for `x1`. The individual with `id = 1` will yield 4 observation in the M-step: two for the first row, one for the second row and one for the third row. The first row will have two observations as it crosses two bins. Each observation in the M-step will have a separate offset given by $\emNotee{\vec{a}}{t}{d}$ times the value of `x1` where $t$ match the bin number. For example, the third rows' observation will have an offset of $\emNotee{\vec{a}}{4}{d}$ times `r data_frame$x1[3]`

## Starting value
QUESTION: Should I write about the static model that can be used for $\vec{\alpha}_0$?

## Binning
```{r binning_fig, echo=FALSE, results="hide", fig.cap = "Illustration of binning. See the text for explanation", fig.height=3}
par(cex = .8, mar = c(1, 4, 1, 2))
plot(c(0, 4), c(0, 1), type="n", xlab="", ylab="", axes = F)

abline(v = c(0.5, 1.5, 2.5), lty = 2)

n_series = 7
y_pos = seq(0, 1, length.out = n_series + 2)[-c(1, n_series +2)]

captioner::captioner()("binning_fig")

set.seed(1992)
x_vals_and_point_codes = list(
  cbind(0:4 + c(.1, rep(0, 4)),
        rep(4, 5)),
  cbind(c(0.1, 1, 1.5 + runif(1)),
        c(4, 4, 16)),
  cbind(c(0.1, 1, 2, 2 + runif(1, min = 0, max = .25)),
        c(4, 4, 4, 16)),
  cbind(c(0.1, runif(1)), c(4, 16)),
  cbind(c(0.1, 1, 2, 2.5 + runif(1)),
        c(4, 4, 4, 16)),
  cbind(c(2, 2.33),
        c(4, 16)), 
  cbind(c(0.1, .75),
        c(4, 4)))

x_vals_and_point_codes = x_vals_and_point_codes[
  c(n_series, sample(n_series - 1, n_series - 1, replace = F))]

for(i in seq_along(x_vals_and_point_codes)){
  vals = x_vals_and_point_codes[[i]]
  y = y_pos[i]
  
  xs = vals[, 1]
  n_xs = length(xs)
  
  # add lines
  segments(xs[-n_xs], rep(y, n_xs - 1),
           xs[-1], rep(y, n_xs - 1))
  
  # Add point
  points(xs, rep(y, n_xs), pch = vals[, 2])
  
  # Add numbers
  text(xs, rep(y + .05, n_xs), as.character(0:(n_xs -1)))
}

# add letters
text(rep(0, n_series), rev(y_pos), letters[1:n_series], cex = par()$cex * 1.5)
```

This section will illustrate how binning is performed for the logistic model and how this can lead to loss of information. It is elementary but included to stress this point and motivate the exponential model. We will use `r tolower(captioner::captioner()("binning_fig", display = "cite"))` as the illustration. Each horizontal line represent an individual. A cross represents when the co-variate values change for the individual and a circle represents the death of an individual. Lines that ends with a cross are right censored

The vertical dashed lines represents the bin borders. The first vertical from the left is where we start our model, the second vertical line is where the first bin ends and the second bin starts and the third vertical line is where the second bin ends. Thus, only have two bins in this example

We can now cover how the individuals (horizontal lines) are used in the estimation:

a. Is a control in both bins. We use the co-variates from 0 in the first bin and the co-variates from 1 in the second bin
b. Is not included in any of the bins. We do not know the co-variates values at the start of the second bin so we cannot include him
c. Is a control in the first bin with the co-variates from 0. He will count as a death in the second bin with the co-variates from 1
d. Acts like a. 
e. Is a death in the first bin with co-variates from 0
f. Is a control in the first bin with the co-variates from 0. He is a death in the second bin with the co-variates from 1
g. Is not included in any bins. We do not know if he survived the entire period of the first bin and thus we cannot include him

The example illustrates that: 

1. We loose  information about co-variates that are updated within bins. For instance, a., c., d. and f. all use the co-variates from 0 for the entire period of the first bin despite that the co-variates change at 1. Moreover, we never use the information at 2 from a., d. and f.
2. We loose information when we have right censoring. For instance, g. is not included at all since we only know that survives parts of the first bin
3. We loose information for observation that only occurs within bins as is the case for b.

The above motivates the exponential model that will be covered in the next sections

# Exponential model
The following section introduce the exponential model. The exponential model has a tuple for each observation. The tuples contains an indicator for whether the individual dies and a right truncated time variable for the observed survival time. We start by describing the assumption of the exponential model. Then we turn to the indicator followed by the right truncated time variable. Finally, a few comment is added to the EKF implementation.


## Assumptions
We make the following assumption in the exponential model: 

1. Parameters (i.e. state variables $\vec{\alpha}_1,\dots, \vec{\alpha}_T$) change at time $1, 2, \dots , T$
2. The individuals co-variates change at discrete times
3. We have exponential distributed arrival times within bins conditional on the parameters and co-variates

These assumption means that we have piecewise constant exponential distributed arrival times. The instantaneous hazard change when either the individual co-variates change or the parameters change. We make the following definitions to formalize the above. Let $\vec{x}_{ij}$ denote the $i$'th individuals $j$'th co-variate vector. For each individual we observe $j = 1, 2, \dots , l_i$ co-variate vectors. Each co-variate vector is valid in a period $(t_{i,j-1}, t_{i,j}]$. This definition differs from the previous definition of $\vec{x}_{ij}$ where the subscript $j$ referred to the bin number. Let $T_i$ denote the random death time of the $i$'th individual. Lastly, let $y_{ij} = 1_{\{T_i \in (t_{i,j - 1}, t_{i,j}]\}}$ be the indicator for whether the $i$'th individual dies in period $(t_{i,j - 1}, t_{i,j}]$

The likelihood of observing a death for the $i$'th individual in the last period $(t_{i,l_i - 1}, t_{i,l_i}]$ is: 

$$\begin{aligned}
  \proppCond{Y_{il_i} = 1}{\vec{\alpha}_0,\vec{\alpha}_1,\dots} =& \proppCond{Y_{il_i} = 1}{Y_{i, l_i -1} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{5pt} \cdot \proppCond{Y_{i, l_i -1} = 0}{Y_{i, l_i -2} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{75pt} \vdots \\
  &\hspace{5pt} \cdot \proppCond{Y_{i, 2} = 0}{Y_{i, 1} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots}
    \proppCond{Y_{i, 1} = 0}{\vec{\alpha}_0,\vec{\alpha}_1,\dots}
\end{aligned}$$

We can use the memory less property of the exponential distribution to conclude that each of factor and the right hand site have:

$$\begin{aligned}
\proppCond{Y_{i,s} = 1}{Y_{i, s - 1} = 0\wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} &= 1 - \prod_{z = \Lfloor{t_{i,s-1}} + 1 }^{\Lceil{t_{i,s}} } \exp\Lparen{ - \exp(\vec{x}_{is}^T \vec{\alpha}_{z})
    \Lparen{\min\{ z, t_{i,s} \} - \max\{ z - 1, t_{i,s-1} \} }}
\end{aligned}$$
    
where $\Lfloor{\cdot }$ is the floor function and $\Lceil{\cdot}$ is the ceiling function. In order to get this into the state space model notation we further have to separate $Y_{i,s}$ if $(t_{i,s-1}, t_{i,s}]$ crosses one or more bins. Take for example $(0.5, 2.5]$. Here we add three binary observations: one with time period of $(0.5,1]$, one with $(1, 2]$ and one with $(2, 2.5]$. Notice that this also implies that an individual who has different co-variate vectors within in a bin will have more than one observation in that bin. For example, an individual with a co-variate vector for $(0, 0.5]$ and a co-variate vector for $(0.5, 1]$ will yield two observation to the observation equation in the first bin

Computing the conditional mean, $h$, can done as follows. Assume for simplicity of notation that the observation $Y_{i,s}$ is inside an interval. In other words $\Lceil{t_{i,s}} - 1 = \Lfloor{t_{i,s-1}}$. This could be an observation we have introduced because the initial interval crossed a bin. The conditional mean is:

$$\begin{aligned}
  \tilde{z}(\vec{\alpha}) &= \expecpCond{Y_{i, s}}{Y_{i, s - 1} = 0 \wedge \vec{\alpha}_{\Lceil{t_{i,s}} } = \vec{\alpha}, \vec{\alpha}_{\Lceil{t_{i,s}} - 1}, \dots , \vec{\alpha}_0 } \\
   &= \left. h(\eta) \right|_{\eta = \vec{x}_{is}^T\vec{\alpha} } \\
   &= \left. 1 - \exp\Lparen{ - \exp(\eta) \Lparen{t_{i,s} - t_{i,s-1} }}\right|_{\eta = \vec{x}_{is}^T\vec{\alpha} }
\end{aligned}$$

where the tilde is added to stress that $\tilde{z}(\vec{\alpha}_{\Lceil{t_{i,s}}})$ is a margin of the mean in the observational equation $\vec{z}_{\Lceil{t_{i,s}}}$ where we do not provide a subscript for the elements index. The variance for the diagonal in $\mat{H}_{\Lceil{t_{i,s}}}(\vec{\alpha}_{\Lceil{t_{i,s}}})$ is given by $h(\eta)(1 - h(\eta))$ as $Y_{is}$ is binary. 

Right censoring is easily handled if we assume independent censoring. In this case the '$\min$' condition in $\proppCond{Y_{i,s} = 1}{Y_{i, s - 1} = 0 \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots}$ is valid for the last $t_{il_i}$ even if $t_{il_i} < \Lceil{t_{il_i}}$. We only know that the individual survives up to $t_{il_i}$ after which he was censored by an independent mechanism which we can condition on. However, the same logic does not apply when $t_{il_i} < \Lceil{t_{il_i}}$ and it is due to a death ($T_i = t_{il_i}$). We cannot condition on $T_i = t_{il_i}$ as we did with independent censoring because then $\proppCond{Y_{i,l_i} = 1}{T_i = t_{il_i}} = 1$

What we do instead is that we round $t_{il_i}$ up when we compute $h$ (the conditional mean) and when we compute the conditional variance in the case of a death. This means that we loose the information of the time of the event. However, we incorporate this information with the addition described in the next section

## Truncated observations time
We start by defining the truncated observation time $\Delta_{is}$:
$$\begin{aligned}
  &\Delta_{is} = (T_i - t_{i,s-1}) + \Lparen{t_{i,s} - T_i}1_{\{T_i \geq t_{i,s}\}} \\
  &T_i \geq t_{i,s-1}\Rightarrow\Delta_{is} \in [0,t_{i,s} - t_{i,s-1}]
\end{aligned}$$

The proposed and implemented model is to let every observation yield a tuple $(Y_{i,s}, \Delta_{i,s})$. The reason to include the tuple will be given and the end of this section. First, we will need to find the conditional mean and variance of $\Delta_{i,s}$ in order to use $\Delta_{i,s}$ in the observational equation. We start by recursively conditioning to get:
$$\begin{aligned}
  \proppCond{T_i = t_{i,l_i}}{\vec{\alpha}_0,\vec{\alpha}_1,\dots} =&
    \proppCond{\Delta_{i,l_i} = t_{i,l_i} - t_{i,l_i -1}}{\Delta_{i,l_i - 1} = t_{i,l_i - 1} - t_{i,l_i - 2} \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{5pt} \cdot \proppCond{\Delta_{i,l_i - 1} = t_{i,l_i - 1} - t_{i,l_i - 2} }
    {\Delta_{i,l_i - 2} = t_{i,l_i - 2} - t_{i,l_i - 3} \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots} \\
  &\hspace{140pt} \vdots \\
  &\hspace{5pt} \cdot \proppCond{\Delta_{i,2} = t_{i,2} - t_{i,1} }
    {\Delta_{i,1} = t_{i,1} \wedge \vec{\alpha}_0,\vec{\alpha}_1,\dots}
    \proppCond{\Delta_{i,1} = t_{i,1} }{\vec{\alpha}_0,\vec{\alpha}_1,\dots}
\end{aligned}$$

The conditional probabilities simplifies in a similar manner as for the binary $Y_{i,s}$ due to the memoryless property of the exponential distribution. Thus, computing the conditional mean, $h$, can done as follows. Again, assume for simplicity of notation that the observation $(t_{i, s-1}, t_{i,s}]$ is inside an interval (see the previous section). Then:

$$\begin{aligned}
  \tilde{z}(\vec{\alpha}) &= \expecpCond{\Delta_{i,s}}
    {\Delta_{i,s - 1} = t_{i,s - 1} - t_{i,s - 2} \wedge \vec{\alpha}_{\Lceil{t_{i,s}} } = \vec{\alpha}, \vec{\alpha}_{\Lceil{t_{i,s}} - 1}, \dots , \vec{\alpha}_0  } \\
  &= \left. h(\eta) \right|_{\eta = \vec{x}_{is}^T\vec{\alpha} } \\
  &= (t_{i,s} - t_{i,s - 1}) P(\tilde{T} \geq t_{i,s} - t_{i,s - 1}) + \int_0^{t_{i,s} - t_{i,s - 1}} r f_{\tilde{T}}(r) \mathrm{d} r
\end{aligned}$$

where $\tilde{T} \sim \text{Exp}\Lparen{\exp(\vec{x}_{is}^T\vec{\alpha})}$ and $f_{\tilde{T}}$ is the density function of $\tilde{T}$. Set $\lambda = \exp(\vec{x}_{is}^T\vec{\alpha})$ and $\delta =t_{i,s} - t_{i,s - 1}$ . The resulting conditional mean is:

$$\tilde{z}(\vec{\alpha}) = \frac{1 - \exp \Lparen{- \lambda \delta} }{\lambda}$$

Moreover, we can show that the variance is:
$$\begin{aligned}
  \hspace{50pt}&\hspace{-50pt}\varpCond{\Delta_{i,s}}
    {\Delta_{i,s - 1} = t_{i,s - 1} - t_{i,s - 2} \wedge \vec{\alpha}_{\Lceil{t_{i,s}} } = \vec{\alpha}, \vec{\alpha}_{\Lceil{t_{i,s}} - 1}, \dots , \vec{\alpha}_0  } \\
  &= \frac{1 - \exp\Lparen{-2\delta\lambda} - 2 \lambda \delta \exp \Lparen{-\delta\lambda}}{\lambda^2}
\end{aligned}$$

Right censoring is treated the same way as for $Y_{i,s}$. That is, we can have $t_{i,l_i} < \Lceil{t_{i,l_i}}$ in the case censoring. Further, we still round up in the case where $T_i = t_{i,l_i}$ (in the case of a death). The individual could have survived $\Lceil{t_{i,l_i}}$ but only survived $t_{i,l_i}$. Denote the predicted mean $\Delta_{i,l_i}$ by $\widehat{\Delta}_{i,l_i}$. We can have $\widehat{\Delta}_{i,l_i} \geq \Delta_{i,l_i}$ in the case of death. This will affect the correction step in both the UKF end EKF when we compute $\emNotee{\vec{a}}{t}{t}$

## Why not use $\Delta_{i,s}$ solely
The reason also to use the binary $Y_{i,t}$ is that we cannot distinguish between deaths when times are rounded. For example, say that the observed times are rounded up to multiplies of $0.5$ (i.e. $0$, $0.5$, $1$, $1.5$ etc.). We cannot distinguish between the death of an individual, change in co-variate vector or change of parameters if $t_{i,s}$ is a whole number. That is, $\Delta_{i,s} = 1$ either because $T_i = 1$ or because $t_{i,s} = 1$ since we have new co-variate vector $\vec{x}_{i,s+1}$ or(/and) because the parameters changed from $\vec{\alpha}_1$ to $\vec{\alpha}_2$

## Covariance 
$Y_{i,t}$ and $\Delta_{i,s} = 1$ are correlated and we have to account for that. This will affect the score vector and information matrix ($\vec{u}_t(\emNotee{\vec{a}}{t}{t-1})$ and $\mat{U}_t(\emNotee{\vec{a}}{t}{t-1})$) in the EKF. Further, it will affect the covariance matrix $\widehat{\mat{H}}$ in the UKF. First though, we have to derive the covariance. We start by defining the following variables to ease the notation:
$$\delta = t_{i,s}-t_{i,s-1}, \qquad \lambda = \exp \Lparen{\vec{x}_{is}^T \vec{\alpha}_{\lceil t_{i,s} \rceil}}, \qquad
  Z \sim \text{Exp}\Lparen{\lambda}$$

Thus, we have the following relation (conditional on having survied up the this point):
$$Y_{i,s} \sim 1_{\Lbrace{Z\in[0,\delta\}}}, \qquad \Delta_{i,s} \sim Z + (\delta - Z) 1_{\Lbrace{Z\in[\delta,\infty\}}}$$

where $\sim$ denotes "similary destributed" and $1_{\Lbrace{\cdot}}$ is one if the condition in the braces are satisfied and zero otherwise. Hence, we can find the covariance by: 
$$\begin{aligned}
\covp{1_{\Lbrace{Z\in[0,\delta\}}}, Z + (\delta - Z) 1_{\Lbrace{Z\in[\delta,\infty\}}}}
  =\hspace{-150pt}& \\ 
  &\covp{1_{\Lbrace{Z\in[0,\delta\}}}, Z}
  + \delta\covp{1_{\Lbrace{Z\in[0,\delta\}}},1_{\Lbrace{Z\in[\delta,\infty\}}}} 
  - \covp{1_{\Lbrace{Z\in[0,\delta\}}}, Z 1_{\Lbrace{Z\in[\delta,\infty\}}}} =\\
  &-\exp (-\lambda\delta)\delta 
    + \delta \Lparen{- \Lparen{1 - \exp(-\lambda\delta)} \exp(-\lambda\delta)} 
    - \frac{- \Lparen{1 - \exp(-\lambda\delta)} \exp(-\lambda\delta)(1+\delta\lambda)}{\lambda} = \\
  &  - \frac{\exp (-2\lambda\delta)\Lparen{1 + \lambda\delta\exp(\lambda\delta)-\exp(\lambda\delta)}}{\lambda}
\end{aligned}$$

Next, we turn to the EKF. Define:
$$\varp{Y_{i,s}} = \sigma_{Y_{i,s}}^2, \qquad \varp{\Delta_{i,s}} = \sigma_{\Delta_{i,s}}^2,  \qquad
  \covp{Y_{i,s}, \Delta_{i,s}} = \rho_{i,s} \sigma_{Y_{i,s}} \sigma_{\Delta_{i,s}} = \xi_{i,s}$$
  
Say that we are in looking at a given bin and order the observation equation such that:
$$\vec{y}_t = \Lparen{y_{1,i_{1,1}}, y_{1,i_{1,2}}, \dots y_{m,i_{m,k}}, \Delta_{1,i_{1,1}}, \Delta_{1,i_{1,2}}, \dots, \Delta_{m,i_{m,k}}}^T$$

where $i_{j,i}$ is an index function that returns the index of the $j$'th individuals $i$'th observation in this bin. Although this is tedious, it is included to stress that any given individual could have none, one or multiple enteries in this bin. Further, notice that $\vec{y}_t$ refers to the whole state vector while $y_{i,j}$ refers to the indicator for whetehr invidiual $i$ dies at his $j$'th observations. The co-variance matrix $\mat{H}(\alpha)$ is then of the following structure due to the ordering of the elements of $\vec{y}_t$:

$$\mat{H}(\vec{\alpha}_t) =\begin{pmatrix}
  \diag{\sigma_{Y_{1,i_{1,1}}}^2, \sigma_{Y_{1,i_{1,2}}}^2,\dots, \sigma_{Y_{m,i_{m,k}}}^2}
    & \diag{\xi_{1,i_{1,1}}, \xi_{1,i_{1,2}},\dots, \xi_{m,i_{m,k}}} \\
  \diag{\xi_{1,i_{1,1}}, \xi_{1,i_{1,2}},\dots, \xi_{m,i_{m,k}}} 
    & \diag{\sigma_{\Delta_{1,i_{1,1}}}^2, \sigma_{\Delta_{1,i_{1,2}}}^2,\dots, \sigma_{\Delta_{m,i_{m,k}}}^2}
\end{pmatrix}$$

where all the entries implicitly depends on the state vector $\vec{\alpha}_t$. In order to update the EKF we have to go back to the forumlas for the score vector and information matrix. They are:
$$\begin{array}{c}
  \dot{\vec{z}}_t(\alpha_t) = \left. \frac{\partial\vec{z}_t(\vec{\eta})}{\partial \vec{\eta}} \right\vert_{\vec{\eta}= \vec{\alpha}_t}\vspace{10pt} \\
  \vec{u}_t(\vec{\alpha}_t) = \dot{\vec{z}}_t(\alpha_t) \mat{H}(\vec{\alpha}_t)^{-1}\Lparen{\vec{y}_t - \vec{z}_t(\vec{\alpha}_t)}, \qquad
\mat{U}_t(\vec{\alpha}_t) = \dot{\vec{z}}_t(\alpha_t) \mat{H}(\vec{\alpha}_t)^{-1} \dot{\vec{z}}_t(\alpha_t)^T
\end{array}$$

where we get the simplier expression we saw previously when $\mat{H}(\vec{\alpha}_t)$ is a diagonal matrix. In the case with off-diagonal terms we can use the idenity: 
$$\begin{bmatrix} \mathbf{A} & \mathbf{C}^T \\ \mathbf{C} & \mathbf{D} \end{bmatrix}^{-1} = \begin{bmatrix} (\mathbf{A}-\mathbf{C}^T\mathbf{D}^{-1}\mathbf{C})^{-1} & -\mathbf{A}^{-1}\mathbf{C}^T(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{C}^T)^{-1} \\ -(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{C}^T)^{-1}\mathbf{CA}^{-1} & (\mathbf{D}-\mathbf{CA}^{-1}\mathbf{C}^T)^{-1} \end{bmatrix}$$
We can find a closed form solution since the three matrices are diagonal matricies. Define:
$$\mat{K}(\vec{\alpha}_t) = \mat{H}(\vec{\alpha}_t)^{-1} = \begin{pmatrix}
  \mat{K}_{11} & \mat{K}_{12} \\
  \mat{K}_{12} & \mat{K}_{22}
\end{pmatrix}$$

Then:
$$\begin{aligned}
&\mat{K}_{11} = \diag{\frac{\exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(-2 \delta _{s,j} \lambda _{s,j} \exp\Lparen{\delta
   _{s,j} \lambda _{s,j}}+\exp\Lparen{2 \delta _{s,j} \lambda _{s,j}}-1\right)}{-\exp\Lparen{\delta _{s,j}
   \lambda _{s,j}} \left(\delta _{s,j}^2 \lambda _{s,j}^2+2\right)+\exp\Lparen{2 \delta _{s,j}
   \lambda _{s,j}}+1}} \\
&\mat{K}_{22} = \diag{\frac{\lambda _{s,j}^2 \exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(\exp\Lparen{\delta _{s,j} \lambda
   _{s,j}}-1\right)}{-\exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(\delta _{s,j}^2 \lambda
   _{s,j}^2+2\right)+\exp\Lparen{2 \delta _{s,j} \lambda _{s,j}}+1}} \\
&\mat{K}_{12} = \diag{\frac{\lambda _{s,j} \exp\Lparen{\delta _{s,j} \lambda _{s,j}} \left(\exp\Lparen{\delta _{s,j} \lambda _{s,j}}
   \left(\delta _{s,j} \lambda _{s,j}-1\right)+1\right)}{-\exp\Lparen{\delta _{s,j} \lambda _{s,j}}
   \left(\delta _{s,j}^2 \lambda _{s,j}^2+2\right)+\exp\Lparen{2 \delta _{s,j} \lambda _{s,j}}+1}}
\end{aligned}$$

where we have omitted the exact entries of the subscript. The subscripts follow such that the pair match the order for the observation vector ($(1,i_{1,1}),(1,i_{1,2}),\dots,(m,i_{m,k})$). The score and information matrix can than be computed as follows. Suppose we have $r$ observations such that the observation equation has dimension $2r$. Next, denote the binaries of $\vec{y}_t$ by $y_1,y_2,\dots,y_r$ and denote the truncated times by $\Delta_1,\Delta_2,\dots,\Delta_r$. The score vector is then:

$$\begin{aligned}
\vec{u}_t(\vec{\alpha}_t) = \sum_{i=1}^r 
  &\vec{x}_i\Lparen{h^y_i}'(\eta_i)\Lparen{\MyInd{\mat{K}_{11}}{ii} + \MyInd{\mat{K}_{12}}{ii}}(y_i - h^y_i(\eta_i)) \\
  &\hspace{4pt}+\vec{x}_i\Lparen{h^\Delta_i}'(\eta_i)\Lparen{\MyInd{\mat{K}_{22}}{ii} + \MyInd{\mat{K}_{12}}{ii}}(\Delta_i - h^\Delta_i(\eta_i))
\end{aligned}$$

where $\vec{x}_i$ is the $i$'th observation's covariate vector, $\eta_i = \vec{x}_i^T\vec{\alpha}_t$ is the linear predictor and $\Lparen{h^\Delta_i}'$ and $\Lparen{h^y_i}'$ are the first deriatives of the link function w.r.t. the linear predictor for the truncated waiting time and the binary outcome. Similarly, $h^\Delta_i$ and $h^y_i$ are the link functions the truncated waiting time and the binary outcome. We need the subscript because each observation can be add risk for different amount of time in the bin we are focusing on. Lastly, $(\cdot)_{ij}$ is the $(i,j)$'th entry of the matrix in the parenthsis. Finally, the infomration matrix can be computed by:

$$\begin{aligned}
\mat{U}_t(\vec{\alpha}_t) = \sum_{i=1}^r 
  &\vec{x}_i\Lparen{\Lparen{\Lparen{h^y_i}'(\eta_i)}^2\MyInd{\mat{K}_{11}}{ii} +
    \Lparen{h^y_i}'(\eta_i)\cdot\Lparen{h^\Delta_i}'(\eta_i)\MyInd{\mat{K}_{12}}{ii}}\vec{x}_i^T\\
  & \hspace{4pt}+\vec{x}_i\Lparen{\Lparen{h^y_i}'(\eta_i)\cdot\Lparen{h^\Delta_i}'(\eta_i)\MyInd{\mat{K}_{12}}{ii} +
    \Lparen{\Lparen{h^\Delta_i}'(\eta_i)}^2\MyInd{\mat{K}_{22}}{ii}}\vec{x}_i^T
\end{aligned}$$

We could implement the above. I have done that for the EKF. There is and issue though with divergence. The entries of $\mat{K}$ can become numerically large. In particular, we can show that: 

$$\mat{K} = \begin{pmatrix}
  \diag{\frac{1}{\sigma_{Y_1}^2(1 - \rho_1^2)}, \dots, \frac{1}{\sigma_{Y_r}^2(1 - \rho_r^2)}} &
    \diag{\frac{\rho_1}{\sigma_{Y_1}\sigma_{\Delta_1}(\rho_1^2 - 1)}, \dots, \frac{\rho_r}{\sigma_{Y_r}\sigma_{\Delta_r}(\rho_r^2 - 1)}} \\
  \diag{\frac{\rho_1}{\sigma_{Y_1}\sigma_{\Delta_1}(\rho_1^2 - 1)}, \dots, \frac{\rho_r}{\sigma_{Y_r}\sigma_{\Delta_r}(\rho_r^2 - 1)}} &
    \diag{\frac{1}{\sigma_{\Delta_1}^2(1 - \rho_1^2)}, \dots, \frac{1}{\sigma_{\Delta_r}^2(1 - \rho_r^2)}}
\end{pmatrix}$$

We can compute some numerical examples to illustrate why this is an issue. This is done below. `eta` refers to the linear predictor, `delta` refers to at the at risk length and `Delta` refers to the outcome:

```{r}
grid <- expand.grid(delta = c(.1, 1, 2, 3), eta = c(-4, -2, -1, 0, 1))

grid$`Link Y` <- 1 - exp(- exp(grid$eta) * grid$delta)
grid$`Var Y` <- grid$`Link Y` * (1 - grid$`Link Y`)
grid$`Deriv link Y` <- exp(- exp(grid$eta) * grid$delta) * exp(grid$eta) * grid$delta


grid$`Link Delta` <- (1 - exp(- exp(grid$eta) * grid$delta)) / exp(grid$eta)
grid$`Var Delta` <- (1 - exp(- 2 * exp(grid$eta) * grid$delta) 
                     - 2 * exp(grid$eta) * grid$delta 
                     * exp(- exp(grid$eta) * grid$delta)) / exp(2 * grid$eta)
grid$`Deriv link Delta` <- exp(- exp(grid$eta) * grid$delta) * (
  1 - exp(exp(grid$eta) * grid$delta) + exp(grid$eta) * grid$delta) / 
    exp(grid$eta)

grid$Covariance <- - exp(- 2 * exp(grid$eta) * grid$delta) * 
  (1 + exp(grid$eta) * grid$delta * exp(exp(grid$eta) * grid$delta)
   - exp(exp(grid$eta) * grid$delta)) / exp(grid$eta)

grid$Correlation <- grid$Covariance / sqrt(grid$`Var Delta` * grid$`Var Y`)

grid$K11 <- 1 / (grid$`Var Y` * (1 - grid$Correlation^2))
grid$K22 <- 1 / (grid$`Var Delta` * (1 - grid$Correlation^2))
grid$K12 <- grid$Correlation / (sqrt(grid$`Var Y` * grid$`Var Delta`) * 
                   (grid$Correlation^2 - 1))

knitr::kable(grid[, 1:8], digits = 3)
```
```{r}
knitr::kable(grid[, -(3:8)], digits = 3)
```

Where we see that certain combinations of $(\delta, \eta)$ cause $\mat{K}$ are large. We will illustrate why this may be an issue in the following paragraphs. Notice that the deriatives of the two link functions have opposing signs while all the entries of $\mat{K}$ are positive. Thus, there is a canceling effect in the computation of $\mat{U}_t(\vec{\alpha}_t)$. Hence, $\emNotee{\mat{V}}{t}{t}$ seems not to be an issue in the sense that it "stays around" sensible values before divergence

The update from $\emNotee{\vec{a}}{t}{t-1}$ to $\emNotee{\vec{a}}{t}{t}$ seems to be cause divergence. Re-call that:

$$\emNotee{\vec{a}}{t}{t} = \vec{a} + l \cdot \emNotee{\mat{V}}{t}{t}\vec{u}_t (\vec{a})$$
```{r, echo=FALSE}
ex <- 6
```

Supose now that we have somewhat rare events, the linear predictor $\eta$ is `r grid$eta[ex]`, $\delta$ is `r grid$delta[ex]`  and the indvidual with $\Delta = 1$. The latter can occour often if times are reported discritly (in say days or months). Then the factor from the residual in the term  with $(y_i - h_i^y(\eta_i))$ is the product of `r grid[ex, "Deriv link Y"]` ($\Lparen{h_i^y(\eta_i)}'$), (`r grid[ex, "K11"]` plus `r grid[ex, "K12"]`) ($\MyInd{\mat{K}_{11}}{ii} + \MyInd{\mat{K}_{12}}{ii}$) and ($y_i$ minus `r grid[ex, "Link Y"]` ($h_i^y(\eta_i)$)). The result is `r grid[ex, "Deriv link Y"] * (grid[ex, "K11"] + grid[ex, "K12"]) * (1 - grid[ex, "Link Y"])` if it is a death and `r grid[ex, "Deriv link Y"] * (grid[ex, "K11"] + grid[ex, "K12"]) * (0 - grid[ex, "Deriv link Y"])` otherwise

The factor for the residual time is the product of `r grid[ex, "Deriv link Delta"]`, (`r grid[ex, "K22"]` plus `r grid[ex, "K12"]`) and (1 minus `r grid[ex, "Link Delta"]`). The result is `r grid[ex, "Deriv link Delta"] * (grid[ex, "K22"] + grid[ex, "K12"]) * (1 - grid[ex, "Link Delta"])`. Meanwhile the factor for the information matrix $\mat{U}_t(\vec{\alpha})_t$ is `r grid[ex, "Deriv link Y"]^2 * grid[ex, "K11"] + 2 * grid[ex, "Deriv link Y"] * grid[ex, "Deriv link Delta"] * grid[ex, "K12"] + grid[ex, "Deriv link Delta"]^2 * grid[ex, "K22"]` ($\Lparen{\Lparen{h^y_i}'(\eta_i)}^2\MyInd{\mat{K}_{11}}{ii} + 2\cdot\Lparen{h^y_i}'(\eta_i)\cdot\Lparen{h^\Delta_i}'(\eta_i)\MyInd{\mat{K}_{12}}{ii} + \Lparen{\Lparen{h^\Delta_i}'(\eta_i)}^2\MyInd{\mat{K}_{22}}{ii}$). Recalling that:

$$\emNotee{\mat{V}}{t}{t} = \left(\emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\emNotee{\vec{a}}{t}{t - 1})\right)^{-1}$$
Depending on the co-varaite vectors $\vec{x}_i$ and covariance matrix $\emNotee{\mat{V}}{t}{t - 1}$ this can imply that $\vec{u}_t(\vec{\alpha}_t)$ becomes large compared to $\emNotee{\mat{V}}{t}{t }$ which causes a large step in the score direction. One solution is to set the learning rate $l$ (very) low. Though, I have to stress that this has to be set it at a very low level 

A solution may be to make the two variables uncorrelated. For example, we can set:
$$\begin{aligned}
&\begin{pmatrix} y_i \\ \Delta_i + c y_i \end{pmatrix} = 
  \underbrace{\begin{pmatrix} 1 & 0 \\ c & 1 \end{pmatrix}}_C \begin{pmatrix} y_i \\ \Delta_i \end{pmatrix} \\
\Leftrightarrow\hspace{4pt}& 
  \covp{\begin{pmatrix} y_i \\ \Delta_i + c y_i \end{pmatrix}} = C 
    \begin{pmatrix}\sigma_{Y_i}^2 & \xi_i \\ \xi_i  & \sigma_{\Delta_i}^2\end{pmatrix} C^T
\end{aligned}$$

and get zero correlation by selecting $c$ such that:
$$\begin{aligned}
& c \sigma_{Y_i}^2 + \xi_i = 0 \\
\Leftrightarrow\hspace{4pt}& 
  c = -\frac{\xi_i}{\sigma_{Y_i}^2} = -\frac{\exp\Lparen{-\eta _i} \left(\delta _i \left(-\exp\Lparen{\eta
   _i}\right)-\exp\Lparen{\delta _i \left(-\exp\Lparen{\eta _i}\right)}+1\right)}{1-\exp\Lparen{\delta _i
   \left(-\exp\Lparen{\eta _i}\right)}}
\end{aligned}$$

Though, this requires that we know the $\vec{\alpha}_t$ in order to compute $\eta_i$. Question: Would it be ok to plug in the current estimate of $\emNotee{\vec{a}}{t}{t - 1}$? Do you think this is a good idea?

As an example here is the output from the E-step of the EKF for the head neck data in [@fahrmeir94]. `U` is the diagonal of the information matrix $\mat{U}$:

```
##########################################
Starting iteration 0 with the following values
a_0
    -2.157    0.25186 
diag(Q)
     0.001      0.001 
Score vector and diagonal of information matrix at time 1 are:
u
   -249.44    -143.08 # <-- the big issue. This is way larger than ... 
U
    22.422     13.128 # <-- this
a_(1|1)
   -12.538   -0.22944 # <-- hence big step here
diag(V_(1|1))

  0.089128    0.14777 # <-- this is reasonable though 
Score vector and diagonal of information matrix at time 2 are:
u
    119.99     78.996 
U
0.00057956 0.00027098 
a_(2|2)
   -8.1467     1.6628 
diag(V_(2|2))

  0.091125    0.14977 
Score vector and diagonal of information matrix at time 3 are:
u
    244.06     151.36 
U
   0.14404    0.12203 
a_(3|3)
    2.0082     4.3287 
diag(V_(3|3))

  0.092922    0.15104 
(Divergence)
```

The starting value here is not too far off from the what we would expect. The logit model is estimated to start of at roughly $\vec{\alpha}_0 = (-3, 0)$. Here is the output from the exponential model with a learning rate of 0.1 (it is rather long). Both the final state vector and smoothed covariance matricies match well with the logistic:

```
##########################################
Starting iteration 0 with the following values
a_0
    -2.157    0.25186 
diag(Q)
     0.001      0.001 
Score vector and diagonal of information matrix at time 1 are:
u
   -249.44    -143.08 
U
    22.422     13.128 
a_(1|1)
   -3.1951    0.20373 # <-- we take a ten fold smaller step now
diag(V_(1|1))

  0.089128    0.14777 
Score vector and diagonal of information matrix at time 2 are:
u
    15.903     19.187 
U
    7.9229     4.5404 
a_(2|2)
   -3.2088    0.32334 
diag(V_(2|2))

  0.069521    0.11604 
Score vector and diagonal of information matrix at time 3 are:
u
    150.07      96.22 
U
     7.176      4.226 
a_(3|3)
   -2.8486     0.4468 
diag(V_(3|3))

  0.058956   0.097866 
Score vector and diagonal of information matrix at time 4 are:
u
   -33.291    -32.133 
U
    7.1251     4.0622 
a_(4|4)
    -2.869    0.32938 
diag(V_(4|4))

  0.051267   0.085012 
Score vector and diagonal of information matrix at time 5 are:
u
   -15.404   -0.39358 
U
    5.7482     3.0655 
a_(5|5)
    -2.939    0.39036 
diag(V_(5|5))

  0.046521   0.077379 
Score vector and diagonal of information matrix at time 6 are:
u
    -39.33    -29.158 
U
    4.5565     2.2466 
a_(6|6)
   -2.9999    0.32706 
diag(V_(6|6))

  0.043551   0.072777 
Score vector and diagonal of information matrix at time 7 are:
u
   -30.221    -4.4363 
U
    3.9255      1.935 
a_(7|7)
   -3.1102    0.40249 
diag(V_(7|7))

  0.041675   0.069663 
Score vector and diagonal of information matrix at time 8 are:
u
   -10.016    -6.5804 
U
    3.2901     1.4984 
a_(8|8)
    -3.129    0.39104 
diag(V_(8|8))

  0.040419   0.067686 
Score vector and diagonal of information matrix at time 9 are:
u
   -24.125    -10.405 
U
    2.9275     1.3352 
a_(9|9)
    -3.192    0.39768 
diag(V_(9|9))

  0.039638   0.066326 
Score vector and diagonal of information matrix at time 10 are:
u
   -16.012    0.34931 
U
    2.2985     1.0362 
a_(10|10)
   -3.2562    0.44826 
diag(V_(10|10))

  0.039464   0.065768 
Score vector and diagonal of information matrix at time 11 are:
u
   -12.381    -10.299 
U
     1.946    0.79563 
a_(11|11)
   -3.2751    0.41665 
diag(V_(11|11))

  0.039487   0.065664 
Score vector and diagonal of information matrix at time 12 are:
u
   -16.116    -9.8222 
U
    1.7783    0.75864 
a_(12|12)
   -3.3114    0.39762 
diag(V_(12|12))

    0.0397   0.065729 
Score vector and diagonal of information matrix at time 13 are:
u
   -20.784    -9.3242 
U
    1.5999    0.72003 
a_(13|13)
    -3.369    0.39351 
diag(V_(13|13))

  0.040107   0.065953 
Score vector and diagonal of information matrix at time 14 are:
u
   -6.2501     -8.796 
U
    1.4775    0.67909 
a_(14|14)
   -3.3705    0.35214 
diag(V_(14|14))

  0.040609   0.066282 
Score vector and diagonal of information matrix at time 15 are:
u
   -17.921    -8.4453 
U
    1.3827    0.65193 
a_(15|15)
   -3.4218    0.34361 
diag(V_(15|15))

   0.04118    0.06668 



##########################################
Starting iteration 1 with the following values
a_0
   -3.2328    0.43541 
diag(Q)
 0.0011127   0.001026 
Score vector and diagonal of information matrix at time 1 are:
u
   -79.602    -49.196 
U
    9.2682     5.8545 
a_(1|1)
   -3.9465    0.32723 
diag(V_(1|1))

   0.18996    0.28459 
Score vector and diagonal of information matrix at time 2 are:
u
    65.921      46.29 
U
    4.1094     2.4795 
a_(2|2)
   -3.5845    0.51198 
diag(V_(2|2))

    0.1454     0.2234 
Score vector and diagonal of information matrix at time 3 are:
u
    171.19     105.44 
U
    5.5898     3.5382 
a_(3|3)
   -2.7342    0.59168 
diag(V_(3|3))

   0.11278    0.17393 
Score vector and diagonal of information matrix at time 4 are:
u
   -51.733    -46.032 
U
    8.5425     5.1317 
a_(4|4)
   -2.8101    0.38962 
diag(V_(4|4))

   0.08231    0.12931 
Score vector and diagonal of information matrix at time 5 are:
u
   -22.039    -5.0397 
U
    6.2555     3.4198 
a_(5|5)
   -2.9287    0.47201 
diag(V_(5|5))

  0.068053    0.10908 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.189    -31.722 
U
    4.7778     2.4453 
a_(6|6)
   -3.0111    0.38969 
diag(V_(6|6))

  0.060277   0.098025 
Score vector and diagonal of information matrix at time 7 are:
u
    -31.19     -5.678 
U
    3.9993     2.0299 
a_(7|7)
   -3.1567    0.49023 
diag(V_(7|7))

  0.055549   0.090945 
Score vector and diagonal of information matrix at time 8 are:
u
   -9.7665    -7.3468 
U
    3.2711     1.5573 
a_(8|8)
    -3.175    0.47054 
diag(V_(8|8))

   0.05247   0.086388 
Score vector and diagonal of information matrix at time 9 are:
u
   -23.787    -10.957 
U
    2.9014     1.3777 
a_(9|9)
    -3.249    0.47889 
diag(V_(9|9))

  0.050379   0.083105 
Score vector and diagonal of information matrix at time 10 are:
u
   -15.454   0.037081 
U
     2.255       1.06 
a_(10|10)
   -3.3255    0.54049 
diag(V_(10|10))

  0.049387   0.081264 
Score vector and diagonal of information matrix at time 11 are:
u
   -11.633    -10.523 
U
    1.8891    0.81299 
a_(11|11)
   -3.3424    0.50014 
diag(V_(11|11))

  0.048795   0.080198 
Score vector and diagonal of information matrix at time 12 are:
u
   -15.434    -9.9729 
U
     1.726    0.77032 
a_(12|12)
    -3.381    0.47708 
diag(V_(12|12))

   0.04853   0.079461 
Score vector and diagonal of information matrix at time 13 are:
u
   -20.128     -9.411 
U
    1.5494    0.72675 
a_(13|13)
   -3.4456    0.47372 
diag(V_(13|13))

  0.048587   0.079022 
Score vector and diagonal of information matrix at time 14 are:
u
    -5.535    -8.8259 
U
    1.4228    0.68141 
a_(14|14)
   -3.4423    0.42319 
diag(V_(14|14))

  0.048819   0.078789 
Score vector and diagonal of information matrix at time 15 are:
u
   -17.281    -8.4395 
U
    1.3332    0.65148 
a_(15|15)
    -3.499    0.41476 
diag(V_(15|15))

  0.049162   0.078687 



##########################################
Starting iteration 2 with the following values
a_0
   -3.3217    0.51158 
diag(Q)
 0.0012475  0.0010641 
Score vector and diagonal of information matrix at time 1 are:
u
   -75.067    -48.294 
U
    8.9186     5.7849 
a_(1|1)
   -4.0017    0.37946 
diag(V_(1|1))

   0.20069     0.2934 
Score vector and diagonal of information matrix at time 2 are:
u
    67.136     46.382 
U
    4.0166     2.4725 
a_(2|2)
   -3.6006    0.53668 
diag(V_(2|2))

   0.15358    0.23051 
Score vector and diagonal of information matrix at time 3 are:
u
    171.23     105.05 
U
     5.587     3.5672 
a_(3|3)
   -2.7037    0.57949 
diag(V_(3|3))

   0.11806    0.17856 
Score vector and diagonal of information matrix at time 4 are:
u
   -54.111    -47.122 
U
    8.7251     5.2157 
a_(4|4)
    -2.794    0.38268 
diag(V_(4|4))

  0.084377    0.13102 
Score vector and diagonal of information matrix at time 5 are:
u
   -22.981    -5.4175 
U
    6.3277     3.4487 
a_(5|5)
   -2.9191    0.46824 
diag(V_(5|5))

  0.069332    0.11006 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.636    -31.891 
U
    4.8123     2.4585 
a_(6|6)
   -3.0063    0.38628 
diag(V_(6|6))

  0.061314    0.09875 
Score vector and diagonal of information matrix at time 7 are:
u
   -31.342    -5.7132 
U
     4.011     2.0325 
a_(7|7)
   -3.1554    0.48754 
diag(V_(7|7))

  0.056528   0.091582 
Score vector and diagonal of information matrix at time 8 are:
u
   -9.7646     -7.319 
U
     3.271     1.5551 
a_(8|8)
   -3.1748    0.46767 
diag(V_(8|8))

  0.053471    0.08701 
Score vector and diagonal of information matrix at time 9 are:
u
   -23.744    -10.912 
U
    2.8981     1.3742 
a_(9|9)
   -3.2514    0.47531 
diag(V_(9|9))

  0.051428   0.083732 
Score vector and diagonal of information matrix at time 10 are:
u
    -15.34    0.11517 
U
    2.2463      1.054 
a_(10|10)
   -3.3294    0.53678 
diag(V_(10|10))

  0.050528   0.081922 
Score vector and diagonal of information matrix at time 11 are:
u
   -11.505    -10.448 
U
    1.8792    0.80719 
a_(11|11)
   -3.3478    0.49543 
diag(V_(11|11))

  0.050039   0.080903 
Score vector and diagonal of information matrix at time 12 are:
u
   -15.276    -9.8793 
U
    1.7138    0.76306 
a_(12|12)
   -3.3884    0.47116 
diag(V_(12|12))

  0.049881   0.080212 
Score vector and diagonal of information matrix at time 13 are:
u
   -19.934    -9.2935 
U
    1.5344    0.71765 
a_(13|13)
   -3.4559    0.46629 
diag(V_(13|13))

  0.050052   0.079819 
Score vector and diagonal of information matrix at time 14 are:
u
   -5.2925    -8.6793 
U
    1.4041    0.67005 
a_(14|14)
   -3.4532    0.41504 
diag(V_(14|14))

  0.050402   0.079633 
Score vector and diagonal of information matrix at time 15 are:
u
   -17.037    -8.2881 
U
    1.3143    0.63974 
a_(15|15)
   -3.5126    0.40512 
diag(V_(15|15))

  0.050859   0.079577 



##########################################
Starting iteration 3 with the following values
a_0
   -3.3134    0.51433 
diag(Q)
 0.0014215  0.0011136 
Score vector and diagonal of information matrix at time 1 are:
u
   -76.183    -49.081 
U
    9.0047     5.8456 
a_(1|1)
   -3.9973    0.38132 
diag(V_(1|1))

   0.19963    0.29176 
Score vector and diagonal of information matrix at time 2 are:
u
    66.853     46.185 
U
    4.0382     2.4874 
a_(2|2)
   -3.5989    0.53664 
diag(V_(2|2))

     0.153    0.22932 
Score vector and diagonal of information matrix at time 3 are:
u
    171.11     104.97 
U
     5.596     3.5729 
a_(3|3)
   -2.7009    0.58081 
diag(V_(3|3))

   0.11784    0.17784 
Score vector and diagonal of information matrix at time 4 are:
u
   -54.483    -47.373 
U
    8.7538      5.235 
a_(4|4)
   -2.7941    0.38343 
diag(V_(4|4))

  0.084331    0.13058 
Score vector and diagonal of information matrix at time 5 are:
u
   -23.006    -5.4457 
U
    6.3297     3.4508 
a_(5|5)
   -2.9198    0.46804 
diag(V_(5|5))

  0.069494    0.10983 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.591    -31.866 
U
    4.8088     2.4565 
a_(6|6)
   -3.0097    0.38437 
diag(V_(6|6))

  0.061661   0.098679 
Score vector and diagonal of information matrix at time 7 are:
u
   -31.127    -5.5812 
U
    3.9945     2.0225 
a_(7|7)
   -3.1603    0.48418 
diag(V_(7|7))

  0.057059    0.09165 
Score vector and diagonal of information matrix at time 8 are:
u
    -9.508    -7.1654 
U
    3.2513     1.5433 
a_(8|8)
     -3.18     0.4638 
diag(V_(8|8))

  0.054178   0.087207 
Score vector and diagonal of information matrix at time 9 are:
u
   -23.495    -10.761 
U
    2.8789     1.3626 
a_(9|9)
   -3.2587    0.46983 
diag(V_(9|9))

  0.052298   0.084035 
Score vector and diagonal of information matrix at time 10 are:
u
   -15.067    0.28026 
U
    2.2253     1.0414 
a_(10|10)
   -3.3375    0.53049 
diag(V_(10|10))

  0.051579   0.082328 
Score vector and diagonal of information matrix at time 11 are:
u
   -11.255    -10.308 
U
      1.86    0.79631 
a_(11|11)
   -3.3574    0.48789 
diag(V_(11|11))

  0.051267   0.081413 
Score vector and diagonal of information matrix at time 12 are:
u
   -15.004     -9.721 
U
    1.6927    0.75079 
a_(12|12)
   -3.4002    0.46196 
diag(V_(12|12))

  0.051279   0.080815 
Score vector and diagonal of information matrix at time 13 are:
u
   -19.631    -9.1109 
U
    1.5109    0.70349 
a_(13|13)
   -3.4708    0.45493 
diag(V_(13|13))

  0.051621   0.080506 
Score vector and diagonal of information matrix at time 14 are:
u
   -4.9413    -8.4658 
U
     1.377    0.65351 
a_(14|14)
   -3.4689    0.40301 
diag(V_(14|14))

  0.052141   0.080404 
Score vector and diagonal of information matrix at time 15 are:
u
   -16.691    -8.0731 
U
    1.2875     0.6231 
a_(15|15)
   -3.5313     0.3911 
diag(V_(15|15))

  0.052759   0.080423 



##########################################
Starting iteration 4 with the following values
a_0
   -3.3064     0.5139 
diag(Q)
 0.0016482  0.0011785 
Score vector and diagonal of information matrix at time 1 are:
u
   -76.925    -49.547 
U
    9.0619     5.8816 
a_(1|1)
   -3.9935    0.38103 
diag(V_(1|1))

   0.19876    0.29062 
Score vector and diagonal of information matrix at time 2 are:
u
    66.663     46.072 
U
    4.0526      2.496 
a_(2|2)
   -3.5963    0.53668 
diag(V_(2|2))

   0.15259    0.22851 
Score vector and diagonal of information matrix at time 3 are:
u
    170.92     104.86 
U
    5.6099     3.5818 
a_(3|3)
   -2.6953    0.58438 
diag(V_(3|3))

   0.11774    0.17731 
Score vector and diagonal of information matrix at time 4 are:
u
   -55.276    -47.926 
U
    8.8147     5.2776 
a_(4|4)
   -2.7932    0.38535 
diag(V_(4|4))

  0.084314    0.13013 
Score vector and diagonal of information matrix at time 5 are:
u
   -23.155    -5.5628 
U
    6.3411     3.4597 
a_(5|5)
   -2.9199    0.46858 
diag(V_(5|5))

  0.069715     0.1096 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.597    -31.876 
U
    4.8092     2.4573 
a_(6|6)
   -3.0136    0.38261 
diag(V_(6|6))

  0.062108   0.098622 
Score vector and diagonal of information matrix at time 7 are:
u
   -30.895    -5.4431 
U
    3.9767     2.0119 
a_(7|7)
   -3.1659    0.48054 
diag(V_(7|7))

  0.057737   0.091758 
Score vector and diagonal of information matrix at time 8 are:
u
   -9.2144    -6.9921 
U
    3.2287       1.53 
a_(8|8)
   -3.1862    0.45952 
diag(V_(8|8))

  0.055078   0.087471 
Score vector and diagonal of information matrix at time 9 are:
u
   -23.207    -10.589 
U
    2.8567     1.3494 
a_(9|9)
   -3.2675    0.46351 
diag(V_(9|9))

    0.0534   0.084428 
Score vector and diagonal of information matrix at time 10 are:
u
   -14.746    0.47301 
U
    2.2007     1.0268 
a_(10|10)
   -3.3473    0.52318 
diag(V_(10|10))

  0.052909   0.082845 
Score vector and diagonal of information matrix at time 11 are:
u
   -10.961    -10.144 
U
    1.8373     0.7836 
a_(11|11)
    -3.369    0.47904 
diag(V_(11|11))

  0.052821   0.082059 
Score vector and diagonal of information matrix at time 12 are:
u
   -14.681    -9.5354 
U
    1.6678     0.7364 
a_(12|12)
   -3.4145    0.45108 
diag(V_(12|12))

  0.053047   0.081576 
Score vector and diagonal of information matrix at time 13 are:
u
   -19.272    -8.8965 
U
    1.4832    0.68688 
a_(13|13)
   -3.4889     0.4414 
diag(V_(13|13))

  0.053604   0.081371 
Score vector and diagonal of information matrix at time 14 are:
u
   -4.5262    -8.2153 
U
     1.345    0.63411 
a_(14|14)
   -3.4877    0.38874 
diag(V_(14|14))

  0.054337   0.081373 
Score vector and diagonal of information matrix at time 15 are:
u
   -16.285    -7.8225 
U
    1.2561     0.6037 
a_(15|15)
   -3.5538     0.3744 
diag(V_(15|15))

  0.055155   0.081484 



##########################################
Starting iteration 5 with the following values
a_0
   -3.2981    0.51357 
diag(Q)
 0.0019499  0.0012654 
Score vector and diagonal of information matrix at time 1 are:
u
   -77.833    -50.123 
U
    9.1319      5.926 
a_(1|1)
   -3.9889    0.38082 
diag(V_(1|1))

   0.19772    0.28924 
Score vector and diagonal of information matrix at time 2 are:
u
    66.431     45.932 
U
    4.0703     2.5067 
a_(2|2)
    -3.593     0.5369 
diag(V_(2|2))

   0.15213    0.22754 
Score vector and diagonal of information matrix at time 3 are:
u
    170.67     104.69 
U
    5.6285     3.5938 
a_(3|3)
   -2.6877    0.58909 
diag(V_(3|3))

   0.11765    0.17667 
Score vector and diagonal of information matrix at time 4 are:
u
   -56.357    -48.677 
U
    8.8979     5.3355 
a_(4|4)
   -2.7918    0.38785 
diag(V_(4|4))

    0.0843    0.12956 
Score vector and diagonal of information matrix at time 5 are:
u
   -23.361    -5.7217 
U
    6.3568     3.4719 
a_(5|5)
   -2.9201    0.46929 
diag(V_(5|5))

  0.070007    0.10931 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.608    -31.893 
U
    4.8101     2.4586 
a_(6|6)
   -3.0185    0.38036 
diag(V_(6|6))

  0.062691   0.098549 
Score vector and diagonal of information matrix at time 7 are:
u
     -30.6    -5.2673 
U
     3.954     1.9985 
a_(7|7)
   -3.1731    0.47592 
diag(V_(7|7))

  0.058617   0.091897 
Score vector and diagonal of information matrix at time 8 are:
u
   -8.8431    -6.7734 
U
    3.2002     1.5132 
a_(8|8)
    -3.194    0.45414 
diag(V_(8|8))

  0.056244   0.087809 
Score vector and diagonal of information matrix at time 9 are:
u
   -22.847    -10.374 
U
     2.829     1.3329 
a_(9|9)
   -3.2786    0.45559 
diag(V_(9|9))

  0.054825   0.084929 
Score vector and diagonal of information matrix at time 10 are:
u
   -14.347    0.71163 
U
    2.1701     1.0086 
a_(10|10)
   -3.3595    0.51409 
diag(V_(10|10))

  0.054625   0.083505 
Score vector and diagonal of information matrix at time 11 are:
u
   -10.598    -9.9432 
U
    1.8093    0.76802 
a_(11|11)
   -3.3835    0.46808 
diag(V_(11|11))

  0.054823   0.082883 
Score vector and diagonal of information matrix at time 12 are:
u
   -14.287    -9.3096 
U
    1.6374    0.71889 
a_(12|12)
   -3.4323    0.43763 
diag(V_(12|12))

  0.055322   0.082545 
Score vector and diagonal of information matrix at time 13 are:
u
   -18.836    -8.6375 
U
    1.4496    0.66681 
a_(13|13)
   -3.5113    0.42469 
diag(V_(13|13))

  0.056152   0.082471 
Score vector and diagonal of information matrix at time 14 are:
u
   -4.0259     -7.915 
U
    1.3065    0.61086 
a_(14|14)
   -3.5109    0.37125 
diag(V_(14|14))

  0.057157   0.082603 
Score vector and diagonal of information matrix at time 15 are:
u
     -15.8    -7.5252 
U
    1.2186    0.58068 
a_(15|15)
   -3.5814    0.35397 
diag(V_(15|15))

  0.058229    0.08283 



##########################################
Starting iteration 6 with the following values
a_0
   -3.2887    0.51274 
diag(Q)
 0.0023603   0.001384 
Score vector and diagonal of information matrix at time 1 are:
u
   -78.827     -50.74 
U
    9.2086     5.9736 
a_(1|1)
   -3.9837    0.38027 
diag(V_(1|1))

   0.19656    0.28773 
Score vector and diagonal of information matrix at time 2 are:
u
    66.177     45.782 
U
    4.0896      2.518 
a_(2|2)
   -3.5886    0.53743 
diag(V_(2|2))

   0.15168    0.22651 
Score vector and diagonal of information matrix at time 3 are:
u
    170.33     104.47 
U
    5.6542     3.6108 
a_(3|3)
   -2.6767    0.59571 
diag(V_(3|3))

   0.11763    0.17596 
Score vector and diagonal of information matrix at time 4 are:
u
   -57.912    -49.756 
U
    9.0175     5.4187 
a_(4|4)
   -2.7897    0.39131 
diag(V_(4|4))

  0.084298    0.12885 
Score vector and diagonal of information matrix at time 5 are:
u
   -23.666    -5.9528 
U
    6.3802     3.4895 
a_(5|5)
   -2.9201    0.47037 
diag(V_(5|5))

  0.070398    0.10893 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.638    -31.923 
U
    4.8124     2.4609 
a_(6|6)
   -3.0249    0.37755 
diag(V_(6|6))

  0.063461    0.09846 
Score vector and diagonal of information matrix at time 7 are:
u
   -30.226    -5.0452 
U
    3.9252     1.9815 
a_(7|7)
   -3.1824    0.47008 
diag(V_(7|7))

  0.059776   0.092081 
Score vector and diagonal of information matrix at time 8 are:
u
   -8.3733    -6.4978 
U
    3.1642      1.492 
a_(8|8)
   -3.2038    0.44743 
diag(V_(8|8))

  0.057777   0.088249 
Score vector and diagonal of information matrix at time 9 are:
u
   -22.397    -10.108 
U
    2.7944     1.3123 
a_(9|9)
   -3.2925    0.44569 
diag(V_(9|9))

  0.056689   0.085577 
Score vector and diagonal of information matrix at time 10 are:
u
   -13.851     1.0064 
U
    2.1321    0.98615 
a_(10|10)
   -3.3748    0.50279 
diag(V_(10|10))

  0.056869   0.084357 
Score vector and diagonal of information matrix at time 11 are:
u
   -10.152    -9.6977 
U
     1.775    0.74898 
a_(11|11)
   -3.4015    0.45451 
diag(V_(11|11))

   0.05744   0.083943 
Score vector and diagonal of information matrix at time 12 are:
u
   -13.807    -9.0359 
U
    1.6003    0.69768 
a_(12|12)
   -3.4544    0.42101 
diag(V_(12|12))

   0.05829   0.083788 
Score vector and diagonal of information matrix at time 13 are:
u
    -18.31    -8.3262 
U
    1.4089     0.6427 
a_(13|13)
   -3.5392    0.40405 
diag(V_(13|13))

  0.059472   0.083881 
Score vector and diagonal of information matrix at time 14 are:
u
   -3.4253    -7.5573 
U
    1.2602    0.58316 
a_(14|14)
   -3.5393    0.34983 
diag(V_(14|14))

  0.060832   0.084179 
Score vector and diagonal of information matrix at time 15 are:
u
   -15.225     -7.175 
U
    1.1741    0.55357 
a_(15|15)
   -3.6153    0.32897 
diag(V_(15|15))

   0.06223   0.084554 



##########################################
Starting iteration 7 with the following values
a_0
   -3.2785    0.51115 
diag(Q)
 0.0029315  0.0015494 
Score vector and diagonal of information matrix at time 1 are:
u
   -79.875    -51.372 
U
    9.2894     6.0224 
a_(1|1)
    -3.978    0.37922 
diag(V_(1|1))

   0.19531    0.28613 
Score vector and diagonal of information matrix at time 2 are:
u
    65.909      45.63 
U
      4.11     2.5296 
a_(2|2)
   -3.5827    0.53849 
diag(V_(2|2))

   0.15131    0.22545 
Score vector and diagonal of information matrix at time 3 are:
u
    169.85     104.14 
U
    5.6902      3.635 
a_(3|3)
    -2.661    0.60507 
diag(V_(3|3))

   0.11773     0.1752 
Score vector and diagonal of information matrix at time 4 are:
u
   -60.177    -51.327 
U
    9.1918     5.5398 
a_(4|4)
   -2.7865     0.3961 
diag(V_(4|4))

  0.084314    0.12794 
Score vector and diagonal of information matrix at time 5 are:
u
   -24.115    -6.2876 
U
    6.4144     3.5151 
a_(5|5)
     -2.92    0.47194 
diag(V_(5|5))

  0.070928    0.10846 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.692    -31.973 
U
    4.8166     2.4648 
a_(6|6)
   -3.0332    0.37399 
diag(V_(6|6))

  0.064493    0.09835 
Score vector and diagonal of information matrix at time 7 are:
u
   -29.743    -4.7602 
U
    3.8882     1.9597 
a_(7|7)
   -3.1944    0.46266 
diag(V_(7|7))

  0.061322   0.092326 
Score vector and diagonal of information matrix at time 8 are:
u
   -7.7741    -6.1481 
U
    3.1182     1.4651 
a_(8|8)
   -3.2164      0.439 
diag(V_(8|8))

  0.059818   0.088827 
Score vector and diagonal of information matrix at time 9 are:
u
   -21.835    -9.7757 
U
    2.7511     1.2867 
a_(9|9)
   -3.3103    0.43326 
diag(V_(9|9))

   0.05916   0.086421 
Score vector and diagonal of information matrix at time 10 are:
u
   -13.236     1.3702 
U
     2.085    0.95844 
a_(10|10)
    -3.394    0.48876 
diag(V_(10|10))

  0.059843   0.085465 
Score vector and diagonal of information matrix at time 11 are:
u
   -9.6068    -9.3989 
U
    1.7329    0.72582 
a_(11|11)
    -3.424    0.43772 
diag(V_(11|11))

  0.060905   0.085319 
Score vector and diagonal of information matrix at time 12 are:
u
   -13.224    -8.7062 
U
    1.5553    0.67214 
a_(12|12)
   -3.4819    0.40049 
diag(V_(12|12))

  0.062215   0.085399 
Score vector and diagonal of information matrix at time 13 are:
u
   -17.676    -7.9548 
U
      1.36    0.61394 
a_(13|13)
   -3.5739    0.37857 
diag(V_(13|13))

   0.06386   0.085705 
Score vector and diagonal of information matrix at time 14 are:
u
   -2.7094    -7.1349 
U
     1.205    0.55047 
a_(14|14)
   -3.5743    0.32369 
diag(V_(14|14))

  0.065689   0.086218 
Score vector and diagonal of information matrix at time 15 are:
u
   -14.549    -6.7674 
U
    1.1219    0.52204 
a_(15|15)
   -3.6569    0.29846 
diag(V_(15|15))

  0.067513    0.08678 



##########################################
Starting iteration 8 with the following values
a_0
   -3.2679     0.5083 
diag(Q)
 0.0037463  0.0017857 
Score vector and diagonal of information matrix at time 1 are:
u
   -80.879    -51.941 
U
    9.3668     6.0663 
a_(1|1)
   -3.9719    0.37731 
diag(V_(1|1))

   0.19402    0.28458 
Score vector and diagonal of information matrix at time 2 are:
u
    65.653     45.497 
U
    4.1295     2.5397 
a_(2|2)
   -3.5745    0.54043 
diag(V_(2|2))

   0.15113    0.22449 
Score vector and diagonal of information matrix at time 3 are:
u
    169.16     103.67 
U
    5.7418     3.6702 
a_(3|3)
   -2.6381    0.61837 
diag(V_(3|3))

   0.11805    0.17443 
Score vector and diagonal of information matrix at time 4 are:
u
   -63.517    -53.641 
U
    9.4489     5.7183 
a_(4|4)
   -2.7817    0.40273 
diag(V_(4|4))

  0.084348    0.12677 
Score vector and diagonal of information matrix at time 5 are:
u
   -24.768    -6.7702 
U
    6.4644     3.5519 
a_(5|5)
   -2.9197    0.47419 
diag(V_(5|5))

  0.071648    0.10784 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.776    -32.048 
U
    4.8232     2.4706 
a_(6|6)
   -3.0442    0.36941 
diag(V_(6|6))

  0.065886   0.098214 
Score vector and diagonal of information matrix at time 7 are:
u
   -29.113      -4.39 
U
    3.8398     1.9315 
a_(7|7)
     -3.21    0.45315 
diag(V_(7|7))

   0.06341   0.092656 
Score vector and diagonal of information matrix at time 8 are:
u
   -7.0068    -5.7032 
U
    3.0593      1.431 
a_(8|8)
   -3.2324    0.42847 
diag(V_(8|8))

   0.06257   0.089593 
Score vector and diagonal of information matrix at time 9 are:
u
   -21.133     -9.364 
U
     2.697      1.255 
a_(9|9)
   -3.3329    0.41771 
diag(V_(9|9))

  0.062474   0.087529 
Score vector and diagonal of information matrix at time 10 are:
u
   -12.477     1.8159 
U
    2.0268    0.92452 
a_(10|10)
    -3.418    0.47142 
diag(V_(10|10))

   0.06383   0.086915 
Score vector and diagonal of information matrix at time 11 are:
u
   -8.9451    -9.0395 
U
     1.682    0.69796 
a_(11|11)
    -3.452    0.41711 
diag(V_(11|11))

   0.06555   0.087117 
Score vector and diagonal of information matrix at time 12 are:
u
   -12.526    -8.3146 
U
    1.5014     0.6418 
a_(12|12)
   -3.5159    0.37535 
diag(V_(12|12))

   0.06747   0.087498 
Score vector and diagonal of information matrix at time 13 are:
u
   -16.925    -7.5185 
U
     1.302    0.58016 
a_(13|13)
    -3.617    0.34728 
diag(V_(13|13))

  0.069733    0.08808 
Score vector and diagonal of information matrix at time 14 are:
u
   -1.8677    -6.6443 
U
    1.1402    0.51252 
a_(14|14)
   -3.6168    0.29207 
diag(V_(14|14))

  0.072199   0.088874 
Score vector and diagonal of information matrix at time 15 are:
u
   -13.769    -6.3021 
U
    1.0616    0.48605 
a_(15|15)
   -3.7076     0.2615 
diag(V_(15|15))

  0.074588   0.089677 



##########################################
Starting iteration 9 with the following values
a_0
   -3.2581    0.50346 
diag(Q)
 0.0049382  0.0021315 
Score vector and diagonal of information matrix at time 1 are:
u
    -81.65    -52.307 
U
    9.4263     6.0945 
a_(1|1)
   -3.9663    0.37403 
diag(V_(1|1))

   0.19286    0.28333 
Score vector and diagonal of information matrix at time 2 are:
u
    65.458     45.419 
U
    4.1444     2.5456 
a_(2|2)
   -3.5631    0.54385 
diag(V_(2|2))

   0.15136    0.22386 
Score vector and diagonal of information matrix at time 3 are:
u
    168.16     102.97 
U
    5.8169     3.7222 
a_(3|3)
    -2.605    0.63715 
diag(V_(3|3))

   0.11873    0.17374 
Score vector and diagonal of information matrix at time 4 are:
u
   -68.467    -57.071 
U
    9.8301     5.9828 
a_(4|4)
   -2.7747    0.41182 
diag(V_(4|4))

  0.084384    0.12525 
Score vector and diagonal of information matrix at time 5 are:
u
   -25.701     -7.455 
U
    6.5358     3.6041 
a_(5|5)
   -2.9193    0.47728 
diag(V_(5|5))

  0.072621    0.10703 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.891    -32.151 
U
    4.8321     2.4786 
a_(6|6)
    -3.059    0.36341 
diag(V_(6|6))

  0.067776   0.098045 
Score vector and diagonal of information matrix at time 7 are:
u
   -28.281    -3.9034 
U
    3.7758     1.8943 
a_(7|7)
   -3.2307    0.44098 
diag(V_(7|7))

  0.066251   0.093106 
Score vector and diagonal of information matrix at time 8 are:
u
   -6.0255    -5.1385 
U
     2.984     1.3876 
a_(8|8)
   -3.2527    0.41542 
diag(V_(8|8))

  0.066316   0.090617 
Score vector and diagonal of information matrix at time 9 are:
u
   -20.267    -8.8596 
U
    2.6303     1.2162 
a_(9|9)
   -3.3614    0.39845 
diag(V_(9|9))

  0.066957   0.088988 
Score vector and diagonal of information matrix at time 10 are:
u
   -11.552     2.3536 
U
     1.956    0.88359 
a_(10|10)
   -3.4476    0.45032 
diag(V_(10|10))

  0.069233   0.088825 
Score vector and diagonal of information matrix at time 11 are:
u
   -8.1572    -8.6158 
U
    1.6213    0.66513 
a_(11|11)
   -3.4862    0.39219 
diag(V_(11|11))

  0.071845   0.089479 
Score vector and diagonal of information matrix at time 12 are:
u
   -11.706    -7.8596 
U
    1.4381    0.60657 
a_(12|12)
   -3.5576    0.34497 
diag(V_(12|12))

  0.074585   0.090247 
Score vector and diagonal of information matrix at time 13 are:
u
   -16.051    -7.0179 
U
    1.2346    0.54142 
a_(13|13)
   -3.6699    0.30932 
diag(V_(13|13))

  0.077687   0.091188 
Score vector and diagonal of information matrix at time 14 are:
u
  -0.89917    -6.0883 
U
    1.0657    0.46952 
a_(14|14)
   -3.6679    0.25446 
diag(V_(14|14))

  0.081042   0.092358 
Score vector and diagonal of information matrix at time 15 are:
u
   -12.891    -5.7858 
U
    0.9937    0.44614 
a_(15|15)
   -3.7688    0.21737 
diag(V_(15|15))

  0.084194   0.093474 



##########################################
Starting iteration 10 with the following values
a_0
   -3.2511     0.4956 
diag(Q)
 0.0067267  0.0026506 
Score vector and diagonal of information matrix at time 1 are:
u
   -81.877    -52.242 
U
    9.4438     6.0895 
a_(1|1)
    -3.962    0.36866 
diag(V_(1|1))

   0.19206    0.28279 
Score vector and diagonal of information matrix at time 2 are:
u
    65.409     45.456 
U
    4.1482     2.5428 
a_(2|2)
   -3.5471    0.54973 
diag(V_(2|2))

   0.15235    0.22391 
Score vector and diagonal of information matrix at time 3 are:
u
    166.67     101.92 
U
    5.9284     3.8007 
a_(3|3)
   -2.5579    0.66307 
diag(V_(3|3))

   0.11998    0.17326 
Score vector and diagonal of information matrix at time 4 are:
u
   -75.752    -62.118 
U
    10.391     6.3724 
a_(4|4)
   -2.7651      0.424 
diag(V_(4|4))

  0.084364    0.12327 
Score vector and diagonal of information matrix at time 5 are:
u
   -26.993    -8.3999 
U
    6.6346     3.6763 
a_(5|5)
   -2.9191    0.48125 
diag(V_(5|5))

  0.073922    0.10599 
Score vector and diagonal of information matrix at time 6 are:
u
   -43.016    -32.272 
U
    4.8418      2.488 
a_(6|6)
   -3.0792    0.35541 
diag(V_(6|6))

  0.070342   0.097848 
Score vector and diagonal of information matrix at time 7 are:
u
   -27.172    -3.2583 
U
    3.6907      1.845 
a_(7|7)
   -3.2577    0.42544 
diag(V_(7|7))

  0.070143   0.093733 
Score vector and diagonal of information matrix at time 8 are:
u
   -4.7797    -4.4281 
U
    2.8884      1.333 
a_(8|8)
   -3.2781    0.39954 
diag(V_(8|8))

   0.07146   0.092002 
Score vector and diagonal of information matrix at time 9 are:
u
   -19.218    -8.2553 
U
    2.5496     1.1697 
a_(9|9)
   -3.3969      0.375 
diag(V_(9|9))

  0.073065   0.090923 
Score vector and diagonal of information matrix at time 10 are:
u
   -10.451     2.9865 
U
    1.8717    0.83543 
a_(10|10)
   -3.4835    0.42522 
diag(V_(10|10))

  0.076615   0.091352 
Score vector and diagonal of information matrix at time 11 are:
u
   -7.2455    -8.1316 
U
    1.5512    0.62763 
a_(11|11)
   -3.5273    0.36279 
diag(V_(11|11))

  0.080451   0.092594 
Score vector and diagonal of information matrix at time 12 are:
u
   -10.771    -7.3482 
U
     1.366    0.56698 
a_(12|12)
   -3.6077    0.30909 
diag(V_(12|12))

  0.084304   0.093863 
Score vector and diagonal of information matrix at time 13 are:
u
   -15.066    -6.4622 
U
    1.1586    0.49844 
a_(13|13)
    -3.734    0.26413 
diag(V_(13|13))

  0.088571   0.095277 
Score vector and diagonal of information matrix at time 14 are:
u
   0.18348    -5.4786 
U
   0.98241     0.4224 
a_(14|14)
    -3.728    0.21087 
diag(V_(14|14))

    0.0932   0.096958 
Score vector and diagonal of information matrix at time 15 are:
u
   -11.935    -5.2341 
U
   0.91991     0.4035 
a_(15|15)
   -3.8414    0.16579 
diag(V_(15|15))

  0.097397   0.098481 



##########################################
Starting iteration 11 with the following values
a_0
     -3.25    0.48332 
diag(Q)
 0.0094775  0.0034491 
Score vector and diagonal of information matrix at time 1 are:
u
   -81.101     -51.42 
U
    9.3839     6.0261 
a_(1|1)
    -3.961    0.36023 
diag(V_(1|1))

   0.19204    0.28358 
Score vector and diagonal of information matrix at time 2 are:
u
    65.625     45.695 
U
    4.1317     2.5247 
a_(2|2)
    -3.524    0.55971 
diag(V_(2|2))

   0.15461    0.22521 
Score vector and diagonal of information matrix at time 3 are:
u
     164.4     100.29 
U
    6.0978      3.922 
a_(3|3)
   -2.4935    0.69704 
diag(V_(3|3))

   0.12202    0.17312 
Score vector and diagonal of information matrix at time 4 are:
u
   -86.167    -69.328 
U
    11.194     6.9294 
a_(4|4)
   -2.7525    0.43964 
diag(V_(4|4))

  0.084166     0.1207 
Score vector and diagonal of information matrix at time 5 are:
u
   -28.693     -9.645 
U
    6.7647     3.7713 
a_(5|5)
   -2.9201    0.48585 
diag(V_(5|5))

  0.075636    0.10468 
Score vector and diagonal of information matrix at time 6 are:
u
   -43.094    -32.378 
U
    4.8478     2.4962 
a_(6|6)
   -3.1069    0.34443 
diag(V_(6|6))

  0.073814   0.097662 
Score vector and diagonal of information matrix at time 7 are:
u
   -25.691    -2.3987 
U
    3.5769     1.7794 
a_(7|7)
    -3.293    0.40571 
diag(V_(7|7))

  0.075502   0.094648 
Score vector and diagonal of information matrix at time 8 are:
u
   -3.2241    -3.5499 
U
    2.7691     1.2656 
a_(8|8)
   -3.3091    0.38073 
diag(V_(8|8))

  0.078571   0.093914 
Score vector and diagonal of information matrix at time 9 are:
u
   -17.992    -7.5557 
U
    2.4552     1.1159 
a_(9|9)
     -3.44    0.34716 
diag(V_(9|9))

  0.081411   0.093511 
Score vector and diagonal of information matrix at time 10 are:
u
   -9.1839     3.7057 
U
    1.7746    0.78074 
a_(10|10)
   -3.5254     0.3964 
diag(V_(10|10))

  0.086757   0.094723 
Score vector and diagonal of information matrix at time 11 are:
u
   -6.2338    -7.6019 
U
    1.4733    0.58662 
a_(11|11)
   -3.5751    0.32928 
diag(V_(11|11))

  0.092276   0.096726 
Score vector and diagonal of information matrix at time 12 are:
u
   -9.7493    -6.7982 
U
    1.2872    0.52443 
a_(12|12)
   -3.6663    0.26802 
diag(V_(12|12))

  0.097649   0.098636 
Score vector and diagonal of information matrix at time 13 are:
u
   -13.999    -5.8712 
U
    1.0763    0.45274 
a_(13|13)
   -3.8101    0.21166 
diag(V_(13|13))

   0.10356    0.10068 
Score vector and diagonal of information matrix at time 14 are:
u
    1.3496    -4.8371 
U
   0.89275    0.37284 
a_(14|14)
   -3.7962    0.16208 
diag(V_(14|14))

   0.11008    0.10307 
Score vector and diagonal of information matrix at time 15 are:
u
   -10.942    -4.6722 
U
   0.84321    0.36011 
a_(15|15)
   -3.9253    0.10717 
diag(V_(15|15))

   0.11571    0.10512 



##########################################
Starting iteration 12 with the following values
a_0
   -3.2589    0.46493 
diag(Q)
  0.013792  0.0047011 
Score vector and diagonal of information matrix at time 1 are:
u
   -78.757    -49.445 
U
    9.2031     5.8737 
a_(1|1)
   -3.9653     0.3477 
diag(V_(1|1))

    0.1933    0.28656 
Score vector and diagonal of information matrix at time 2 are:
u
    66.254     46.235 
U
     4.084     2.4837 
a_(2|2)
   -3.4904    0.57636 
diag(V_(2|2))

   0.15886    0.22849 
Score vector and diagonal of information matrix at time 3 are:
u
    160.88     97.731 
U
    6.3611     4.1132 
a_(3|3)
   -2.4123     0.7371 
diag(V_(3|3))

   0.12492    0.17338 
Score vector and diagonal of information matrix at time 4 are:
u
   -100.01    -78.881 
U
    12.262      7.668 
a_(4|4)
   -2.7376    0.45795 
diag(V_(4|4))

  0.083645    0.11755 
Score vector and diagonal of information matrix at time 5 are:
u
   -30.742    -11.147 
U
    6.9215     3.8861 
a_(5|5)
   -2.9238    0.48985 
diag(V_(5|5))

  0.077886     0.1032 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.995    -32.387 
U
    4.8402     2.4969 
a_(6|6)
   -3.1445     0.3286 
diag(V_(6|6))

  0.078491   0.097639 
Score vector and diagonal of information matrix at time 7 are:
u
   -23.725     -1.255 
U
     3.426     1.6922 
a_(7|7)
   -3.3378    0.38067 
diag(V_(7|7))

  0.082899   0.096086 
Score vector and diagonal of information matrix at time 8 are:
u
   -1.3372    -2.4935 
U
    2.6244     1.1846 
a_(8|8)
   -3.3449    0.35912 
diag(V_(8|8))

  0.088426   0.096646 
Score vector and diagonal of information matrix at time 9 are:
u
   -16.633    -6.7869 
U
    2.3506     1.0567 
a_(9|9)
   -3.4903    0.31506 
diag(V_(9|9))

  0.092762    0.09702 
Score vector and diagonal of information matrix at time 10 are:
u
   -7.7938     4.4841 
U
    1.6681    0.72155 
a_(10|10)
   -3.5717    0.36472 
diag(V_(10|10))

   0.10066   0.099261 
Score vector and diagonal of information matrix at time 11 are:
u
   -5.1769    -7.0559 
U
     1.392    0.54436 
a_(11|11)
   -3.6277    0.29271 
diag(V_(11|11))

   0.10847    0.10223 
Score vector and diagonal of information matrix at time 12 are:
u
   -8.6958    -6.2402 
U
     1.206    0.48127 
a_(12|12)
   -3.7317    0.22274 
diag(V_(12|12))

   0.11591    0.10495 
Score vector and diagonal of information matrix at time 13 are:
u
   -12.902    -5.2754 
U
   0.99167     0.4067 
a_(13|13)
   -3.8974    0.15253 
diag(V_(13|13))

   0.12418    0.10783 
Score vector and diagonal of information matrix at time 14 are:
u
    2.5489    -4.1949 
U
   0.80059    0.32325 
a_(14|14)
   -3.8699     0.1099 
diag(V_(14|14))

   0.13359    0.11124 
Score vector and diagonal of information matrix at time 15 are:
u
   -9.9672    -4.1332 
U
   0.76796    0.31849 
a_(15|15)
   -4.0189    0.04281 
diag(V_(15|15))

   0.14115    0.11396 



##########################################
Starting iteration 13 with the following values
a_0
   -3.2821    0.43846 
diag(Q)
  0.020583  0.0066691 
Score vector and diagonal of information matrix at time 1 are:
u
   -74.315    -45.961 
U
    8.8605      5.605 
a_(1|1)
   -3.9772    0.33011 
diag(V_(1|1))

   0.19647    0.29271 
Score vector and diagonal of information matrix at time 2 are:
u
    67.424     47.162 
U
     3.995     2.4133 
a_(2|2)
   -3.4416    0.60323 
diag(V_(2|2))

   0.16587    0.23458 
Score vector and diagonal of information matrix at time 3 are:
u
    155.36     93.649 
U
    6.7746     4.4179 
a_(3|3)
   -2.3248    0.77504 
diag(V_(3|3))

   0.12838    0.17381 
Score vector and diagonal of information matrix at time 4 are:
u
   -115.64    -89.561 
U
    13.469     8.4944 
a_(4|4)
   -2.7234    0.47457 
diag(V_(4|4))

  0.082864    0.11423 
Score vector and diagonal of information matrix at time 5 are:
u
   -32.701    -12.578 
U
    7.0714     3.9954 
a_(5|5)
   -2.9329    0.48961 
diag(V_(5|5))

  0.080906    0.10196 
Score vector and diagonal of information matrix at time 6 are:
u
   -42.459    -32.112 
U
    4.7989     2.4756 
a_(6|6)
   -3.1943    0.30425 
diag(V_(6|6))

  0.084724   0.098189 
Score vector and diagonal of information matrix at time 7 are:
u
   -21.182    0.24352 
U
    3.2309     1.5778 
a_(7|7)
   -3.3919    0.34858 
diag(V_(7|7))

  0.093062   0.098535 
Score vector and diagonal of information matrix at time 8 are:
u
   0.83592    -1.2778 
U
    2.4579     1.0913 
a_(8|8)
   -3.3821    0.33486 
diag(V_(8|8))

   0.10195     0.1007 
Score vector and diagonal of information matrix at time 9 are:
u
   -15.258    -6.0056 
U
    2.2448    0.99667 
a_(9|9)
   -3.5446    0.27902 
diag(V_(9|9))

   0.10782    0.10183 
Score vector and diagonal of information matrix at time 10 are:
u
   -6.3765     5.2729 
U
    1.5596     0.6616 
a_(10|10)
   -3.6181     0.3315 
diag(V_(10|10))

   0.11933     0.1054 
Score vector and diagonal of information matrix at time 11 are:
u
   -4.1686    -6.5375 
U
    1.3145    0.50426 
a_(11|11)
   -3.6807    0.25453 
diag(V_(11|11))

   0.13012    0.10954 
Score vector and diagonal of information matrix at time 12 are:
u
   -7.6953     -5.715 
U
    1.1289    0.44067 
a_(12|12)
   -3.7999    0.17469 
diag(V_(12|12))

   0.14028    0.11323 
Score vector and diagonal of information matrix at time 13 are:
u
   -11.848    -4.7128 
U
   0.91048    0.36324 
a_(13|13)
   -3.9925   0.087953 
diag(V_(13|13))

   0.15195    0.11723 
Score vector and diagonal of information matrix at time 14 are:
u
    3.7112    -3.5891 
U
   0.71133     0.2765 
a_(14|14)
   -3.9433   0.056894 
diag(V_(14|14))

   0.16585    0.12213 
Score vector and diagonal of information matrix at time 15 are:
u
   -9.0834    -3.6534 
U
   0.69976    0.28146 
a_(15|15)
   -4.1176  -0.025247 
diag(V_(15|15))

   0.17584    0.12564 



##########################################
Starting iteration 14 with the following values
a_0
   -3.3226    0.40192 
diag(Q)
  0.030888  0.0096497 
Score vector and diagonal of information matrix at time 1 are:
u
     -67.6     -40.86 
U
    8.3429     5.2117 
a_(1|1)
   -3.9976    0.30696 
diag(V_(1|1))

   0.20201    0.30285 
Score vector and diagonal of information matrix at time 2 are:
u
    69.156     48.483 
U
    3.8633      2.313 
a_(2|2)
   -3.3737    0.64351 
diag(V_(2|2))

   0.17604    0.24404 
Score vector and diagonal of information matrix at time 3 are:
u
    146.91     87.317 
U
    7.4075     4.8907 
a_(3|3)
   -2.2548    0.79568 
diag(V_(3|3))

    0.1314    0.17378 
Score vector and diagonal of information matrix at time 4 are:
u
   -128.07    -97.772 
U
    14.429     9.1301 
a_(4|4)
    -2.718    0.47889 
diag(V_(4|4))

  0.082409    0.11175 
Score vector and diagonal of information matrix at time 5 are:
u
   -33.368     -13.04 
U
    7.1225     4.0307 
a_(5|5)
   -2.9501    0.47847 
diag(V_(5|5))

  0.085064    0.10183 
Score vector and diagonal of information matrix at time 6 are:
u
   -41.151    -31.293 
U
    4.6978     2.4121 
a_(6|6)
    -3.255    0.26653 
diag(V_(6|6))

  0.092766    0.10002 
Score vector and diagonal of information matrix at time 7 are:
u
    -18.13     2.0861 
U
    2.9968     1.4374 
a_(7|7)
   -3.4505    0.30795 
diag(V_(7|7))

   0.10657    0.10271 
Score vector and diagonal of information matrix at time 8 are:
u
    3.1064  0.0093888 
U
    2.2839    0.99266 
a_(8|8)
   -3.4133     0.3083 
diag(V_(8|8))

   0.11967     0.1067 
Score vector and diagonal of information matrix at time 9 are:
u
   -14.081    -5.3099 
U
    2.1543     0.9432 
a_(9|9)
   -3.5956    0.23976 
diag(V_(9|9))

   0.12647    0.10826 
Score vector and diagonal of information matrix at time 10 are:
u
   -5.0879     5.9988 
U
     1.461    0.60645 
a_(10|10)
    -3.657    0.29828 
diag(V_(10|10))

   0.14288    0.11347 
Score vector and diagonal of information matrix at time 11 are:
u
   -3.3352    -6.0991 
U
    1.2505    0.47036 
a_(11|11)
   -3.7269    0.21621 
diag(V_(11|11))

    0.1572    0.11887 
Score vector and diagonal of information matrix at time 12 are:
u
   -6.8513    -5.2663 
U
    1.0639      0.406 
a_(12|12)
   -3.8636    0.12554 
diag(V_(12|12))

   0.17074    0.12367 
Score vector and diagonal of information matrix at time 13 are:
u
   -10.927    -4.2228 
U
   0.83959    0.32541 
a_(13|13)
   -4.0878   0.019999 
diag(V_(13|13))

   0.18716    0.12914 
Score vector and diagonal of information matrix at time 14 are:
u
    4.7509    -3.0583 
U
   0.63151    0.23556 
a_(14|14)
   -4.0068  0.0059823 
diag(V_(14|14))

   0.20789    0.13621 
Score vector and diagonal of information matrix at time 15 are:
u
   -8.3682     -3.266 
U
   0.64459    0.25157 
a_(15|15)
   -4.2125  -0.094244 
diag(V_(15|15))

   0.22045    0.14049 



##########################################
Starting iteration 15 with the following values
a_0
   -3.3771    0.35479 
diag(Q)
  0.044975    0.01371 
Score vector and diagonal of information matrix at time 1 are:
u
    -59.29    -34.635 
U
    7.7026      4.732 
a_(1|1)
   -4.0237    0.27921 
diag(V_(1|1))

   0.20971    0.31681 
Score vector and diagonal of information matrix at time 2 are:
u
    71.229     50.042 
U
    3.7057     2.1946 
a_(2|2)
   -3.2888    0.69628 
diag(V_(2|2))

   0.18858    0.25639 
Score vector and diagonal of information matrix at time 3 are:
u
    135.11     78.362 
U
    8.2913     5.5598 
a_(3|3)
   -2.2265    0.78807 
diag(V_(3|3))

   0.13273    0.17254 
Score vector and diagonal of information matrix at time 4 are:
u
   -131.78    -99.709 
U
    14.715     9.2801 
a_(4|4)
   -2.7304    0.46121 
diag(V_(4|4))

  0.083062    0.11107 
Score vector and diagonal of information matrix at time 5 are:
u
   -31.492     -11.63 
U
     6.979     3.9229 
a_(5|5)
   -2.9739    0.45346 
diag(V_(5|5))

   0.09053    0.10353 
Score vector and diagonal of information matrix at time 6 are:
u
   -39.116    -29.923 
U
    4.5405     2.3059 
a_(6|6)
   -3.3182    0.21628 
diag(V_(6|6))

    0.1022    0.10351 
Score vector and diagonal of information matrix at time 7 are:
u
   -15.006     4.0197 
U
    2.7574       1.29 
a_(7|7)
   -3.5031    0.26213 
diag(V_(7|7))

   0.12288    0.10884 
Score vector and diagonal of information matrix at time 8 are:
u
    5.1136     1.1773 
U
    2.1302    0.90318 
a_(8|8)
   -3.4299    0.28219 
diag(V_(8|8))

   0.14036    0.11456 
Score vector and diagonal of information matrix at time 9 are:
u
    -13.34    -4.8192 
U
    2.0973    0.90549 
a_(9|9)
   -3.6346    0.20055 
diag(V_(9|9))

   0.14648    0.11584 
Score vector and diagonal of information matrix at time 10 are:
u
    -4.094     6.5805 
U
     1.385    0.56228 
a_(10|10)
   -3.6817    0.26772 
diag(V_(10|10))

   0.16902    0.12294 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.7755    -5.7826 
U
    1.2074    0.44589 
a_(11|11)
   -3.7599    0.18021 
diag(V_(11|11))

    0.1869    0.12952 
Score vector and diagonal of information matrix at time 12 are:
u
   -6.2397     -4.926 
U
    1.0168     0.3797 
a_(12|12)
   -3.9149   0.078504 
diag(V_(12|12))

   0.20428    0.13546 
Score vector and diagonal of information matrix at time 13 are:
u
   -10.212    -3.8361 
U
   0.78448    0.29556 
a_(13|13)
    -4.173   -0.04642 
diag(V_(13|13))

   0.22686    0.14276 
Score vector and diagonal of information matrix at time 14 are:
u
    5.5913    -2.6349 
U
   0.56702    0.20291 
a_(14|14)
   -4.0517  -0.038955 
diag(V_(14|14))

   0.25708    0.15274 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.8727    -2.9904 
U
   0.60638    0.23032 
a_(15|15)
   -4.2931    -0.1592 
diag(V_(15|15))

   0.27136    0.15748 



##########################################
Starting iteration 16 with the following values
a_0
   -3.4348    0.30202 
diag(Q)
  0.061139   0.018347 
Score vector and diagonal of information matrix at time 1 are:
u
   -51.045    -28.484 
U
    7.0677     4.2584 
a_(1|1)
   -4.0489    0.25082 
diag(V_(1|1))

   0.21821    0.33242 
Score vector and diagonal of information matrix at time 2 are:
u
     73.19     51.519 
U
    3.5566     2.0825 
a_(2|2)
      -3.2    0.75332 
diag(V_(2|2))

   0.20118    0.26948 
Score vector and diagonal of information matrix at time 3 are:
u
    121.19     67.666 
U
    9.3346     6.3597 
a_(3|3)
   -2.2371    0.76122 
diag(V_(3|3))

    0.1321    0.17003 
Score vector and diagonal of information matrix at time 4 are:
u
   -127.62    -96.223 
U
    14.394     9.0102 
a_(4|4)
   -2.7577    0.42856 
diag(V_(4|4))

  0.084747    0.11189 
Score vector and diagonal of information matrix at time 5 are:
u
   -27.783    -8.9234 
U
    6.6951     3.7162 
a_(5|5)
   -2.9971    0.42353 
diag(V_(5|5))

  0.096571    0.10651 
Score vector and diagonal of information matrix at time 6 are:
u
   -37.053    -28.495 
U
    4.3812     2.1953 
a_(6|6)
    -3.372    0.16593 
diag(V_(6|6))

   0.11145    0.10777 
Score vector and diagonal of information matrix at time 7 are:
u
   -12.437      5.628 
U
    2.5606     1.1675 
a_(7|7)
    -3.541    0.22142 
diag(V_(7|7))

   0.13938    0.11569 
Score vector and diagonal of information matrix at time 8 are:
u
    6.5511     2.0365 
U
    2.0201    0.83739 
a_(8|8)
    -3.431    0.26212 
diag(V_(8|8))

   0.16042    0.12271 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.086    -4.5819 
U
    2.0778    0.88727 
a_(9|9)
   -3.6588    0.16796 
diag(V_(9|9))

   0.16396    0.12293 
Score vector and diagonal of information matrix at time 10 are:
u
   -3.4488     6.9772 
U
    1.3357    0.53216 
a_(10|10)
   -3.6929    0.24395 
diag(V_(10|10))

   0.19327    0.13202 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.4759    -5.5909 
U
    1.1844    0.43108 
a_(11|11)
   -3.7796    0.15101 
diag(V_(11|11))

   0.21413    0.13951 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.8526    -4.6974 
U
   0.98696    0.36205 
a_(12|12)
   -3.9517   0.039448 
diag(V_(12|12))

    0.2354    0.14656 
Score vector and diagonal of information matrix at time 13 are:
u
   -9.7127    -3.5621 
U
   0.74605    0.27442 
a_(13|13)
   -4.2407   -0.10281 
diag(V_(13|13))

   0.26485    0.15586 
Score vector and diagonal of information matrix at time 14 are:
u
    6.2026    -2.3315 
U
   0.52013    0.17952 
a_(14|14)
   -4.0769  -0.072829 
diag(V_(14|14))

   0.30615    0.16919 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.5846    -2.8217 
U
   0.58416    0.21731 
a_(15|15)
   -4.3535   -0.21238 
diag(V_(15|15))

   0.32028    0.17378 



##########################################
Starting iteration 17 with the following values
a_0
    -3.484    0.25444 
diag(Q)
  0.076484   0.022722 
Score vector and diagonal of information matrix at time 1 are:
u
   -44.443    -23.579 
U
    6.5595      3.881 
a_(1|1)
   -4.0683     0.2276 
diag(V_(1|1))

   0.22579    0.34649 
Score vector and diagonal of information matrix at time 2 are:
u
    74.674     52.639 
U
    3.4439     1.9976 
a_(2|2)
   -3.1235    0.80375 
diag(V_(2|2))

   0.21162     0.2807 
Score vector and diagonal of information matrix at time 3 are:
u
    107.77     57.248 
U
    10.341     7.1394 
a_(3|3)
   -2.2623    0.73409 
diag(V_(3|3))

   0.13048    0.16707 
Score vector and diagonal of information matrix at time 4 are:
u
   -121.31    -91.473 
U
    13.906     8.6425 
a_(4|4)
   -2.7866    0.39786 
diag(V_(4|4))

  0.086596    0.11304 
Score vector and diagonal of information matrix at time 5 are:
u
   -24.184    -6.3589 
U
    6.4197     3.5205 
a_(5|5)
   -3.0141    0.40006 
diag(V_(5|5))

    0.1019    0.10941 
Score vector and diagonal of information matrix at time 6 are:
u
    -35.55    -27.449 
U
    4.2651     2.1144 
a_(6|6)
   -3.4107    0.12779 
diag(V_(6|6))

   0.11886    0.11142 
Score vector and diagonal of information matrix at time 7 are:
u
   -10.682     6.7208 
U
    2.4262     1.0843 
a_(7|7)
   -3.5642    0.19324 
diag(V_(7|7))

   0.15322    0.12153 
Score vector and diagonal of information matrix at time 8 are:
u
    7.4248     2.5657 
U
    1.9532    0.79687 
a_(8|8)
   -3.4247    0.25072 
diag(V_(8|8))

   0.17655    0.12933 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.124    -4.5257 
U
    2.0807    0.88294 
a_(9|9)
   -3.6727    0.14595 
diag(V_(9|9))

   0.17668    0.12818 
Score vector and diagonal of information matrix at time 10 are:
u
   -3.0689     7.2172 
U
    1.3067    0.51394 
a_(10|10)
   -3.6968    0.22889 
diag(V_(10|10))

   0.21237    0.13911 
Score vector and diagonal of information matrix at time 11 are:
u
    -2.336     -5.489 
U
    1.1736     0.4232 
a_(11|11)
   -3.7912    0.13132 
diag(V_(11|11))

   0.23539    0.14724 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.6205    -4.5568 
U
   0.96908    0.35119 
a_(12|12)
   -3.9767   0.012117 
diag(V_(12|12))

   0.26012    0.15525 
Score vector and diagonal of information matrix at time 13 are:
u
    -9.386    -3.3842 
U
    0.7209     0.2607 
a_(13|13)
   -4.2903   -0.14365 
diag(V_(13|13))

     0.296     0.1664 
Score vector and diagonal of information matrix at time 14 are:
u
    6.6147    -2.1325 
U
   0.48853    0.16418 
a_(14|14)
   -4.0888  -0.093784 
diag(V_(14|14))

   0.34794    0.18292 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.4395    -2.7319 
U
   0.57297    0.21039 
a_(15|15)
   -4.3951    -0.2494 
diag(V_(15|15))

   0.36006    0.18681 



##########################################
Starting iteration 18 with the following values
a_0
   -3.5206    0.21954 
diag(Q)
  0.089179   0.026315 
Score vector and diagonal of information matrix at time 1 are:
u
   -39.877    -20.226 
U
    6.2082     3.6231 
a_(1|1)
   -4.0815    0.21223 
diag(V_(1|1))

   0.23163    0.35721 
Score vector and diagonal of information matrix at time 2 are:
u
    75.646     53.366 
U
      3.37     1.9424 
a_(2|2)
   -3.0661    0.84219 
diag(V_(2|2))

   0.21928    0.28893 
Score vector and diagonal of information matrix at time 3 are:
u
    96.802     48.662 
U
    11.164     7.7824 
a_(3|3)
   -2.2852    0.71525 
diag(V_(3|3))

   0.12883    0.16447 
Score vector and diagonal of information matrix at time 4 are:
u
   -116.23    -87.792 
U
    13.514     8.3575 
a_(4|4)
   -2.8091    0.37656 
diag(V_(4|4))

  0.088043    0.11389 
Score vector and diagonal of information matrix at time 5 are:
u
   -21.582     -4.548 
U
    6.2207     3.3823 
a_(5|5)
   -3.0251    0.38551 
diag(V_(5|5))

   0.10588    0.11153 
Score vector and diagonal of information matrix at time 6 are:
u
    -34.62    -26.809 
U
    4.1933     2.0648 
a_(6|6)
   -3.4366     0.1033 
diag(V_(6|6))

   0.12407    0.11395 
Score vector and diagonal of information matrix at time 7 are:
u
   -9.5784     7.3954 
U
    2.3418      1.033 
a_(7|7)
   -3.5779    0.17648 
diag(V_(7|7))

   0.16348    0.12572 
Score vector and diagonal of information matrix at time 8 are:
u
    7.9263     2.8676 
U
    1.9149    0.77377 
a_(8|8)
   -3.4176    0.24575 
diag(V_(8|8))

   0.18813    0.13395 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.256    -4.5494 
U
    2.0909    0.88477 
a_(9|9)
   -3.6811     0.1329 
diag(V_(9|9))

   0.18509    0.13155 
Score vector and diagonal of information matrix at time 10 are:
u
   -2.8452     7.3568 
U
    1.2896    0.50334 
a_(10|10)
   -3.6983    0.22061 
diag(V_(10|10))

   0.22609      0.144 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.2703    -5.4375 
U
    1.1685    0.41922 
a_(11|11)
   -3.7985    0.11956 
diag(V_(11|11))

   0.25055    0.15255 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.4788    -4.4728 
U
   0.95817    0.34471 
a_(12|12)
   -3.9937 -0.0050317 
diag(V_(12|12))

   0.27804    0.16132 
Score vector and diagonal of information matrix at time 13 are:
u
   -9.1753    -3.2728 
U
   0.70468     0.2521 
a_(13|13)
   -4.3252   -0.17046 
diag(V_(13|13))

   0.31916    0.17399 
Score vector and diagonal of information matrix at time 14 are:
u
    6.8844    -2.0064 
U
   0.46785    0.15447 
a_(14|14)
    -4.094   -0.10496 
diag(V_(14|14))

   0.37997    0.19313 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.3722    -2.6884 
U
   0.56778    0.20703 
a_(15|15)
   -4.4228   -0.27267 
diag(V_(15|15))

   0.38916    0.19607 



##########################################
Starting iteration 19 with the following values
a_0
   -3.5463     0.1968 
diag(Q)
  0.098907   0.029043 
Score vector and diagonal of information matrix at time 1 are:
u
     -36.9    -18.076 
U
    5.9792     3.4577 
a_(1|1)
   -4.0904    0.20323 
diag(V_(1|1))

   0.23583    0.36469 
Score vector and diagonal of information matrix at time 2 are:
u
    76.253      53.81 
U
    3.3238     1.9086 
a_(2|2)
    -3.026     0.8693 
diag(V_(2|2))

   0.22462    0.29454 
Score vector and diagonal of information matrix at time 3 are:
u
     88.64     42.243 
U
    11.777     8.2634 
a_(3|3)
   -2.3016     0.7038 
diag(V_(3|3))

   0.12753    0.16249 
Score vector and diagonal of information matrix at time 4 are:
u
   -112.83    -85.377 
U
    13.251     8.1706 
a_(4|4)
   -2.8248    0.36334 
diag(V_(4|4))

  0.089034    0.11438 
Score vector and diagonal of information matrix at time 5 are:
u
   -19.883    -3.3896 
U
    6.0907      3.294 
a_(5|5)
   -3.0319    0.37719 
diag(V_(5|5))

   0.10864    0.11291 
Score vector and diagonal of information matrix at time 6 are:
u
   -34.064    -26.434 
U
    4.1504     2.0358 
a_(6|6)
   -3.4536   0.088397 
diag(V_(6|6))

   0.12758    0.11556 
Score vector and diagonal of information matrix at time 7 are:
u
   -8.8899     7.8072 
U
     2.289     1.0017 
a_(7|7)
   -3.5862    0.16694 
diag(V_(7|7))

    0.1707    0.12853 
Score vector and diagonal of information matrix at time 8 are:
u
    8.2194     3.0406 
U
    1.8924    0.76052 
a_(8|8)
   -3.4119    0.24409 
diag(V_(8|8))

    0.1961    0.13698 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.387    -4.5943 
U
     2.101    0.88822 
a_(9|9)
   -3.6866    0.12545 
diag(V_(9|9))

   0.19055    0.13362 
Score vector and diagonal of information matrix at time 10 are:
u
   -2.7081     7.4397 
U
    1.2791    0.49705 
a_(10|10)
    -3.699    0.21629 
diag(V_(10|10))

   0.23562    0.14724 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.2367     -5.411 
U
     1.166    0.41717 
a_(11|11)
   -3.8035     0.1128 
diag(V_(11|11))

     0.261    0.15605 
Score vector and diagonal of information matrix at time 12 are:
u
    -5.389    -4.4219 
U
   0.95125    0.34078 
a_(12|12)
   -4.0053  -0.015457 
diag(V_(12|12))

   0.29056    0.16539 
Score vector and diagonal of information matrix at time 13 are:
u
   -9.0381    -3.2027 
U
   0.69411    0.24669 
a_(13|13)
   -4.3493   -0.18757 
diag(V_(13|13))

   0.33565    0.17919 
Score vector and diagonal of information matrix at time 14 are:
u
    7.0612    -1.9263 
U
   0.45429     0.1483 
a_(14|14)
   -4.0961    -0.1105 
diag(V_(14|14))

   0.40328    0.20032 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.3422    -2.6682 
U
   0.56546    0.20547 
a_(15|15)
   -4.4413   -0.28687 
diag(V_(15|15))

    0.4095    0.20232 



##########################################
Starting iteration 20 with the following values
a_0
    -3.564    0.18246 
diag(Q)
   0.10606   0.031027 
Score vector and diagonal of information matrix at time 1 are:
u
   -34.967    -16.704 
U
    5.8306     3.3522 
a_(1|1)
   -4.0964    0.19812 
diag(V_(1|1))

   0.23878    0.36977 
Score vector and diagonal of information matrix at time 2 are:
u
    76.636     54.084 
U
    3.2947     1.8879 
a_(2|2)
   -2.9986    0.88785 
diag(V_(2|2))

    0.2283    0.29829 
Score vector and diagonal of information matrix at time 3 are:
u
    82.827      37.66 
U
    12.213      8.607 
a_(3|3)
   -2.3127    0.69702 
diag(V_(3|3))

   0.12658    0.16107 
Score vector and diagonal of information matrix at time 4 are:
u
   -110.64    -83.849 
U
    13.082     8.0524 
a_(4|4)
   -2.8354     0.3553 
diag(V_(4|4))

  0.089688    0.11466 
Score vector and diagonal of information matrix at time 5 are:
u
   -18.792    -2.6586 
U
    6.0073     3.2382 
a_(5|5)
   -3.0363    0.37245 
diag(V_(5|5))

   0.11051    0.11379 
Score vector and diagonal of information matrix at time 6 are:
u
   -33.725     -26.21 
U
    4.1242     2.0185 
a_(6|6)
   -3.4649   0.079303 
diag(V_(6|6))

   0.12992    0.11657 
Score vector and diagonal of information matrix at time 7 are:
u
   -8.4519      8.064 
U
    2.2555    0.98211 
a_(7|7)
   -3.5915    0.16147 
diag(V_(7|7))

   0.17569    0.13038 
Score vector and diagonal of information matrix at time 8 are:
u
    8.3978     3.1434 
U
    1.8788    0.75266 
a_(8|8)
   -3.4077    0.24383 
diag(V_(8|8))

   0.20152    0.13895 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.493    -4.6372 
U
    2.1092    0.89152 
a_(9|9)
   -3.6903    0.12116 
diag(V_(9|9))

   0.19412    0.13489 
Score vector and diagonal of information matrix at time 10 are:
u
   -2.6206     7.4905 
U
    1.2724     0.4932 
a_(10|10)
   -3.6995    0.21405 
diag(V_(10|10))

   0.24218    0.14935 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.2177    -5.3967 
U
    1.1645    0.41607 
a_(11|11)
    -3.807    0.10888 
diag(V_(11|11))

   0.26816    0.15833 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.3304    -4.3903 
U
   0.94674    0.33834 
a_(12|12)
   -4.0134  -0.021828 
diag(V_(12|12))

   0.29922    0.16808 
Score vector and diagonal of information matrix at time 13 are:
u
   -8.9473    -3.1577 
U
   0.68713    0.24323 
a_(13|13)
    -4.366   -0.19849 
diag(V_(13|13))

    0.3472    0.18269 
Score vector and diagonal of information matrix at time 14 are:
u
    7.1784    -1.8745 
U
   0.44531     0.1443 
a_(14|14)
    -4.097   -0.11313 
diag(V_(14|14))

   0.41985    0.20528 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.3293    -2.6591 
U
   0.56447    0.20478 
a_(15|15)
   -4.4536   -0.29555 
diag(V_(15|15))

   0.42351    0.20647 



##########################################
Starting iteration 21 with the following values
a_0
   -3.5762    0.17337 
diag(Q)
    0.1112   0.032429 
Score vector and diagonal of information matrix at time 1 are:
u
   -33.698    -15.815 
U
    5.7329     3.2838 
a_(1|1)
   -4.1005    0.19517 
diag(V_(1|1))

   0.24083     0.3732 
Score vector and diagonal of information matrix at time 2 are:
u
    76.883     54.255 
U
     3.276     1.8749 
a_(2|2)
   -2.9799     0.9004 
diag(V_(2|2))

   0.23083     0.3008 
Score vector and diagonal of information matrix at time 3 are:
u
    78.768     34.454 
U
    12.518     8.8474 
a_(3|3)
   -2.3201    0.69292 
diag(V_(3|3))

   0.12592    0.16008 
Score vector and diagonal of information matrix at time 4 are:
u
   -109.23    -82.877 
U
    12.973     7.9772 
a_(4|4)
   -2.8424    0.35034 
diag(V_(4|4))

   0.09012    0.11482 
Score vector and diagonal of information matrix at time 5 are:
u
   -18.086    -2.1918 
U
    5.9533     3.2026 
a_(5|5)
   -3.0392    0.36968 
diag(V_(5|5))

   0.11178    0.11434 
Score vector and diagonal of information matrix at time 6 are:
u
   -33.512    -26.073 
U
    4.1078     2.0078 
a_(6|6)
   -3.4725   0.073665 
diag(V_(6|6))

   0.13149     0.1172 
Score vector and diagonal of information matrix at time 7 are:
u
   -8.1674     8.2279 
U
    2.2337    0.96965 
a_(7|7)
   -3.5949    0.15825 
diag(V_(7|7))

   0.17914    0.13159 
Score vector and diagonal of information matrix at time 8 are:
u
    8.5104     3.2067 
U
    1.8702    0.74781 
a_(8|8)
   -3.4049    0.24407 
diag(V_(8|8))

   0.20522    0.14022 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.572     -4.672 
U
    2.1152    0.89418 
a_(9|9)
   -3.6929    0.11864 
diag(V_(9|9))

   0.19649    0.13566 
Score vector and diagonal of information matrix at time 10 are:
u
   -2.5633     7.5225 
U
     1.268    0.49076 
a_(10|10)
   -3.6999    0.21287 
diag(V_(10|10))

   0.24669    0.15073 
Score vector and diagonal of information matrix at time 11 are:
u
    -2.206    -5.3886 
U
    1.1636    0.41544 
a_(11|11)
   -3.8094    0.10658 
diag(V_(11|11))

   0.27307    0.15981 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.2915    -4.3701 
U
   0.94374    0.33678 
a_(12|12)
   -4.0189  -0.025764 
diag(V_(12|12))

   0.30519    0.16984 
Score vector and diagonal of information matrix at time 13 are:
u
   -8.8866    -3.1285 
U
   0.68246    0.24097 
a_(13|13)
   -4.3775    -0.2055 
diag(V_(13|13))

   0.35524    0.18502 
Score vector and diagonal of information matrix at time 14 are:
u
    7.2569    -1.8405 
U
   0.43929    0.14169 
a_(14|14)
   -4.0972   -0.11431 
diag(V_(14|14))

    0.4315    0.20864 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.3243    -2.6553 
U
   0.56408    0.20448 
a_(15|15)
    -4.462    -0.3009 
diag(V_(15|15))

   0.43313     0.2092 



##########################################
Starting iteration 22 with the following values
a_0
   -3.5846    0.16752 
diag(Q)
   0.11485   0.033401 
Score vector and diagonal of information matrix at time 1 are:
u
   -32.854     -15.23 
U
     5.668     3.2389 
a_(1|1)
   -4.1034    0.19344 
diag(V_(1|1))

   0.24225    0.37553 
Score vector and diagonal of information matrix at time 2 are:
u
    77.044     54.366 
U
    3.2637     1.8665 
a_(2|2)
   -2.9672    0.90886 
diag(V_(2|2))

   0.23256    0.30248 
Score vector and diagonal of information matrix at time 3 are:
u
    75.959     32.236 
U
    12.729     9.0137 
a_(3|3)
    -2.325    0.69035 
diag(V_(3|3))

   0.12546     0.1594 
Score vector and diagonal of information matrix at time 4 are:
u
   -108.32     -82.25 
U
    12.902     7.9287 
a_(4|4)
   -2.8472    0.34722 
diag(V_(4|4))

  0.090407     0.1149 
Score vector and diagonal of information matrix at time 5 are:
u
   -17.624     -1.889 
U
     5.918     3.1795 
a_(5|5)
   -3.0412    0.36801 
diag(V_(5|5))

   0.11264    0.11469 
Score vector and diagonal of information matrix at time 6 are:
u
   -33.375    -25.986 
U
    4.0972     2.0011 
a_(6|6)
   -3.4776   0.070121 
diag(V_(6|6))

   0.13256     0.1176 
Score vector and diagonal of information matrix at time 7 are:
u
   -7.9797     8.3344 
U
    2.2193    0.96154 
a_(7|7)
   -3.5972    0.15633 
diag(V_(7|7))

   0.18151    0.13238 
Score vector and diagonal of information matrix at time 8 are:
u
    8.5838     3.2468 
U
    1.8646    0.74474 
a_(8|8)
   -3.4029    0.24445 
diag(V_(8|8))

   0.20775    0.14104 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.628    -4.6981 
U
    2.1195    0.89619 
a_(9|9)
   -3.6947    0.11715 
diag(V_(9|9))

   0.19809    0.13614 
Score vector and diagonal of information matrix at time 10 are:
u
    -2.525     7.5432 
U
    1.2651    0.48919 
a_(10|10)
   -3.7002    0.21225 
diag(V_(10|10))

   0.24981    0.15162 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.1982    -5.3837 
U
     1.163    0.41506 
a_(11|11)
   -3.8111    0.10523 
diag(V_(11|11))

   0.27645    0.16076 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.2653    -4.3571 
U
   0.94172    0.33577 
a_(12|12)
   -4.0228  -0.028203 
diag(V_(12|12))

   0.30931    0.17098 
Score vector and diagonal of information matrix at time 13 are:
u
   -8.8458    -3.1093 
U
   0.67932    0.23949 
a_(13|13)
   -4.3854      -0.21 
diag(V_(13|13))

   0.36082    0.18656 
Score vector and diagonal of information matrix at time 14 are:
u
    7.3098     -1.818 
U
   0.43524    0.13995 
a_(14|14)
   -4.0973   -0.11476 
diag(V_(14|14))

   0.43964    0.21088 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.3227     -2.654 
U
   0.56396    0.20438 
a_(15|15)
   -4.4677   -0.30419 
diag(V_(15|15))

   0.43973    0.21097 



##########################################
Starting iteration 23 with the following values
a_0
   -3.5904    0.16372 
diag(Q)
   0.11741   0.034062 
Score vector and diagonal of information matrix at time 1 are:
u
   -32.288    -14.841 
U
    5.6245     3.2089 
a_(1|1)
   -4.1053     0.1924 
diag(V_(1|1))

   0.24323    0.37711 
Score vector and diagonal of information matrix at time 2 are:
u
    77.152     54.438 
U
    3.2556      1.861 
a_(2|2)
   -2.9585    0.91454 
diag(V_(2|2))

   0.23376    0.30361 
Score vector and diagonal of information matrix at time 3 are:
u
    74.026     30.711 
U
    12.875     9.1281 
a_(3|3)
   -2.3282     0.6887 
diag(V_(3|3))

   0.12515    0.15894 
Score vector and diagonal of information matrix at time 4 are:
u
   -107.72    -81.839 
U
    12.856     7.8969 
a_(4|4)
   -2.8504    0.34522 
diag(V_(4|4))

    0.0906    0.11495 
Score vector and diagonal of information matrix at time 5 are:
u
   -17.318    -1.6902 
U
    5.8946     3.1644 
a_(5|5)
   -3.0425    0.36698 
diag(V_(5|5))

   0.11323    0.11491 
Score vector and diagonal of information matrix at time 6 are:
u
   -33.285     -25.93 
U
    4.0903     1.9968 
a_(6|6)
   -3.4811    0.06788 
diag(V_(6|6))

    0.1333    0.11784 
Score vector and diagonal of information matrix at time 7 are:
u
   -7.8545     8.4044 
U
    2.2097    0.95622 
a_(7|7)
   -3.5988    0.15517 
diag(V_(7|7))

   0.18315    0.13289 
Score vector and diagonal of information matrix at time 8 are:
u
    8.6327     3.2729 
U
    1.8608    0.74275 
a_(8|8)
   -3.4016    0.24482 
diag(V_(8|8))

    0.2095    0.14156 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.668    -4.7172 
U
    2.1226    0.89766 
a_(9|9)
    -3.696    0.11626 
diag(V_(9|9))

   0.19919    0.13642 
Score vector and diagonal of information matrix at time 10 are:
u
    -2.499     7.5567 
U
    1.2631    0.48817 
a_(10|10)
   -3.7005    0.21193 
diag(V_(10|10))

   0.25197    0.15218 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.1925    -5.3806 
U
    1.1626    0.41483 
a_(11|11)
   -3.8123    0.10444 
diag(V_(11|11))

   0.27878    0.16136 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.2475    -4.3486 
U
   0.94035    0.33512 
a_(12|12)
   -4.0254  -0.029697 
diag(V_(12|12))

   0.31217    0.17171 
Score vector and diagonal of information matrix at time 13 are:
u
   -8.8181    -3.0966 
U
   0.67719    0.23851 
a_(13|13)
   -4.3909   -0.21286 
diag(V_(13|13))

   0.36469    0.18755 
Score vector and diagonal of information matrix at time 14 are:
u
    7.3458     -1.803 
U
   0.43248     0.1388 
a_(14|14)
   -4.0973   -0.11482 
diag(V_(14|14))

   0.44531    0.21235 
Score vector and diagonal of information matrix at time 15 are:
u
   -7.3225    -2.6538 
U
   0.56395    0.20437 
a_(15|15)
   -4.4716   -0.30618 
diag(V_(15|15))

   0.44427    0.21209 



##########################################
Starting iteration 24 with the following values
a_0
   -3.5944    0.16123 
diag(Q)
    0.1192   0.034503 
Score vector and diagonal of information matrix at time 1 are:
u
   -31.905     -14.58 
U
    5.5951     3.1889 
a_(1|1)
   -4.1067    0.19177 
diag(V_(1|1))

   0.24391    0.37818 
Score vector and diagonal of information matrix at time 2 are:
u
    77.224     54.486 
U
    3.2501     1.8574 
a_(2|2)
   -2.9525    0.91832 
diag(V_(2|2))

    0.2346    0.30437 
Score vector and diagonal of information matrix at time 3 are:
u
    72.701     29.667 
U
    12.974     9.2064 
a_(3|3)
   -2.3304    0.68759 
diag(V_(3|3))

   0.12494    0.15862 
Score vector and diagonal of information matrix at time 4 are:
u
   -107.32    -81.567 
U
    12.825     7.8758 
a_(4|4)
   -2.8525    0.34392 
diag(V_(4|4))

  0.090731    0.11498 
Score vector and diagonal of information matrix at time 5 are:
u
   -17.114    -1.5583 
U
     5.879     3.1543 
a_(5|5)
   -3.0434    0.36633 
diag(V_(5|5))

   0.11364    0.11505 
Score vector and diagonal of information matrix at time 6 are:
u
   -33.225    -25.893 
U
    4.0856     1.9939 
a_(6|6)
   -3.4835   0.066468 
diag(V_(6|6))

    0.1338    0.11799 
Score vector and diagonal of information matrix at time 7 are:
u
   -7.7702     8.4508 
U
    2.2033    0.95269 
a_(7|7)
      -3.6    0.15447 
diag(V_(7|7))

   0.18429     0.1332 
Score vector and diagonal of information matrix at time 8 are:
u
    8.6661     3.2901 
U
    1.8582    0.74143 
a_(8|8)
   -3.4007    0.24514 
diag(V_(8|8))

   0.21071    0.14188 
Score vector and diagonal of information matrix at time 9 are:
u
   -13.695    -4.7308 
U
    2.1247     0.8987 
a_(9|9)
   -3.6969    0.11576 
diag(V_(9|9))

   0.19995    0.13658 
Score vector and diagonal of information matrix at time 10 are:
u
   -2.4813     7.5656 
U
    1.2618     0.4875 
a_(10|10)
   -3.7007    0.21178 
diag(V_(10|10))

   0.25347    0.15252 
Score vector and diagonal of information matrix at time 11 are:
u
   -2.1882    -5.3786 
U
    1.1622    0.41467 
a_(11|11)
   -3.8132      0.104 
diag(V_(11|11))

   0.28041    0.16173 
Score vector and diagonal of information matrix at time 12 are:
u
   -5.2353    -4.3431 
U
   0.93942     0.3347 
a_(12|12)
   -4.0273  -0.030582 
diag(V_(12|12))

   0.31416    0.17216 
Score vector and diagonal of information matrix at time 13 are:
u
   -8.7992    -3.0882 
U
   0.67574    0.23786 
a_(13|13)
   -4.3948   -0.21465 
diag(V_(13|13))

   0.36739    0.18817 
Score vector and diagonal of information matrix at time 14 are:
u
    7.3703     -1.793 
U
    0.4306    0.13803 
a_(14|14)
   -4.0973   -0.11469 
diag(V_(14|14))

   0.44926    0.21329 
Score vector and diagonal of information matrix at time 15 are:
u
    -7.323    -2.6542 
U
   0.56398     0.2044 
a_(15|15)
   -4.4743   -0.30733 
diag(V_(15|15))

   0.44742    0.21278 
```

## EKF
The EKF method for the exponential model has shown to have issues with divergence. The cause seem to be overstepping in the correction step. Hence, we can set the learning rate below 1. For example, you can do so by passing `list(LR = .5, ...)` to the `control` argument of `ddhazard` to gain a learning rate of $0.5$

The direct formulas for the conditional mean, conditional variance and derivatives of the conditional mean are numerically unstable for certain combinations of interval lengths ($t_{i,s} - t_{i, s - 1}$) and the linear predictor ($\eta = \vec{x}_{is}^T \vec{\alpha}_{\Lceil{t_{i,s}}}$). In particular they are subject to cancellation. This can cause both high relative errors and even wrong sign in some of the cases. Thus, Lauren series or Tylor series are used for critical points. This requires no action for user but is stressed for readers who worry after seeing both the conditional mean and conditional variance of $\Delta_{i,s}$

## UKF
TODO: WRITE ONCE QUESTION ABOUT CORRELATION IS SOLVED

## Fixed effects
Fixed effects in the M-step are estimated using a Poisson model with an offset equal to the logarithm of the time observed in each bin plus the estimated offset from time varying effects. That is, we use that if an arrival time $T$ is exponential distributed with rate $\lambda$ then having an outcome at at time $t$ is Poisson distributed $Y\sim\text{Poisson}(\lambda t)$. For example, say that we fit the following model:

```{r}
# The data we use
head(data_frame)
```

```{r, eval=FALSE}
# The fit
fit <- ddhazard(Surv(tstart, tstop, y) ~ x1 + ddFixed(x2), data_frame, 
                by = 1, # bin lengths are 1
                id = data_frame$id, model = "exponential")
```

Take the individual with  `id = 1`. As in the logistic model, he will yield four observations in the M-step. Each will have an offset of $\log (1) = 0$ because the interval length is $1$ plus $\emNotee{\vec{a}}{t}{d}$ times the value of `x1`. Say instead that the data frame was:

```{r, echo=FALSE}
data_frame_new <- data_frame
data_frame_new[1, 2:3] <- c(.5, 2)
data_frame_new <- rbind(c(1, 0, .5, 0, .43, .33),
                        data_frame_new)
```

```{r}
head(data_frame_new)
```

Then individual 1 would yield five observation. The first row would only have an offset $\log 0.5$ plus $\emNotee{\vec{a}}{1}{d}$ times `r data_frame_new$x1[1]`. The second row will yield two observations: one with an offset of $\log 0.5$ plus $\emNotee{\vec{a}}{1}{d}$ times `r data_frame_new$x1[2]` and the other with an offset $\log 1$ plus $\emNotee{\vec{a}}{2}{d}$ times `r data_frame_new$x1[2]`. Note that this is not the case with the logistic model as we have the same binning issue as described in the Binning section

## Starting value
QUESTION: Should I write about the static model that can be used for $\vec{\alpha}_0$?

# Further tasks and ideas
The last section will cover further task and ideas. Please, let me know what you think. Is it relevant, got ideas to the question I pose and how would you priorities?

## Comparisson of other package
Look further at the vignette I have made where I compare with other packages. The goal of the vignette is to: 

1. Show how the API for package works
2. Show how these modal compares with other models
3. Illustrate the pros and cons of the model I have implemented compared to other models 

## Confidence bounds
How do we construct confidence bounds both for the state vectors and for the predicted values?

## Diagnostics
I am thinking of making a second vignettes with diagnostic. It will contain raw residuals, Pearson residuals, non-standardized and standardized state space error. The motivation to keep it in a separate vignette is to include more examples and keep it as more of an example of how to use the package

Further, I need to look into tests the effects are time varying or not. One idea is to test entries in $\mat{Q}$. Though, this involves tests on the boundary of the parameter space. Another idea is to make an F-test if we can estimate a model where only some of the parameters are fixed. This thread here suggest the idea when make all the parameter time invariant [http://stats.stackexchange.com/a/161917](http://stats.stackexchange.com/a/161917)

## Other state equations
We can replace the state equation with other models then a given order random walk. For example, we can replace it with a stationary process: 
$$\vec{\alpha}_t = \vec{\mu} + \mat{F} \vec{\alpha}_{t - 1} + \mat{R}\vec{\eta}_t$$

where we require $\mat{F}$ is such that the process is stable. $\mat{F}$ and $\vec{\mu}$ can be estimated in the M-step with closed form solutions when the noise is Gaussian. Both the UKF end EKF can be adapted to this setting. Further, we can change the distribution of $\vec{\eta}$ or change make a non-linear dependence between  $\vec{\alpha}_t$ and $\vec{\alpha}_{t - 1}$

# References

