---
title: "Applying dynamichazard to survival::cancer"
output: pdf_document
bibliography: pbc_data.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

  ## Set the width of the R-terminal to # characters
  options(width = 80, warn = -1)

# Derfine function to make small margin in pictures
# you can use the bool flag to set what to excute before, after chunk 
# or always
# If you do not whant to excute the code than do e.g. small.mar = F
knitr::knit_hooks$set(small.mar = function(before, options, envir){
  if (before){ par(
    mar = c(5,5,0.5,0.5), 
    tcl = -0.3, 
    mgp = c(2.5,.5,0), 
    oma = c(0,0,0,0),
    pch=16
    # cex=.6,
    # cex.axis = 1,
    # cex.lab = .8/.6,
    # lwd= 1
  )}},
  my.options=function(before, options, envir){
    if(before){
      options(digits = 3)
    }})

## opts_chunk$set() can change the default global options in a document (e.g. put this in a code chunk
knitr::opts_chunk$set(fig.path='figures/',
               fig.align='center',
               fig.width=6, fig.height=4,
               out.width="0.7\\textwidth",
               size='tiny',                ## See R highligt package for possible values (https://cran.r-project.org/web/packages/highlight/highlight.pdf)
               small.mar = TRUE,
               my.options = TRUE,
               comment = "##", 
               warnings = T,
               errors = T
)
```

# Summary
This note will compare the package `dynamichazard` I am working on with current methods available in `R`. First, we will look at other packages in `R` that have time varying effects in multiplicative hazard models. Then we will turn to the methods I have implemented. Lastly, future developments are suggested

In short the advantages of the state space model and methods for estimation I have implemented are: 

* Few decision regarding the smoothing. Only the length intervals are needed
* Has linear time complexity in the number observations 
* Most time consuming parts of computation can done in parallel
* A parametric model is provided for state equation. Thus, extrapolation beyond the last time period is possible. This is useful for instance when working with calender time

# survival::cancer
This section will use the data set `survival::cancer` to compare the methods. The data set consists of lung cancer patients where some die after `time` column variable while others are right censored. Dying is coded through `status` with `1` implying right censored and `2` implying a death. For more information on the dataset see `?cancer`. We will focus on the `age` and `sex` variable in the data set. We start by attaching the `survival` package and standardizing the `age` variable. The later is important when we the use the Unscented Kalman filter (UKF)

```{r}
library(survival)
head(cancer)

# Standardize
cancer$age <- (cancer$age - mean(cancer$age)) / sd(cancer$age)

# Add id to keep track later
cancer$id <- seq_len(nrow(cancer))

c("Number of patients" = nrow(cancer), 
  "Number of deaths" = sum(cancer$status == 2))
```

## mgcv
The first method we will compare with is Generalized Additive Models (GAM) by using the `gam` function in the `mgcv` package. The model we fit is of the form: 

$$\text{logit}(\pi_i) = \vec{\gamma}_{\text{time}}\vec{f}_{\text{time}}(t_i)
    + \vec{\gamma}_{\text{age}}\vec{f}_{\text{time}}(t_i)a_i
    + \vec{\gamma}_{\text{sex}}\vec{f}_{\text{sex}}(t_i)s_i$$
    
where $\pi_i$ is the probability that the $i$'th individual dies of cancer, $t_i$ is the stop time of the $i$'th individual, $a_i$ is the age of the $i$'th individual and $s_i$ is the sex of the $i$'th individual. $\vec{f}_{\cdot}$ is a basis function. We will use cubic regression splines with knots spread evenly through the covariate values. We fit the model with the following call

```{r message=FALSE}
library(mgcv, quietly = T)
spline_fit <- gam(
  formula = status == 2 ~ -1 + s(time, bs = "cr", k = 6) + # cr yields cubic ba-
                                                           # sis with dim of k
    s(time, bs = "cr", k = 6, by = age) + 
    s(time, bs = "cr", k = 6, by = sex),
  family = binomial, data = cancer,
  method = "GCV.Cp" # estimate smoothing parameters with generalized cross vali-
                    # dation  
                  )

plot(spline_fit, xlim = c(0, 800), ylim = c(-1.5, 2), rug = F, 
     pages = 1)
```

The plot in the upper right corner is the intercept coefficient. The lower right corner is the coefficient of `sex` and the upper left corner is the coefficient of `age`. The parameters in the sets $\vec{\gamma}_{\text{time}}$, $\vec{\gamma}_{\text{age}}$ and $\vec{\gamma}_{\text{sex}}$ are penalized with a smoothing parameter selected with generalized cross validation. The final plot of the estimates do suggest that there may be curvature in `sex` variable. The estimate for the `sex` variable seems reasonable given that men have higher propensity to die early from cancer then women as illustrated in the cross table below:

```{r}
# sex = 1 is male
# First we look at the proportion that die before time 200
ftable(xtabs(~ I(time <= 200) + I(status == 2) + sex, data = cancer), 
       row.vars = 1:2)
# Then we look at the proportion that die before time 800
ftable(xtabs(~ I(time <= 800) + I(status == 2) + sex, data = cancer), 
       row.vars = 1:2)
```

The model does not take into account that the people who survive to time say $800$ also survive at time $200$. For instance, if we only had right censoring after time $600$ then the above model would have no controls before time $600$. Right censoring do occur through the time period we look at. Thus, the above model is chosen to keep the model setup simple

##timereg  
Another method we can try is a time varying effects cox model from the package `timereg` based on the book 'Dynamic Regression Models for Survival Data'. The model we fit has an instantaneous hazard $\lambda(t)$ given by:

$$\lambda(t) = \lambda_0(t) \exp \left( \vec{x}\vec{\beta}(t) \right)$$

where each margin of $\vec{\beta}(t)$ is estimated with method we describe shortly. Below we plot the cumulative regression function $B_i(t) = \int_0^t \beta_i(s)ds$ after estimating the model with the `timecox` function:

```{r}
library(timereg)
library(survival)
arg_list <- list( # We will re-use these parameters later
  formula = Surv(rep(0, nrow(cancer)), time, status == 2) ~ age + sex,
  max_T = 800, data = cancer)

# The Breslow estimate of baseline is not available at this point
tryCatch({
  cox_fit <- timecox(arg_list$formula, data = arg_list$data, max.time = arg_list$max_T, 
                   method = "breslow")
}, error = function(e) cat(e$message))

cox_fit <- timecox(arg_list$formula, data = arg_list$data, max.time = arg_list$max_T, 
                   method = "basic")

par(mfcol = c(2, 2), mar = c(5,5,3,0.5))
plot(cox_fit, ylab = "Cum coef")
```

The above code first shows that non-parametric `breslow` estimate is currently not supported. Instead the baseline is $\lambda_0(t)=\exp(\alpha_0(t))$ where $\alpha_0(t)$ is estimated in a similar to way to $\vec{\beta}(t)$. $\vec{\beta}(t)$ is estimated recursively with an update equation that is simplified through a first order Taylor expansion and adding a smoothness through weighting the time changes with a uniform continuous kernel. For details see 'Martinussen, Torben, and Thomas H. Scheike. *Dynamic regression models for survival data*. Springer Science & Business Media, 2007.'

Notice that the cumulative coefficient for the `intercept` and `age` seems close to linear while we do have curvature in the `sex` coefficient. This is consistent with curvature we found with our logistic fit using the `mgcv` 

## Other packages
For completeness, there are many other packages that estimate cox regression and GAM models with time varying effects. For example, we could have used `mgcv::cox.ph` to compare with only one package. The `timereg` was used instead to illustrate two different packages

## dynamichazard
This section will show results for the logistic model with time varying effect that I have implemented. Estimates are shown both with the Extended Kalman filter (EKF) and the Unscented Kalman filter (UKF). First, we will briefly cover the model and then we will return to the two estimations methods

The idea is that we discretize the outcomes into $1,2,\dots,T$ bins (or intervals if you prefer this notion). In each bin, we observe whether the individual dies or is right censored. The state space model we are applying is of the form: 
$$\begin{array}{ll}
    	\vec{y}_t = \vec{z}_t(\vec{\alpha}_t) + \vec{\epsilon}_t \qquad & 
         	\vec{\epsilon}_t \sim (\vec{0}, \mathbf{H}_t(\vec{\alpha}_t))  \\
    	\vec{\alpha}_{t + 1} = \mathbf{F}\vec{\alpha}_t + \mathbf{R}\vec{\eta}_t \qquad & 
         	\vec{\eta}_t \sim N(\vec{0}, \mathbf{Q}) \\
	\end{array}
	, \qquad t = 1,\dots, n$$
	
where $y_{it} \in \{0,1\}$ is an indicator for whether the $i$'th individual dies at interval $t$. $\cdots \sim (a, b)$ denotes a random variable with mean (or mean vector) $a$ and variance (or co-variance matrix) $b$. It needs not to be normally distributed. $z_{it}(\vec{\alpha}_t) = h(\vec{\alpha}_t\vec{x}_{it})$ is the non-linear map from state space variables to mean outcomes where $h$ is the link function. We use the logit model model in this example. The current implementation supports $\mathbf{F}$ and $\mathbf{R}$ such that we have first and second order random walk for the state equation

Firstly, we need to estimate the starting value $\vec{\alpha}_0$ and co-variance matrix $\mathbf{Q}$. This is done through an EM-algorithm. The E-step either use the EKF or UKF. Either methods yields smoothed estimates of $\vec{\alpha}_1,\dots, \vec{\alpha}_T$, smoothed co-variance matrix and smoothed correlation that we need for the M-step. At this point, $\mathbf{Q}_0 = \kappa \mathbf{I}$ is fixed to a large value $\kappa$


### Extended Kalman filter
The idea behind the EKF is to linearize $\vec{z}_t(\vec{\alpha}_t)$ through a first order Taylor expansion and apply the regular Kalman filter to the linearized model.  The implemented version uses the method described in 'Fahrmeir, Ludwig. *Dynamic modelling and penalized likelihood estimation for discrete time survival data*. Biometrika 81.2 (1994): 317-330.' 

Fahrmeier applies the the Woodbury matrix identity to re-write the filter step of the Kalman filter to gain a method that is linear in time complexity of the dimension of the observational equation. In contrast the orginal formulation is cubic. Furhter, the filter step can be carried out in parallel which makes the method more applicable to large data sets. We will return to this later

We can fit the model with the following call to `ddhazard`:

```{r}
# # Download the version the code is run with
# devtools::install_github(
#   "boennecd/dynamichazard@1a96c837c822c4258751c26c88e151949691a55b")

library(dynamichazard)
arg_list <- c(arg_list, list( 
  by = 100,
  Q_0 = diag(rep(1, 3)), est_Q_0 = F,
  verbose = F))
dd_fit_EKF <- do.call(ddhazard, arg_list)

par(mfcol = c(2, 2))
for(i in 1:3)
  plot(dd_fit_EKF, cov_index = i, type = "cov")
```

The method set $\vec{\alpha}_0$ equal to the estimate from one iteration of the iteratively reweighed least squares model with time in-variant coefficients. Each observations is weighted according to the number intervals they are in (we will cover more details in a bit). $\mathbf{Q}_0$ is set to diagonal matrix with (`r diag(arg_list$Q_0)`). Each time interval have length `r arg_list$by` and the maximum time $T$ is set to `r arg_list$max_T`. Thus, the bins are $(0, 100], (100, 200], \dots, (700, 800]$

The plots shows estimates of coefficients $\vec{\alpha}_1,\dots,\vec{\alpha}_T$. The confidence bounds are pointwise estimates using the smoothed co-variance matrix $\text{Var}\left(\left.\alpha_t \right|\vec{y}_1,\dots,\vec{y}_T\right)$. We notice that the `sex` coefficient seems to have an upward slope

### Uncented Kalman Filter
The idea behind UKF is select points from the state equation $\vec{\alpha}_t$ and use these to approximate the distribution of observed outcomes $\vec{y}_t$. To be more concrete, let $m$ denote the dimension of the state equation. Then we select $2m + 1$ sigma points denoted by $\vec{\mathcal{X}}_{0,t}, \vec{\mathcal{X}}_{1,t}, \dots, \vec{\mathcal{X}}_{2m,t}$. Each sigma point have a sigma weight of $W_i$ where $W_0 = \lambda / (m + \lambda)$ and $W_1 =  \dots = W_{2m} = 1 / (2(m + \lambda))$ ($\lambda$ will be specified shortly). The sigma points are computed in each iteration of the UKF by:
$$\begin{array}{c}
  \vec{a}_{t|t} = E\left(\left. \vec{\alpha}_t\right|\vec{y}_{1},\dots,\vec{y}_t\right),
    \qquad \mathbf{V}_{t|t} = \text{Var}\left(\left. \vec{\alpha}_t\right|\vec{y}_{1},\dots,\vec{y}_t\right)  \\
  \vec{\mathcal{X}}_{0,t} = \vec{a}_{t|t} \\
  \vec{\mathcal{X}}_{1,t} = \vec{a}_{t|t} + \sqrt{\lambda + m}\left(\sqrt{\mathbf{V}_{t|t}}\right )_1 \\
  \vec{\mathcal{X}}_{2,t} = \vec{a}_{t|t} + \sqrt{\lambda + m}\left(\sqrt{\mathbf{V}_{t|t}}\right )_2 \\
  \vdots \\
  \vec{\mathcal{X}}_{1 + m,t} = \vec{a}_{t|t} - \sqrt{\lambda + m}\left(\sqrt{\mathbf{V}_{t|t}}\right )_1 \\
  \vdots
\end{array}$$

where $\left(\sqrt{\mathbf{V}_{t|t}}\right )_i$ is the $i$'th column of lower tringular matrix from the Cholesky decomposition of $\mathbf{V}_{t|t}$. $\lambda$ is set such that $\lambda = a(m + \kappa) - m$ where $a\in (0,1]$ controls the spread of the sigma point (usually set to 1) and $\kappa$ (typically set to $0$ or $3 - m$). After we have computed the sigma point, we compute the outcomes $\vec{\mathcal{Y}}_{i,t} = \vec{z}_t(\vec{\mathcal{X}}_{i,t})$ and use the pairs of $(\vec{\mathcal{Y}}_{i,t},\vec{\mathcal{X}}_{i,t})$ and sigma weights to compute the means, co-variance and correlation matrices needed for the Kalman filter

The motivation to use the UKF is that there are suggestions that the UKF matches the moments better that the EKF when the linear approximation for the observational equation is poor. Two useful reference for UKF are 'Julier, Simon J., and Jeffrey K. Uhlmann. *Unscented filtering and nonlinear estimation*. Proceedings of the IEEE 92.3 (2004): 401-422.' and 'Julier, Simon J., and Jeffrey K. Uhlmann. *New extension of the Kalman filter to nonlinear systems*. AeroSense'97. International Society for Optics and Photonics, 1997.'

The definition for $\lambda$ differs from these articles but yields the same results when the theoretical optimal values are used. The details are omitted here to keep focus on the general idea of the UKF. Lastly, the implementation differs from all descriptions I have seen so far by applying the the Woodbury matrix identity to get an algorithm that is linear in the number of observations. We will return to this later

We call `ddhazard` below to estimate with the UKF method. We plot the co-variate estimates after the estimation

```{r}
arg_list$method <- "UKF"
arg_list$kappa <- 3 - 3
arg_list$Q_0 <- diag(rep(1, 3))
arg_list$Q <- diag(rep(1e-2, 3))

dd_fit_UKF <- do.call(ddhazard, arg_list)

par(mfcol = c(2, 2))
for(i in 1:3)
  plot(dd_fit_UKF, cov_index = i, type = "cov")
```

We set the initial $\mathbf{Q}$ to a diagonal matrix with elements c(`r diag(arg_list$Q)`) to avoid issues with the Cholesky decomposition in the first iteration. We find similar estimates as before though the confidence bounds are wider

### Regular glm 
Another idea is to fit a model with time in-variant effects. A straight forward model would be to to include each row in the data set with a weight as one. This is done below:

```{r}
glm_args <- list(
  formula = status == 2 & time <= arg_list$max_T ~ age + sex, 
  family = "binomial", data = cancer) 
glm_fit <- do.call(glm, glm_args)

summary(glm_fit)$coefficients
```

However, we may want to take into account that some individuals survives for longer time than others. For this reason I have made the `static_glm` function. It adds weights to each observation according to how many intervals the observations is in. That is, how many period they survive. Say for instance that we have an individual who dies of cancer at time $750$ were we make intervals of length 100. This observation will have two rows in the final model: 1) which it is a control with a weight of 7 (he survived up to time $700$) and 2) one where it is a case with a weight of 1 (we observe that he dies at in interval $(700,800]$). The call to the function is made below:

```{r}
glm_args$formula <- arg_list$formula
glm_args$by <- arg_list$by
glm_args$max_T <- arg_list$max_T

glm_fit_new <- do.call(static_glm, glm_args)

summary(glm_fit_new)$coefficients
```

The method requires the same `survival::Surv` on the right hand site of the `~` in the formula as the `ddhazard` function. Note that the coefficient are similar to the estimates from `ddhazard` as expected

We can illustrate the difference between the two `glm` models by noting that the number of additional rows is equal to the number of deaths that occur after `arg_list$by`: 

```{r}
# There is an added row for every case that has an event beyond the first interval 
nrow(glm_fit_new$data) # number of rows from static_glm
nrow(glm_fit$data) # number of rows from glm

sum(glm_fit$data$time <= arg_list$max_T & # before max_T
      glm_fit$data$time > arg_list$by & # after first interval
      glm_fit$data$status == 2) # observed to die
```

The additional rows has the same co-variate values but the outcome variable `Y` differs as illustrated below:

```{r}
head(glm_fit$data[, c("status", "time", "sex", "age")]) # data set from glm

# The weight vector count the number of bins
glm_fit_new$data[glm_fit_new$data$id %in% glm_fit$data$id[1:5],
                 c("Y", "weights", "sex", "age")]
```

As a last note, it is one iteration from the iterated weighted least square from the `static_glm` that is used when the initial $\vec{\alpha}_0$ is not supplied to `ddhazard`

### Comparing all the fits from dynamichazard

We can compare all the estimates from this package in a final plot. The code below produces the plot. The black line is the estimates from the `static_glm` fit, the blue line is the estimates from EKF fit and the red line is the estimate from the UKF fit:

```{r}
par(mfcol = c(2, 2))
# par(cex.axis = par()$cex.axis * 1.5, cex.lab = par()$cex.lab * 1.5)
for(i in 1:3){
  # glm estimates
  est_glm <- glm_fit_new$coefficients[i]
  
  # empty plot
  plot(c(0, arg_list$max_T), range(c(
    est_glm,
    dd_fit_EKF$a_t_d_s[, i] +
      sqrt(dd_fit_EKF$V_t_d_s[i, i,]) %*% t(c(1.96, -1.96)),
    dd_fit_UKF$a_t_d_s[, i] +
      sqrt(dd_fit_UKF$V_t_d_s[i, i,]) %*% t(c(1.96, -1.96)))),
    type = "n", xlab = "time", ylab = names(est_glm))
  
  # Add estimates from static_glm
  abline(h = est_glm, lty = 1, col = "black")
  
  # Add estimates from EKF
  plot(dd_fit_EKF, cov_index = i, type = "cov", add = T, col = "blue")
  
  # Add estimates from UKF
  plot(dd_fit_UKF, cov_index = i, type = "cov", add = T, col = "red")
}
```

We observe that the EKF fit and UKF fit are not far from each other. Though, the confidence bounds are wider for the UKF method. Moreover, both estimates are not far from the `static_glm` fit

# dynamichazard implementation
The estimation in `ddhazard` is carried out using compiled `c++` code. The linear algebra library `Armadillo` is used for the computation. `Armadillo` provides an API for `LAPACK` and `BLAS` which means that an CPU optimized version of the two can decrease the computation time

Further, the EKF implementation use the formulation from Fahrmier (1994) as previously mentioned. The `std` library `thread` is used to compute the filter step in parallel. The implementation do take into account that a multithreaded `OpenBLAS` version may have been used to compile the code. Thus, `openblas_set_num_threads` is used to tootle the number of threads `BLAS` will use during and after the filter step

The UKF method is not computed in parallel at this point. Though, parts of the matrix operations will be computed in parallel if a multithreaded `BLAS` is used. 

## Simulation
We will simulate a series of co-variates and individuals in order to show 1) the performance and 2) that the EKF and UKF has a linear time complexity in the number of observations. We will use the function `test_sim_func_logit` in the `R/test_utils.R` file. First, we source the file and then we print the function. You can skip the function definition if you like and go to the explanation that comes after the print:

```{r}
# We are currenlty in the vignettes folder
gsub("(^.*)(dynamichazard.*)", ".../\\2", getwd())
source("../R./test_utils.R")
test_sim_func_logit
```

The function start by simulating $\vec{ \beta }_t = \vec{\beta}_{t-1} + \vec{\eta}_t$ where $\vec{\eta}_t \sim N(\vec{0}, \mathbf{Q})$. We then model the death of individual $i$ in period $t$ by $\pi_{it} = \exp (\vec{\beta}^T \vec{x}_{it}) / (1 + \exp (\vec{\beta}^T \vec{x}_{it}))$. We update the co-variate vector for the $i$'th individual with gabs of $1 + z$ where $z \sim \text{Exp}(1)$. We let $x_{itk} \sim \text{Unif}(a,b)$ for given values $a$ and $b$ (if the co-variates are updated in period $t$ for the $i$'th individual). Below we define a list of arguments for `test_sim_func_logit` and simulate the series:

```{r}
set.seed(20160921) # a resonably productive day
beta_start <- 1
n_vars <- 3
sims_args <- list(n_series = (n_series <- 1e4), 
                  n_vars = n_vars, 
                  t_0 = (t_0 <- 0), 
                  t_max = (t_max <- 10),
                  x_range = (x_range <- 1), 
                  x_mean = (x_mean <- 0), 
                  beta_start = beta_start,
                  intercept_start = (intercept_start <- -3),
                  re_draw = T,
                  sds = (sds <- c(sqrt(.2), rep(1, n_vars))))

sims <- do.call(test_sim_func_logit, sims_args)
```

We have a total of `r n_series` series, with `r n_vars` parameters and an intercept. The simulated parameters are plotted below. The black line is the intercept:
```{r}
matplot(seq_len(nrow(sims$betas)) - 1, sims$betas, type = "l", lty = 1, 
        lwd = c(2, rep(1, n_vars)), xlab = "time", ylab = "True beta")
```

Further, we start at time `r t_0` and end at time `r t_max` giving us `r t_max - t_0` intervals. The $\vec{\beta}_0$ starts at (`r c(intercept_start, rep(beta_start, n_vars))`). We set the variances of state space variables to [`r sds`]. The variance of the intercept is lower to ensure that it does not wonder of too much. Thus, we end with a lower base line risk of dying with greater certainty Lastly, the co-variates are simulated to be uniformly distributed within [`r x_mean + c(-1, 1) * x_range / 2`]. The first 10 rows of the final data frame and number of deaths are printed below

```{r}
head(sims$res, 10)
sum(sims$res$event) # number of indvidiauls who dies
```

We can now fit the model with `ddhazard` using the EKF method. This is done below. `system.time` is used to show the computation time

```{r}
arg_list <- list(
  formula = Surv(tstart, tstop, event) ~ 
           . - tstart - tstop - event - id,
  data = sims$res, 
  Q_0 = diag(rep(1, 4)),
  Q = diag(rep(.001, 4)),
  n_max = 1e3,
  id = sims$res$id,
  method = "EKF",
  max_T = 10, 
  by = 1)

system.time(fit_EKF <- do.call(ddhazard, arg_list))
```

We can plot the true parameters and estimated parameters to get an idea of the fit. This is done below where the continuous lines are true parameters and dashed lines are estimates

```{r}
matplot(seq_len(nrow(sims$betas)) - 1, sims$betas, type = "l", lty = 1, 
        lwd = c(2, rep(1, n_vars)), xlab = "time", ylab = "True beta")
matplot(seq_len(nrow(sims$betas)) - 1, fit_EKF$a_t_d_s, add = T,
        type = "l", lty = 2)
```

Similarly we can do the same with the UKF method. This is done below

```{r}
arg_list$method <- "UKF"
arg_list$k <- 0
  
system.time(fit_UKF <- do.call(ddhazard, arg_list))
```

```{r}
matplot(seq_len(nrow(sims$betas)) - 1, sims$betas, type = "l", lty = 1, 
        lwd = c(2, rep(1, n_vars)), xlab = "time", ylab = "True beta")
matplot(seq_len(nrow(sims$betas)) - 1, fit_UKF$a_t_d_s, add = T,
        type = "l", lty = 2)
```

## Linear time complexity
Finally, we can illustrate that the time complexity is linear in the current implementation. First we find the computation time:

```{r message=FALSE}
ns <- 2^(13:18)

comp_time <- matrix(NA_real_, nrow = length(ns), ncol = 2)
i <- 0
for(n in ns){
  i <- i + 1
  set.seed(32231) # the same seeed is used to yield the same parameter vector
  sims_args$n_series <- n
  arg_list$data <- do.call(test_sim_func_logit, sims_args)$res
  arg_list$id <- arg_list$data$id
  
  arg_list$method <- "EKF"
  comp_time[i, 1] <- system.time(do.call(ddhazard, arg_list))["user.self"]
  
  arg_list$method <- "UKF"
  comp_time[i, 2] <- system.time(do.call(ddhazard, arg_list))["user.self"]
}
```

We can now make a log-log plot of number observation versus computation time and check that the linear regression has a slope of roughly one 
```{r}
plot(comp_time[, 1] ~ ns, log = "xy",
     xlab = "Number of inviduals", ylab = "EKF computation time")
coefficients(lm(log(comp_time[, 1]) ~ log(ns)))

plot(comp_time[, 2] ~ ns, log = "xy",
     xlab = "Number of inviduals", ylab = "UKF computation time")
coefficients(lm(log(comp_time[, 2]) ~ log(ns)))
```

# Future developments
A list of future ideas are given below

## `s3` generics
The current method have some implementations of `plot`, `residuals`, `logLike` and `predict`. Feedback of the usefulness and further of developments of their capabilities are needed. Further, `summary` and `print` methods would be neat


## Second order
The second order model is implemented and have been used with the EKF. Though, it is unstable and more testing is needed. Implementing higher order is another option


## Tests
Test for model fit, time varying effects versus statistics effects would be useful 


## Time in-variant
Being able to add time in-variant effects. Currently, additional parameters increase the dimension of the co-variance matrix $\mathbf{Q}$ which do cause issues in higher dimensions of the state space vector. Time in-variant effects could easily be estimated in the M-step of the algorithm. For instance, for the glm methods like logit the time in-variant effects would act as offsets terms in the E-step and parameters could be estimated independently of the $\vec{\alpha}_0$ and $\mathbf{Q}$ in the M-step


## Continuous time
Replacing the logit model with continuous time model. I am currently working on this with my advisor Feodor. The continuous time would reduce the loss of information due to the arbitrary binning. Moreover, these methods allows for un-equal intervals length which may be useful for some applications

## Parallel implementation of UKF
The computation time of the UKF can be reduced by computing it is parallel as the EKF

## Initialization
Other ideas than setting $\mathbf{Q}=\kappa\mathbf{I}$ may be worth considering 
