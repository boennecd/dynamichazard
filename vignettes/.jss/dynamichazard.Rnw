\documentclass[article,shortnames]{jss}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
Benjamin Christoffersen\\Copenhagen Business School, Center for Statistics
}
\title{\pkg{dynamichazard}: Dynamic Hazard Models using State Space Models}
\Keywords{survival analysis, time-varying parameters, extended Kalman filter, EM-algorithm, unscented Kalman filter, parallel computing, \proglang{R}, \pkg{Rcpp}, \pkg{RcppArmadillo}}

\Abstract{
The \pkg{dynamichazard} package implements state space models for survival analysis with time-varying effects which can be estimated quickly. I cover the models and estimation methods implemented in \pkg{dynamichazard}, applying them to a large data set with hard disk failures and including a simulation study to illustrate the methods computation time and performance.}

\Plainauthor{Benjamin Christoffersen}
\Plaintitle{dynamichazard: Dynamic Hazard Models using State Space Models}
\Plainkeywords{survival analysis, time-varying parameters, extended Kalman filter, EM-algorithm, unscented Kalman filter, parallel computing, R, Rcpp, RcppArmadillo}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
\Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
  Benjamin Christoffersen\\
  Center for Statistics\\
  Copenhagen Business School\\
  Solbjerg Pl. 3, A4.19, 2000 Frederiksberg, Denmark\\
  E-mail: \href{mailto:bch.fi@cbs.dk}{\nolinkurl{bch.fi@cbs.dk}}\\
  URL: \url{http://bit.ly/2nPbTfK}\\~\\
  }

% Included package by default are: graphicx, color, hyperref, ae, fancyverb and natbib

\usepackage{array}
\usepackage[utf8]{inputenc}
% \usepackage{fancyvrb} % for references inside Verbatim
\usepackage{textcomp} % copy right and trademark
\usepackage{amsmath} \usepackage{bm} \usepackage{amsfonts}
\usepackage{algorithm} \usepackage{algpseudocode} \usepackage{hyperref}
\usepackage{rotating} \usepackage{csquotes}

% fancyvrb documentation regarding commandchars
%   commandchars (three characters) : characters which define the character which
%   starts a macro and marks the beginning and end of a group; thus lets us introduce
%   escape sequences in verbatim code. Of course, it is better to choose special
%   characters which are not used in the verbatim text! (Default: empty).
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,commandchars=\\\&;,fontsize=\small}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,commandchars=\\\&;,fontsize=\small}

% algorithm and algpseudocode definitions
\newcommand\StateX{\Statex\hspace{\algorithmicindent}}
\newcommand\StateXX{\Statex\hspace{\algorithmicindent}\hspace{\algorithmicindent}}
\newcommand\StateXXX{\Statex\hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent}}

\algnewcommand{\UntilElse}[2]{\Until{#1\ \algorithmicelse\ #2}}

\algtext*{EndFor} \algtext*{EndProcedure}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}

\newcommand{\citeAlgLine}[2]{line~\ref{#1} of algorithm~\ref{#2}}
\newcommand{\CiteAlgLine}[2]{Line~\ref{#1} of algorithm~\ref{#2}}
\newcommand{\citeAlgLineTwo}[3]{lines~\ref{#1} and~\ref{#2} of algorithm~\ref{#3}}
\newcommand{\citeAlgLineTo}[3]{lines~\ref{#1}-\ref{#2} of algorithm~\ref{#3}}

% Table commands
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcommand{\WiTbl}[2]{{
\renewcommand{\arraystretch}{2}
\begin{table}[h!]
\centering
\begin{tabular}{R{3cm} p{9cm}}
#1
\end{tabular}
\caption{#2}
\end{table}
}}

% Math commands

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%
\newcommand{\Lparen}[1]{\left( #1\right)}
\newcommand{\Lbrace}[1]{\left\{ #1\right\}}
\newcommand{\LVert}[1]{\left\rVert #1\right\lVert}
%
\newcommand{\Cond}[2]{\left. #1 \vphantom{#2} \right\vert  #2}
\newcommand{\propp}[1]{\Prob\Lparen{#1}}
\newcommand{\proppCond}[2]{\propp{\Cond{#1}{#2}}}
%
\newcommand{\expecp}[1]{\E\Lparen{#1}}
\newcommand{\expecpCond}[2]{\expecp{\Cond{#1}{#2}}}
%
\newcommand{\varp}[1]{\VAR\Lparen{#1}}
\newcommand{\varpCond}[2]{\varp{\Cond{#1}{#2}}}
%
\newcommand{\likep}[1]{L\Lparen{#1}}
\newcommand{\likepCond}[2]{\likep{\Cond{#1}{#2}}}
%
\newcommand{\hvec}[1]{\widehat{\vec{#1}}}
\newcommand{\hmat}[1]{\widehat{\mat{#1}}}
\newcommand{\tvec}[1]{\tilde{\vec{#1}}}
\newcommand{\tmat}[1]{\tilde{\mat{#1}}}
%
\newcommand{\diag}[1]{\text{diag}{\Lparen{#1}}}
\newcommand{\deter}[1]{\left| #1 \right|}
\newcommand{\bigO}[1]{\mathcal{O}\Lparen{#1}}
%
\newcommand{\emNotee}[3]{#1_{\left. #2 \right\vert #3}}
\newcommand{\emNote}[4]{#1_{\left. #2 \right\vert #3}^{(#4)}}
%
%
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\argmaxu}[1]{\underset{#1}{\argmax}\:}
\newcommand{\argminu}[1]{\underset{#1}{\argmin}\:}
%
% Comment back if you edit code without jss commands
% \newcommand{\Prob}{P}
% \newcommand{\VAR}{Var}
% \newcommand{\E}{E}

% Math commands for section on global mode approximation

\newcommand\algGMAscore[1]{
\begin{aligned}
	 #1&\Lparen{\emNotee{\mat{V}}{t}{t-1}^{-1} + \mat{X}_t^\top (-c''(\vec{\alpha}^{(k-1)})\mat{X}_t}^{-1}
	 \left(\zeta_0 \vphantom{\Lparen{-c''(\vec{\alpha}^{(k-1)})}}
		 \emNotee{\mat{V}}{t}{t-1}^{-1} \emNotee{\vec{a}}{t}{t-1} + \zeta_0 \mat{X}_t^\top c'(\vec{\alpha}^{(k-1)})
		\right. \\
		&\hspace{50pt}\left. + \Lparen{\mat{X}_t^\top\Lparen{-c''(\vec{\alpha}^{(k-1)})}\mat{X}_t + (1-\zeta_0)   \emNotee{\mat{V}}{t}{t-1}^{-1}} \vec{a}^{(k - 1)} \right)
\end{aligned}}
%
\newcommand\algGMApPrime{
\left. \frac{\partial\log\proppCond{\vec{y}_t}{\vec{e}'}}{\partial \vec{e}'} \right|_{\vec{e}' = \mat{X}_t \vec{\alpha}}}
%
\newcommand\algGMApPrimePrime{
\left. \frac{\partial\log\proppCond{\vec{y}_t}{\vec{e}'}}{\partial \vec{e}'\partial \Lparen{\vec{e}'}^\top}\right|_{\vec{e}' = \mat{X}_t \vec{\alpha}}}
%
\newcommand\eqnGblModeTerma{
\begin{pmatrix}
  	\vec{h}'\Lparen{\mat{X}_t  \vec{a}^{(k-1)}}\varpCond{\vec{Y}_t}{\mat{X}_t\vec{\alpha}^{(k-1)}}^{-1/2} & \mat{0} \\
	\mat{0} &  \emNotee{\mat{V}}{t}{t-1}^{-1/2}
\end{pmatrix}}
\newcommand\eqnGblModeTermb{
\begin{pmatrix}\mat{X}_t \\ \mat{I} \end{pmatrix}}
\newcommand\eqnGblModeTermc{
\begin{pmatrix} \vec{b} \\  \emNotee{\vec{\alpha}}{t}{t-1} \end{pmatrix}}

% Other commands
\newcommand\notQsens{
 $\mat{Q}_0$ & $\mat{Q}_0$ can be set to a diagonal matrix with large entries where the exact values do not have a big impact in some cases. }

\newcommand\Qsens{
 $\mat{Q}_0$ & Sensitivity to the choice of $\mat{Q}_0$. }

\newcommand\prosLikelihood{%
 Likelihood & We maximize the likelihood directly instead of having to work with residuals. This is useful for the continuous time model in section~\ref{sec:mod}.}

\newcommand\embParallel{
  Embarrassingly parallel & The most computationally expensive part is easily computed in parallel}

\begin{document}

<<setup_knitr, echo=FALSE, cache=FALSE>>=
knitr::render_sweave()

#####
# Hook to set par
with(new.env(), {
  par_default <- function(cex_mult = 1, ...){
    cex <- .75 * cex_mult

    list(
      mar = c(5, 5, 2, 2),
      bty = "L",
      xaxs = "i",
      pch=16,
      cex= cex,
      cex.axis = 1.25,
      cex.lab = 1.4,
      lwd= 1)
  }

  knitr::knit_hooks$set(
    par_1x1 =
      function(before, options, envir) {
        if(!options$par_1x1)
          return()

        if (before){
          par(mfcol = c(1, 1))
          par(par_default(.8))
        }
      },

    par_3x3 =
      function(before, options, envir) {
        if(!options$par_3x3)
          return()

        if (before){
          par(mfcol = c(3, 3))
          tmp <- par_default(.8)
          tmp$mar <- tmp$mar + c(0.5, 0, 0, 0)
          par(tmp)
        }
    })
})

######
# Chunk options
knitr::opts_chunk$set(
  echo = TRUE, warning = F, message = F, dpi = 144,
  cache = T,
  fig.align = "center",
  stop = FALSE,

  # See opts_hooks definition
  fig.height = -1, fig.width = -1)

#####
# Alter width and height of figures
knitr::opts_hooks$set(
  fig.height = function(options) {
    if(options$fig.height > 0)
      return(options)

    if (!is.null(options$par_3x3) && options$par_3x3){
      options$fig.height <- 8
    } else
      options$fig.height <- 3

    options
  },

  fig.width = function(options) {
    if(options$fig.width > 0)
      return(options)

    if (!is.null(options$par_3x3) && options$par_3x3) {
      options$fig.width <- 9
    } else
      options$fig.width <- 5

    options
  })
@


<<setup_other, echo=FALSE, cache=FALSE>>=
#####
# R options
options(digits = 3, scipen=7, width = 60)

#####
# Define colors that are good for red-green colorblinds
cols <-cbind(
  r=c(91,0,23,255,8,255,4,0,0,255,255),
  g=c(0,255,169,232,0,208,255,0,79,21,0),
  b=c(12,233,255,0,91,198,4,255,0,205,0))
cols <- apply(cols, 1, function(x)
  rgb(x[1], x[2], x[3], maxColorValue = 255))

# barplot(rep(1, length(cols)), col = cols)
palette(c("black", cols, "darkgray"))

#####
# Load and attach libaries
library("splines")
library("stringr")
library("splines")
library("plotrix")
library("grDevices")
library("mvtnorm")
library("tcltk")
library("xtable")
library("zoo", quietly = T, warn.conflicts = FALSE)

#####
# Remove comments from xtable
print.xtable <- with(new.env(), {
  org_fun <- print.xtable
  function(..., comment = FALSE)
    org_fun(..., comment = comment)
})
@

The focus of the \pkg{dynamichazard} package is on survival analysis with time-varying parameters using state space models with estimation methods that are fast and scale well. The contribution of this paper and the package is to give an overview of computationally fast nonlinear estimation methods for state space models that scale well in the dimension of the observational equation, provide an easy interface to use for such methods in survival analysis, and illustrate the methods use.

Time-varying parameters are a common part of survival analysis. Splines are often used to model the time-varying parameters. Various packages in \proglang{R} \citep{baseR16} on the comprehensive \proglang{R} archive network (CRAN) take this approach. For example, the \pkg{dynsurv} package \citep{dynsurv} includes the \code{splineCox} function, along with a non-spline-based Bayesian approach with piecewise constant parameters and a transformation-based approach. Another example is the \pkg{rstpm2} package \citep{rstpm2}. An L2 penalized B-spline intercept is implemented in the \pkg{bshazard} package \citep{Rebora14}. Further, penalized time-varying parameters can be estimated by using any of the regularization methods in packages like \pkg{glmnet} \citep{Simon11}, \pkg{glmpath} \citep{glmpath}, \pkg{mgcv} \citep{wood06}, and \pkg{penalized} \citep{Goeman10}, combined with the method described by~\cite{Thomas14}.

The \pkg{timereg} package \citep{martinussen07} uses a non-spline-based approach for both the Cox regression model and the additive hazards models. Similarly, the \code{aareg} function in the package \pkg{survival} \citep{survival} included with \proglang{R} can estimate the non-parametric Aalen model with time-varying parameters. The \pkg{pch} package \citep{pch} fits time-varying parameters by dividing the time into periods and making separate Poisson regressions in each period. \pkg{concreg} \citep{concreg} uses conditional logistic regression to estimate the time-varying parameters.

Another approach to modeling the time-varying parameters is to use discrete state space models where the parameters are assumed to be piecewise constant, see \cite{Fahrmeir92} and \cite{Fahrmeir94}. An advantage of state space approach is that it provides a parametric model for the parameters that allows for extrapolation beyond the last observed period. Moreover, the models can be implemented to have a linear computational cost relative to the number of observed individuals, cubic computational cost relvative to the number of parameters, and are easily computed in parallel. Consequently, they scale well to large data sets.

Various packages for state space models are available on CRAN. Two reviews of packages for the standard Gaussian models from 2011 are \cite{Petris11} and \cite{Tusell11}. They briefly mention nonlinear models that can be used in survival analysis. The package \pkg{KFAS} \citep{kfas,helske16} provides nonlinear models which can be used in survival analysis. Another package is \pkg{pomp} \citep{King16} for general nonlinear models with both Bayesian and frequentist methods. One can also use \pkg{rstan} \citep{rstan} to set up a variety of models. Because, unlike \pkg{dynamichazard}, all these packages are quite general, using them to set up models is like those in \pkg{dynamichazard} is cumbersome and computationally expensive.

The rest of this paper is structured as follows. Section~\ref{sec:notation} introduces this paper's problem and notation. Section~\ref{sec:meth} shows the EM-algorithm on which all the methods are based, followed by four different filters used in the E-step. A data set with hard disk failures will be used throughout this section to illustrate the methods. Section~\ref{sec:mod} covers the two implemented models. Section~\ref{sec:sims} illustrates the methods' performance and computation time on simulated data. Section~\ref{sec:conc} concludes.


All methods are implemented in \proglang{C++} with use of \pkg{BLAS} and \pkg{LAPACK} \citep{laug} either by direct calls to the methods or through the \proglang{C++} library \pkg{Armadillo} \citep{Sanderson16}. The reported computational complexity in the rest of the paper is based on a single iteration of the EM-algorithm.

The inspiration for the package is from \cite{Fahrmeir92} and \cite{Fahrmeir94}. The current implementation uses the EM-algorithm from these papers. The reader may want further resources on the filters covered later, as this paper introduces them only briefly. \citet[chapter 4]{durbin12} cover the Kalman filter, which provides a basis for understanding all the filters in the package. \cite{Fahrmeir92,Fahrmeir94} covers the extended Kalman filter this package uses, \citet[section 10.2]{durbin12} cover the more common form of the extended Kalman filter. \citet[section 10.3]{durbin12} and \cite{Wan00} provide an introduction to the unscented Kalman filter. Another resource is \cite{Hartikainen11} who introduce the Kalman filter, extended Kalman filter, and unscented Kalman filter.

\section{Notation and problem}\label{sec:notation}

I will start by introducing the notation in a discrete time model for survival analysis. Outcomes in the model are binary. Either an individual has an event or not within each interval. I generalize in section~\ref{sec:mod} to a continuous time model. We are observing individual $1,2,\dots,n$ who each has an \emph{event} at time $T_1,T_2,\dots,T_n$ and \emph{censoring indicators} $D_{i1},D_{i2},\dots, D_{in}$ with $D_{it} \in \{0, 1\}$. The censoring indicator is one if the individual is censored. By definition I set $D_{ik} = 1$ for $k > t$ if the we observe an event for individual $i$ at time $t$. I define a series of outcome indicators for each individual by

\begin{equation}\label{eqn:binFirst}
y_{it} = 1_{\left\{T_i \in (t - 1, t]\right\}}
	= \left\{\begin{matrix}1 & \text{if } T_i \in (t-1, t] \\ 0 & \text{otherwise} \end{matrix}\right.
\end{equation}

Thus, $y_{it}$  denotes whether individual $i$ experiences an event in interval $(t-1, t]$. We observe covariate vectors $\vec{x}_{i1},\vec{x}_{i2},\dots, \vec{x}_{in}$ for each individual $i$ where the latter subscript correspond to the interval number. Next, the \emph{risk set} in interval $t$ is given by

\begin{equation}\label{eqn:discreteRiskSet}
  R_t = \Lbrace{i \in \{1,\dots,n\}: D_{it} = 0}
\end{equation}

I will refer to this as the \emph{discrete risk set}, as I will introduce a continuous version later. The risk of an event for a given individual $i$ in interval $t$ is given by

\begin{equation}\begin{aligned}\label{eqn:condProb}
  \proppCond{Y_{it} = 1}{ \vec{\alpha}_t} =  h(\vec{\alpha}_t^\top \vec{x}_{it})
\end{aligned}\end{equation}

$\vec{\alpha}_t$ is the state vector in interval $t$ and $h$ is the inverse link function. The inverse logistic function, $h(\eta) = \exp(\eta) / (1 + \exp(\eta))$, is used by default. The model written in the state space form is

\begin{equation}\label{eqn:stateEqn}
\begin{array}{ll}
  \vec{y}_t = \vec{z}_t(\vec{\alpha}_t) + \vec{\epsilon}_t \qquad &
  \vec{\epsilon}_t \sim (\vec{0}, \varpCond{\vec{y}_t}{\vec{\alpha}_t})  \\
  \vec{\alpha}_{t + 1} = \mat{F}\vec{\alpha}_t + \mat{R}\vec{\eta}_t \qquad &
  \vec{\eta}_t \sim N(\vec{0}, \mat{Q}) \\
  & \vec{\alpha}_{0} \sim N(\vec{a}_0, \mat{Q}_0)
\end{array} , \qquad t = 1,\dots, d
\end{equation}

where

\begin{equation}
  \vec{y}_t = \Lparen{y_{it}}_{i\in R_t}
\end{equation}

The equation for $\vec{y}_t$ is denoted the \emph{observational equation}. $\sim (v,b)$ denotes a random variable with mean (vector) $v$ and variance (covariance matrix) $b$. It need not be a normal distribution. $\vec{\alpha}_t$ is the state vector with the corresponding \emph{state equation}. Further, I denote the observational equation's conditional covariance matrix by $\mat{H}_t(\vec{\alpha}_t) = \varpCond{\vec{y}_t}{\vec{\alpha}_t}$. The mean $\vec{z}_t(\vec{\alpha}_t)$ and variance $\mat{H}(\vec{\alpha}_t)$ are state dependent with

\begin{equation}
\begin{aligned}
  z_{it}(\vec{\alpha}_t) &=\expecpCond{y_{it}}{\vec{\alpha}_t} = h(\vec{\alpha}_t^\top \vec{x}_{it}) \\
  H_{ijt}(\vec{\alpha}_t) &= \left\{\begin{matrix}
    \varpCond{y_{it}}{\vec{\alpha}_t} & i = j \\ 0 & \textrm{otherwise}
  \end{matrix}\right. \\
  &=\left\{\begin{matrix}
    z_{it}(\vec{\alpha}_t)(1 - z_{it}(\vec{\alpha}_t)) & i = j \\ 0 & \textrm{otherwise}
  \end{matrix}\right.
\end{aligned}
\end{equation}

The state equation is implemented with a first- and second-order random walks. The first-order random walk has $\mat{F} = \mat{R} = \mat{I}_m$ where $m$ is the number of time-varying parameters and $\mat{I}_m$ is the identity matrix with dimension $m$. I let $q$ denote the dimension for the state space vector. Thus, $q=m$ for the first-order random walk. As for the second-order random walk, we have
\begin{equation}
\mat{F} = \begin{pmatrix}
  2\mat{I}_m & - \mat{I}_m \\ \mat{I}_m & \mat{0}_m
\end{pmatrix},  \qquad
\mat{R} = \begin{pmatrix} \mat{I}_m \\ \mat{0}_m \end{pmatrix}
\end{equation}

where $\mat{0}_m$ is a $m\times m$ matrix with zeroes in all entries. The vector in the state equation is ordered as $\vec{\alpha}_t = (\tvec{\alpha}_t^\top, \tvec{\alpha}_{t-1}^\top)^\top$ to match the definitions of $\mat{F}$ and $\mat{R}$ where the tilde on the alphas is added to indicate the parameters used when computing the linear predictor in Equation~\eqref{eqn:condProb}. Notice that the dimension of the state vector is $q = 2m$, which affects the computational cost. The likelihood of the model where state vectors are observed can be written as follows by application of the Markovian property of the model:

\begin{equation}
L(\mat{Q},\mat{Q}_0, \vec{a}_0)
	= p(\vec{\alpha}_0)\prod_{t = 1}^d \proppCond{\vec{\alpha}_t}{\vec{\alpha}_{t-1}}
		\prod_{i \in R_t} \proppCond{y_{it}}{\vec{\alpha}_t}
\end{equation}

so that the log likelihood (ignoring a constant) is

\begin{equation}\label{eqn:logLikeFirst}
\begin{aligned}
	\log L \Lparen{\mat{Q},\mat{Q}_0, \vec{a}_0} =
	 & - \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^\top \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^\top\mat{R}^\top\mat{Q}^{-1}\mat{R}\Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\
	&  - \frac{1}{2} \log \deter{\mat{Q}_0} - \frac{1}{2d} \log \deter{\mat{Q}} \\
	&  + \sum_{t = 1}^d \sum_{i \in R_t} l_{it}({\vec{\alpha}_t}) + \dots
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:binModelLikeliFirst}
l_{it}(\vec{\alpha}_t) = y_{it} \log h(\vec{x}_{it}^\top \vec{\alpha}_t) + (1 - y_{it})
	\log \Lparen{1 - h(\vec{x}_{it}^\top \vec{\alpha}_t)}
\end{equation}

This completes the notation I will need for the discrete model. I continue with the methods used to fit the model.

\section{Methods}\label{sec:meth}

The estimation function in \pkg{dynamichazard} is \code{ddhazard}. All the methods implemented in the current version of \code{ddhazard} use the EM-algorithm described in \cite{Fahrmeir94,Fahrmeir92}. The EM-algorithm is similar to the method in \cite{Shumway82} but for a nonlinear observational equation. The unknown hyperparameters in the state Equation~\eqref{eqn:stateEqn} are the covariance matrices $\mat{Q}$ and $\mat{Q}_0$ and the initial state mean  $\vec{a}_0$. $\mat{Q}$ and $\vec{a}_0$ will be estimated in the M-step of the EM-algorithm. It is common practice with Kalman filters to set the diagonal elements of $\mat{Q}_0$ to large values, yielding an information matrix that almost has zeroes in all entries. Another approach is diffuse initialization. The idea is to set $\mat{Q}_0 = c\mat{I} + \tmat{Q}_0$ for a given matrix $\tmat{Q}_0$ and derive the various formulas in the limit $c\rightarrow\infty$. See~\citet[chapter 5]{durbin12}. I use the following notation for the conditional means and covariance matrix:

\begin{equation}
  \emNotee{\vec{a}}{t}{s} = \expecpCond{\vec{\alpha}_t}{\vec{y}_1,\dots,\vec{y}_s},  \qquad
    \emNotee{\mat{V}}{t}{s} = \expecpCond{\mat{V}_t}{\vec{y}_1,\dots,\vec{y}_s}
\end{equation}

Notice that the letter "a" is used for the mean estimates, while "alpha" is used for the unknown state. The notation above both covers filter estimates in the case where $s \leq t$ and smoothed estimates when $s \geq t$. Here I suppress the dependence on the covariates, $\vec{x}_{it}$, to simplify the notation. The EM algorithm is given in algorithm~\ref{alg:EM}.

\begin{algorithm}
\caption{EM algorithm with unspecified filter.}\label{alg:EM}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0,\vec{a}_0,\mat{X}_1,\dots,\mat{X}_d,\vec{y}_1,\dots,\vec{y}_d,R_1,\dots,R_d$
\Statex Convergence threshold $\epsilon$
\State Set $\emNote{\vec{a}}{0}{0}{0} = \vec{a}_0$ and $\mat{Q}^{(0)} = \mat{Q}$
\For{$k=1,2,\dots$}
\Procedure{E-step}{}
\State Apply filter with  $\emNote{\vec{a}}{0}{0}{k-1}$, $\mat{Q}^{(k-1)}$ and $\mat{Q}_0$ to get \label{alg:EM:filter}
\StateXXX $\emNotee{\vec{a}}{1}{0},$ $\emNotee{\vec{a}}{1}{1},$ $\emNotee{\vec{a}}{2}{1},\dots,$ $\emNotee{\vec{a}}{d}{d-1},$ $\emNotee{\vec{a}}{d}{d}$ and
\StateXXX $\emNotee{\mat{V}}{1}{0},$ $\emNotee{\mat{V}}{1}{1},$ $\emNotee{\mat{V}}{2}{1},\dots,$ $\emNotee{\mat{V}}{d}{d-1},$ $\emNotee{\mat{V}}{d}{d}$
	\StateXX Apply smoother by computing
\For{$t=d,d-1,\dots,1$}
\State $\mat{B}_t^{(k)} = \emNotee{\mat{V}}{t - 1}{t - 1} \mat{F} \emNotee{\mat{V}}{t}{t - 1}^{-1}$
\State $\emNote{\vec{a}}{t - 1}{d}{k} = \emNotee{\vec{a}}{t - 1}{t - 1} + \mat{B}_t^{(k)} (
    \emNote{\vec{a}}{t}{d}{k} - \emNotee{\vec{a}}{t}{t - 1})$
\State $\emNote{\mat{V}}{t - 1}{d}{k} = \emNotee{\mat{V}}{t - 1}{t - 1} + \mat{B}_t^{(k)} (
    \emNote{\mat{V}}{t}{d}{k} - \emNotee{\mat{V}}{t}{t - 1}) \Lparen{\mat{B}_t^{(k)}}^\top$
\EndFor
\EndProcedure
\Procedure{M-step}{}
	\StateXX Update initial state and covariance matrix by
\State $\emNote{\vec{a}}{0}{0}{k} = \emNote{\vec{a}}{0}{d}{k}$
\State $\begin{aligned}\mat{Q}^{(k)} &= \frac{1}{d}	\sum_{t = 1}^d \mat{R}^\top\left(
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^\top \right. \\
    &\hspace{15pt} + \emNote{\mat{V}}{t}{d}{k} - \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} -
      \left( \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} \right) ^\top +
      \mat{F}\emNote{\mat{V}}{t - 1}{d}{k}\mat{F}^\top
      \left. \vphantom{\left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^\top} \right)\mat{R}
  \end{aligned}$
\EndProcedure
\StateX Stop the if sum of relative norms is below the threshold
\State $\sum_{t=0}^d \frac{\LVert{\emNote{\vec{a}}{t}{d}{k} - \emNote{\vec{a}}{t}{d}{k - 1}}}{\LVert{\emNote{\vec{a}}{t}{d}{k - 1}}} < \epsilon$
\EndFor
\end{algorithmic}
\end{algorithm}

The matrices $\mat{X}_1,\mat{X}_2,\dots,\mat{X}_d$ are the design matrices given by the risk set $R_1,R_2,\dots,R_d$ and the covariate vectors. The only unspecified part is the filter in \citeAlgLine{alg:EM:filter}{alg:EM}. Notice that the other lines involve only product of matrices and vectors of dimension equal to the state space vector dimension, $q$. Moreover, the computational cost is independent of the size of the risk sets for the specified parts of algorithm~\ref{alg:EM}. Thus, the computational complexity so far is $\bigO{q^3d}$, where $d$ is the number of intervals. The threshold for convergence is determined by the \code{eps} element of the list passed to the \code{control} argument of \code{ddhazard} (e.g., \code{list(eps = 0.01, ...)}). From my experience, the EM-algorithm tends to converge slowly toward the end. However, a tolerance of $0.01$ or $0.001$ is quickly satisfied with minor differences compared with smaller tolerance. The filters implemented for \citeAlgLine{alg:EM:filter}{alg:EM} are an extended Kalman filter, an unscented Kalman filter, a sequential approximation of the posterior modes, and an estimation of posterior modes. I will cover these in their respective order.

\subsection{The Extended Kalman filter}\label{subsec:EKF}
The extended Kalman filter (EKF) approximates nonlinear state spaces where we make a given order Taylor expansion around the state vector, most commonly using the first-orderTaylor expansion. One of the EKF's advantages is that it results in formulas close to the Kalman filter. We can derive EKF presented here in algorithm~\ref{alg:EKF} \citep{Fahrmeir94}. It can be derived by applying the Woodbury Matrix identity to the usual EKF. Commonly, the prediction step is known as the time update and the correction step is known as the measurement update. A few points are worth making. Firstly, the largest computational burden is in \citeAlgLine{alg:EKF:scoreMat}{alg:EKF} when the dimension of the state vector, $q$, is low compared to the number of observations at time $t$, which I denote as $n_t = \vert R_t \vert$. However, the computation here is what is known as embarrassingly parallel. That is, it can easily be computed in parallel because little communication is required between the parallel tasks. This is exploited in the current version of \code{ddhazard}, which computes \citeAlgLineTwo{alg:EKF:scoreVec}{alg:EKF:scoreMat}{alg:EKF} in parallel using the \proglang{C++} library \pkg{thread}. All the matrices and vectors are of dimension $q\times q$ and $q$, facilitating confirmation that the filter is $\bigO{q^2n_t + q^3}$ in computational complexity.

\begin{algorithm}
\caption{Extended Kalman filter (EKF).}\label{alg:EKF}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0, \vec{a}_0,$ $\mat{X}_1,\dots,\mat{X}_d,$ $\vec{y}_1,\dots,\vec{y}_d,$ $R_1,\dots,R_d$
\State Set $\emNotee{\vec{a}}{0}{0} = \vec{a}_0$ and $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$
\For{$t=1,2,\dots,d$}
\Procedure{Prediction step}{}
\State $\emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}$
\State $\emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^\top + \mat{R}\mat{Q}\mat{R}^\top$
\EndProcedure
\Procedure{Correction step}{}
\StateXX Compute score vector and information matrix and set:
\State Let $\vec{a} = \emNotee{\vec{a}}{t}{t - 1}$
%
\State $\vec{u}_t (\vec{a}) = \sum_{i \in R_t} \vec{u}_{it} (\vec{a}), \quad\vec{u}_{it} (\vec{a})= \left. \vec{x}_{it} \frac{\partial h(\eta)/ \partial \eta}{H_{iit}(\vec{a})} \Lparen{y_{it} - h(\eta)} \right\vert_{\eta = \vec{x}_{it}^\top \vec{a}}$ \label{alg:EKF:scoreVec}
%
\State $\mat{U}_t (\vec{a}) = \sum_{i \in R_t} \mat{U}_{it} (\vec{a}), \quad \mat{U}_{it} (\vec{a}) = \left. \vec{x}_{it} \vec{x}_{it}^\top \frac{\left( \partial h(\eta)/ \partial \eta \right)^2}{H_{iit}(\vec{a})} \right\vert_{\eta = \vec{x}_{it}^\top \vec{a}}$ \label{alg:EKF:scoreMat}
%
\State $\emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\vec{a})\right)^{-1}$ \label{alg:EKF:varUpdate}
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1} + \emNotee{\mat{V}}{t}{t}\vec{u}_t (\vec{a})$ \label{alg:EKF:stateUpdate}
\EndProcedure
\EndFor
\end{algorithmic}
\end{algorithm}

The \code{ddhazard} function provides some changes to algorithm~\ref{alg:EKF}. First, \citeAlgLineTwo{alg:EKF:varUpdate}{alg:EKF:stateUpdate}{alg:EKF} are similar to a Newton-Raphson step, which can motivate us to make further steps. The other changes aim to address the EKF's potential divergence. To overcome cases where divergence is a problem, \code{ddhazard} fist introduces a learning rate, $\zeta_0$, in \citeAlgLine{alg:EKF:stateUpdate}{alg:EKF} when we update the state vector. Second, \code{ddhazard} increases the variance in the denominator of the score vector and information matrix in \citeAlgLineTwo{alg:EKF:scoreVec}{alg:EKF:scoreMat}{alg:EKF}, reducing the effect of values predicted near the boundaries of the outcome space. This is similar to the approach \pkg{glmnet} takes \cite[page~9]{friedman10} to deal with large absolute values of the linear predictor. Algorithm~\ref{alg:EKFextra} shows these three changes to the correction step.

\begin{algorithm}
\caption{EKF with extra correction steps, learning rate and hyperparameter $\xi$ replacing \citeAlgLineTo{alg:EKF:scoreVec}{alg:EKF:stateUpdate}{alg:EKF}.}\label{alg:EKFextra}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex Threshold $\epsilon$, learning rate $\zeta_0$ and small numbers $\delta$ and $\xi$
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1}$
\Repeat
\State $\vec{a} = \emNotee{\vec{a}}{t}{t}$
%
\State $\vec{u}_t (\vec{a}) = \sum_{i \in R_t} \vec{u}_{it} (\vec{a}), \quad\vec{u}_{it} (\vec{a})= \left. \vec{x}_{it} \frac{\partial h(\eta)/ \partial \eta}{H_{iit}(\vec{a})} \Lparen{y_{it} - h(\eta)} \right\vert_{\eta = \vec{x}_{it}^\top \vec{a}}$ \label{alg:EKFextra:scoreVec}
%
\State $\mat{U}_t (\vec{a}) = \sum_{i \in R_t} \mat{U}_{it} (\vec{a}), \quad \mat{U}_{it} (\vec{a}) = \left. \vec{x}_{it} \vec{x}_{it}^\top \frac{\left( \partial h(\eta)/ \partial \eta \right)^2}{H_{iit}(\vec{a})} \right\vert_{\eta = \vec{x}_{it}^\top \vec{a}}$ \label{alg:EKFextra:scoreMat}
%
\State $\emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\vec{a})\right)^{-1}$
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\mat{V}}{t}{t}\Lparen{
		\mat{U}_t (\vec{a})\vec{a} + \emNotee{\mat{V}}{t}{t - 1}^{-1}\emNotee{\vec{a}}{t}{t - 1} + \zeta_0 \vec{u}_t (\vec{a})}$
\Until{$\LVert{\emNotee{\vec{a}}{t}{t} - \vec{a}}/ (\LVert{\vec{a}} + \delta) < \epsilon$}
\end{algorithmic}
\end{algorithm}

Extra correction steps are not taken by default. They will be taken if the element \code{NR_eps} of the list passed to the \code{control} argument of \code{ddhazard} is set to the value of $\epsilon$. The user sets the learning rate, $\zeta_0$, by setting the element \code{LR} in the list passed to the \code{control} argument. By default, the current implementation tries a decreasing series of learning rates, starting with $\zeta_0$, until the algorithm does not diverge. $\xi$ is changed by altering the \code{denom_term} element in the list passed to the \code{control} argument. Typically, values in the range $[10^{-6},10^{-4}]$ tend to be sufficient in most cases. My experience is that the user should focus on the learning rate. The motivation for the formulas in algorithm~\ref{alg:EKFextra} is provided in the ddhazard vignette.

\subsubsection{Hard disk failures}

<<load_hd_dat, echo = FALSE>>=
#####
# Load hd data from sub folder
hd_dat <- readRDS("HDS/HDs.RDS")

# Few have data from time zero so we set a few days in as time zero
new_start <- 24 * 4
hd_dat$tstart <- pmax(new_start, hd_dat$tstart)
hd_dat$tstart <- hd_dat$tstart - new_start
hd_dat$tstop <- hd_dat$tstop - new_start

# We need to remove the records that ends before or at the starting time
# sum(hd_dat$tstart >= hd_dat$tstop) # Number of rows thrown away
hd_dat <- hd_dat[hd_dat$tstart < hd_dat$tstop, ]
hd_dat$serial_number <- droplevels(hd_dat$serial_number)

# Re-scale time to months
tmp <- 24 * 30
hd_dat$tstart  <- hd_dat$tstart / tmp
hd_dat$tstop <- hd_dat$tstop / tmp

# Make sure that data is sorted
hd_dat <- hd_dat[order(hd_dat$serial_number, hd_dat$tstart), ]

#####
# Fill in blanks with carry the last observation forward
# Define function to fill in the blanks
library("zoo", quietly = T)
func <- function(x)
  na.locf0(c(0, x))[-1]

# Use the function
for(n in colnames(hd_dat)["smart_12" == colnames(hd_dat)]){
  hd_dat[[n]] <- unlist(
    tapply(hd_dat[[n]], as.integer(hd_dat$serial_number), func),
    use.names = F)
}
@

<<remove_versions_w_few_and_winsorize, echo = FALSE>>=
#####
# Remove version with few unique disks
n_per_model <-
  xtabs(~ model, hd_dat, subset = !duplicated(serial_number))

# We take that have more than a given number of disks
factor_cut <- 400
models_to_keep <- names(n_per_model)[n_per_model >= factor_cut]
hd_dat <- hd_dat[hd_dat$model %in% models_to_keep, ]
hd_dat$model <- droplevels(hd_dat$model)

#####
# Winsorize
win_lvl <- .99
hd_dat$smart_12 <- pmin(hd_dat$smart_12, quantile(
  hd_dat$smart_12, win_lvl))
@

I will use time until failure for hard drives as an example throughout this paper. Predicting when a hard disk will fail is important for any firm that manages large amounts of data in order to replace the hard disks before they fail. Self-monitoring, analysis, and reporting technology (SMART) is one tool used to predict future hard disk failures. The dataset I will use is publicly available from BackBlaze~\cite{backblazestats}, that is a data storage provider that currently manages more than 65,000 hard disks. BackBlaze have a daily snapshot of the SMART attributes for all its hard disks going back to April 2013. The final data set I use has $\Sexpr{length(unique(hd_dat$serial_number))}$ unique hard disks. It has $\Sexpr{nrow(hd_dat)}$ rows in start-stop format for survival analysis.

A hard disk is marked as a failure if \enquote{... the drive will not spin up or connect to the OS, the drive will not sync, or stay synced, in a RAID Array ... [or] the Smart Stats we [BackBlaze] use show values above our [BackBlaze's] thresholds} \citep{backblaze2016Q1}. A hard drive with a failure is removed. I will not use the SMART attributes that BackBlaze uses as covariates because of to the third condition. These are SMART attributes 5, 187, 188, 197, and 198 \citep{backblazesmartstatuse}.

I will use the power-on hours (SMART attribute number 9) as the time scale in the model we estimate. The hard disks run 24 hours a day unless they are shut down (e.g., for maintenance). BackBlaze has stated, \enquote{If one of the drives in a Storage Pod fails, we cycle down the entire Storage Pod to replace the failed drive. This only takes a few minutes and then power is reapplied and everything cycles back up. Occasionally we power cycle a Storage Pod for maintenance and on rare occasions we’ve had power failures, but generally, the drives just stay up} \citep{backblazesmartstatuse}. The quote refers to the storage pods in which the hard disk are placed. BackBlaze uses storage pods with 45 to 60 hard disks in each \citep{backblazepods}.

The SMART attribute I will use as a predictor is the power cycle count (SMART attribute number 12). This counts the number of times a hard disk has undergone a full hard disk power on/off cycle. I will include a factor level for the hard disk version, as the differences in failure rates between hard disk versions are large. In particular, one 3 TB Seagate hard drive (ST3000DM001) has a high failure rate \citep{backblazest3tb}. I have removed versions fewer than $\Sexpr{factor_cut}$ unique hard disks. These have either few cases or few observations. This leaves us with $\Sexpr{length(unique(hd_dat$serial_number))}$ unique hard disks. I winsorize at the $\Sexpr{win_lvl}$ quantile for the power cycle count (i.e., I set values above the $\Sexpr{win_lvl}$ quantile to the $\Sexpr{win_lvl}$ quantile).

% The Pearson correlation and Spearman's rank correlation coefficient is low (less than $0.4$) between the power cycle count and the other SMART statistic that BackBlaze uses. Thus, we may expect to be looking at actual failures.

<<define_get_pretty_model_factors, echo = FALSE>>=
#####
# Define function to format factor levels shorter
library("stringr")
get_pretty_model_factors <- function(x){
  f <- function(lvls){
    lvls <- str_replace(lvls, "^model", "")
    lvls <- str_replace(lvls, "^[A-z]+\\ ", "")

    lvls
  }

  if(class(x) == "fahrmeier_94"){
    colnames(x$state_vecs) <- paste("Param.", f(colnames(x$state_vecs)))
    return(x)
  }

  f(x)
}
@

<<model_stats, echo = FALSE, results='asis', cache=FALSE>>=
#####
# Make data frame to find stats
library("dynamichazard")
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ model,
  data = hd_dat, by = 1, max_T = 60, use_weights = F,
  id = hd_dat$serial_number)

#####
# Make time cut variable and find # disk and # failures
tmp_dat$X$t_cut <- cut(tmp_dat$X$t, breaks = seq(0, 60, 20),
                       right = FALSE)
stats <- by(tmp_dat$X, list(tmp_dat$X$model, tmp_dat$X$t_cut), function(x){
  c("#D" = length(unique(x$serial_number)),
    "#F" = sum(tapply(x$Y, x$serial_number, any, default = 0)))
})

.names <- dimnames(stats)
stats <- sapply(stats, function(x) if(is.null(x)) c(0, 0) else x)

#####
# Format final tabel
n_models <- length(.names[[1]])
rnames <- .names[[1]]
tbl_dat <- lapply(1:length(.names[[2]]), function(i){
  x_vals <- stats[, 1:n_models + n_models * (i - 1L)]
  cnames <- paste(rownames(x_vals), .names[[2]][i])
  structure(t(x_vals), dimnames = list(rnames, cnames))
})
tbl_dat <- do.call(cbind, tbl_dat)
tbl_dat <- tbl_dat[order(tbl_dat[, 1], decreasing = TRUE), ]

# Convert to string
tbl_dat <- structure(
  apply(tbl_dat, 2, sprintf, fmt = "%d"),
  dimnames = dimnames(tbl_dat))

# Format second row header
# See https://stackoverflow.com/a/32490565/5861244
tmp_lvls <- paste0("$t\\in", levels(tmp_dat$X$t_cut), "$")
headers_lvl1 <- paste(
  "\\multicolumn{2}{c}{", tmp_lvls, "}", collapse = " & ")
headers_lvl2 <- paste(substr(colnames(tbl_dat), 0, 2), collapse = " & ")

#####
# Find first observation and first failure and add to table
tbl_xtra <- by(hd_dat, hd_dat$model, function(x) min(x$tstop[x$fails == 1]),
               simplify = FALSE)
tbl_xtra <- do.call(rbind, tbl_xtra)
tbl_xtra[] <- sprintf(tbl_xtra, fmt = "%.3f")

rmatch <- match(row.names(tbl_dat), row.names(tbl_xtra))
tbl_dat <- cbind(tbl_xtra[rmatch], tbl_dat)

# Set header of final tabel
headers_lvl1 <- paste(
  paste(rep(" & ", ncol(tbl_xtra)), # Not -1 as we have the row.names
        collapse = ""),
  "&", headers_lvl1)
headers_lvl2 <- paste(
  "HD version", paste(
    colnames(tbl_xtra), collapse = " & "),
  headers_lvl2, sep = " & ")

headers_lvl2 <- gsub("#", "\\\\#", headers_lvl2)

# Make xtable output
rownames(tbl_dat) <- get_pretty_model_factors(rownames(tbl_dat))

xtbl <- xtable(tbl_dat,
       caption = "Summary statistics for each hard disk versions. The hard disk version is indicated by the first column. The number of disks is abbreviated as '\\#D' and total failures is abbreviated as '\\#F'. The $t\\in[x, y)$ indicates which time interval that the figures applies to.",
       label = "tab:modelDat")

align(xtbl)[-1] <- "r"
align(xtbl)[1] <- "l"
print_out <- capture.output(print(
  xtbl,
  include.colnames = FALSE,
  sanitize.text.function = force,
  hline.after = 0,
  add.to.row = list(
    pos = list(-1),
    command = paste(headers_lvl1, " \\\\[0.33em] \n", headers_lvl2,
                    " \\\\\ \n"))))

print_out <- sapply(
  print_out, gsub,
  pattern = "\\begin{table}[ht]",
  replacement = "\\begin{sidewaystable}",
  fixed = TRUE,
  USE.NAMES = FALSE)

print_out <- sapply(
  print_out, gsub,
  pattern = "\\end{table}",
  replacement = "\\end{sidewaystable}",
  fixed = TRUE,
  USE.NAMES = FALSE)

print_out <- paste0(print_out, collapse = "\n")
cat(print_out)

# Cleanup
rm(tmp_dat, stats, tbl_dat, tbl_xtra)

#####
# Note: You can check the figures against
#   https://www.backblaze.com/blog/hard-drive-failure-rates-q2-2016/
@

Table~\ref{tab:modelDat} provides information about each of the hard disk versions. We see from table~\ref{tab:modelDat} that data is available only for some versions in parts of the 60-month period. Thus, some of the curves shown later will partly be extrapolation. I fit the model using the EKF with a single iteration in the correction step. I use a natural cubic spline basis for the number of power cycles to capture potential nonlinear effects. The code is shown below:

<<first_hd_fit>>=
library("splines")
library("dynamichazard")
# Assign model formula
frm <- Surv(tstart, tstop, fails) ~
      -1 +              # I remove the intercept to not get a reference level
                        # for the hard disk versions factor
      ns(smart_12,                   # Use a natural cubic spline for power
         knots = c(3, (1:5)*10),     # cycle count
         Boundary.knots = c(0, 60))

# Fit model
system.time(           # Used to get the computation time
  ddfit <- ddhazard(
    formula = frm,
    hd_dat,
    id = hd_dat$serial_number,
    Q_0 = diag(1, 24), # Covariance matrix for first state
    Q = diag(.01, 24), # Covariance matrix for transition
    by = 1,            # Length of intervals
    max_T = 60,        # Last time we observe when estimating
    control = list(
      method = "EKF",
      eps = .001)))
@

<<ddfit_change_fac_lvls, echo = FALSE>>=
ddfit <- get_pretty_model_factors(ddfit)
@

The elapsed computation time is in seconds. The \code{by} argument in the call specifies the length of each interval. Thus, \code{by = 0.5} would give twice as many intervals, each with half the length. I will focus on the first nine predicted parameters for the hard disk version in this paper. Figures~\ref{fig:ST3TB} and \ref{fig:otherfaclvl1} show the predicted parameters versions' factor levels. ST3000DM001 (figure~\ref{fig:ST3TB}) differs from the others in its higher failure risk. It is interesting that some versions seems to have a decreasing parameter for the factor in figure~\ref{fig:otherfaclvl1}, whereas the parameter increases for others. Notice that some of the confidence bounds get wider or shorter in the start or at the end because of extrapolation. I only have SMART attributes for at most three years for each hard disk. Furthermore, I only have data for some version in parts of the 60-month period because of BackBlaze purchasing patterns. Thus, we see increasing or decreasing width of the confidence bounds for some parameters of factor levels in certain periods.

<<ST3TB,echo=FALSE,par_1x1 = TRUE, fig.cap="Parameter for the hard disk version ST3000DM001. It is shown in a single plot as it differs from the parameters for the other factor levels shown in figure~\\ref{fig:otherfaclvl1} by being a lot larger around month 30.",cache=FALSE>>=
plot(ddfit, cov_index = 7)
@


<<otherfaclvl1, echo = FALSE, par_3x3 = TRUE, fig.cap="Parameters for factor levels for the hard disk version with EKF with a single iteration in the correction step.",cache=FALSE>>=
plot(ddfit, cov_index = c(1:6, 8:10))
@

<<smart_12_plot, echo = FALSE, par_1x1 = TRUE, fig.cap="Plot of predicted terms on the linear predictor scale for different values of number of power cycles.",cache=FALSE>>=
# Make dummy frame for predict
tmp <- data.frame(
  model = rep(hd_dat$model[1], 3),
  smart_12 = c(1, 10, 40))

# Find predictions
preds <- predict(ddfit, tmp, type = "term", sds = T)

is_ns <- grepl(
  "^ns\\(smart_12", dimnames(preds$terms)[[3]])

# Find lower and upper bounds
preds$lbs <- preds$terms[,, is_ns]  - 1.96 * preds$sds[,, is_ns]
preds$ubs <- preds$terms[,, is_ns] + 1.96 * preds$sds[,, is_ns]

# Plot
cols <- rev(1:3)
xs <- ddfit$times
plot(range(xs), range(preds$lbs, preds$ubs), type = "n",
     xlab = "Time", ylab = "Power cycle term", col = cols)
abline(h = 0, lty = 2)
matplot(xs, preds$terms[,, is_ns], type = "l", add = T, lty = 1,
        col = cols)

# Add confidence bounds
for(i in 1:dim(preds$terms)[2]){
  icol <- adjustcolor(cols[i], alpha.f = 0.1)
  polygon(c(xs, rev(xs)), c(preds$ubs[,i], rev(preds$lbs[,i])),
          col = icol, border = NA)
  lines(xs, preds$ubs[,i], lty = 2, col = cols[i])
  lines(xs, preds$lbs[,i], lty = 2, col = cols[i])
}

# Add legend
legend(
  "bottomright", bty = "n",
  legend = paste0(tmp$smart_12, " Cycles"),
  lty = rep(1, nrow(tmp)),
  col = cols,
  cex = par()$cex * 2.5)
@

<<setup_for_smart_12_illu, echo=FALSE>>=
# Assign caption for next code block
min_obs <- 50
quant <- .1
smart_12_illu_cap <- paste0("Plot showing the mean and ", quant * 100, "\\% quantile of the SMART 12 attribute for each version with those hard disk at risk at the start of each interval. The dashed lines are the quantile curves. Values for a given version in a given interval is excluded if there are less than ", min_obs, " hard disk at risk at the start of the interval. The circle radiuses are proportional to the fraction of hard disk that fail in the month. The transparency of the cirles are inversely log proportional to the number of hard disks at risk.")
@


<<smart_12_illu,echo = FALSE, fig.cap=paste(smart_12_illu_cap), par_1x1=TRUE,cache=FALSE>>=
#####
# Compute figures for the plot
library("plotrix")
library("grDevices")
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ smart_12 + model,
  data = hd_dat, by = 1, max_T = 60, use_weights = F,
  id = hd_dat$serial_number)

smart_12_mean <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$smart_12, X$t, mean))

smart_12_quant <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$smart_12, X$t, quantile, probs = .1))

n_obs <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$smart_12, X$t, length))
max_n_obs <- max(unlist(n_obs))

n_fails <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$Y, X$t, sum))

fail_ratios <- mapply("/", n_fails, n_obs)

#####
# Make plot
plot(c(1, 60), c(0, max(unlist(smart_12_mean))), type = "n",
     xlab = "Month", ylab = "Number of power cycles")

for(i in seq_along(smart_12_mean)){
  col <- if(names(smart_12_mean)[i] == "ST3000DM001") "Red" else "Black"

  # We remove the points with less than min_obs observations
  is_in <- n_obs[[i]] > min_obs
  y_mean <- smart_12_mean[[i]][is_in]
  y_quant <- smart_12_quant[[i]][is_in]
  x <- as.integer(names(smart_12_mean[[i]]))[is_in]

  fail_ratio <- fail_ratios[[i]]
  radius <- fail_ratio[is_in] * 10 + 1e-3

  lines(x, y_mean, col = col)
  lines(x, y_quant, col = col, lty = 2)

  n_obs_use <- n_obs[[i]][is_in]
  for(j in seq_along(x)){
    col_circle <- adjustcolor(
      col, alpha.f = 1 - .75 * (log(max_n_obs) - log(n_obs_use[j])) /
        (log(max_n_obs) - log(min_obs)))
    draw.circle(x[j], y_mean[j], radius[j],
                col = col_circle, border = col_circle)
  }
}

#####
# Cleanup
rm(tmp_dat)
@

% CHECK: claim about curves

Figure~\ref{fig:smart_12_plot} shows how the effect of the number of power cycles evolves over time for three specific choices of the power cycle count. At a first glance, it seems odd that the risk of failure is not monotone in the number of power cycles. We could expect that turning off the hard disk is either good or bad for the hard disk. Moreover, the sharp turns do not seem reasonable in the period from months 20 to 30 in figures~\ref{fig:otherfaclvl1} and~\ref{fig:smart_12_plot}. This may be due to the ST3000DM001 version (TODO: why and change the next text). Figure~\ref{fig:smart_12_illu} shows the mean number of power cycles for each hard disk version during each month. The dashed lines represent the $\Sexpr{quant * 100}$\%. quantile, where $\Sexpr{quant * 100}$ is selected to give an idea of the lower values for the given hard disk version. The ST3000DM001 versions is shown in red. Both the mean and the quantile stand out for ST3000DM001. Thus, the higher level of power cycle counts in the period from months 20 to 30 is primarily the ST3000DM001 hard disks. For this reason, I refit the model without ST3000DM001. Figure~\ref{fig:subset_smart_12_plot} shows the plot for the new model. The new plot shows that the SMART 12 attribute has close to a monotone effect. The pointwise confidence bounds are rather wide despite the large data set. Figure~\ref{fig:subset_EKF_xtra_vs_wo} shows, in black, the parameters in the new model for the hard disk version, whose cruves are less sharp than in figure figure~\ref{fig:otherfaclvl1}.

<<ddfit_check_cond_number, eval = FALSE, echo = FALSE, purl=FALSE>>=
# Run the code here to check the conditioning number in the file tmp.txt
tmp <- ddfit$call
ctrl <- ddfit$control
ctrl["debug"] <- TRUE
tmp$control <- ctrl

sink("tmp.txt")
invisible(eval(tmp))
sink()
@

% CHECK: arguments match

<<refit_ddfit_on_subset, echo = FALSE>>=
#####
# Take subset
hd_dat_sub <- hd_dat
rm(hd_dat)
hd_dat_sub <- hd_dat_sub[hd_dat_sub$model != "ST3000DM001", ]
hd_dat_sub$model <- droplevels(hd_dat_sub$model)

#####
# Re-fit model
new_call <- ddfit$call
new_call$data <- as.name(quote(hd_dat_sub))
new_call$id <- as.call(quote(hd_dat_sub$serial_number))
new_call$Q_0 <- as.call(quote(diag(1, 23)))
new_call$Q <- as.call(quote(diag(.01, 23)))

ddfit <- eval(new_call)
ddfit <- get_pretty_model_factors(ddfit)
@

<<subset_smart_12_plot,echo=FALSE,ref.label='smart_12_plot', par_1x1 = TRUE,fig.cap="Similar plot to figure~\\ref{fig:smart_12_plot} for the model without the ST3000DM001 hard disk version.",cache=FALSE>>=
@

\subsubsection{Examples with more iterations with EKF}
Next, I will look at taking more iterations in the correction step using algorithm~\ref{alg:EKFextra}. I estimate the model again:

% Check: arguments

<<subset_EKF_xtra>>=
system.time(
  ddfit_xtr <- ddhazard(
    formula = frm,
    data = hd_dat_sub,     # Data set without ST3000DM001
    id = hd_dat_sub$serial_number,
    Q_0 = diag(1, 24 - 1), # -1 due to removal of a factor level
    Q = diag(.01, 24 - 1),
    by = 1,
    max_T = 60,
    control = list(
      method = "EKF",
      eps = .001,
      NR_eps = .001))) # Tolerance for extra iterations in correction step
@

I have replaced the previous \code{ddfit} with a fit with the data set without the ST3000DM001 version. Consequently, we can compare the fits with regard to the first nine factor levels with the following call:

<<define_add_hist, echo = FALSE>>=
#####
# Define function to add histogram of number of observations in the background
add_hist <- with(new.env(), {
  tmp_dat <- get_survival_case_weights_and_data(
    Surv(tstart, tstop, fails) ~ smart_12,
    data = hd_dat_sub, by = 1, max_T = 60, use_weights = F,
    id = hd_dat_sub$serial_number)

  # Find number of observations through time for each model
  n_obs <- by(
    tmp_dat$X, tmp_dat$X$model, function(X)
      tapply(X$smart_12, X$t, length))
  n_obs <- xtabs(~ model + t, tmp_dat$X)

  xright <- as.numeric(colnames(n_obs))
  xleft <- xright - 1

  # Cleanup
  rm(tmp_dat)

  # Define function to plot histogram in background
  function(i){
    y_lim <- par("usr")[3:4]
    obs_dat <- n_obs[i, ]

    rect(
      xleft = xleft, xright = xright,
      ybottom = rep(y_lim[1], length(xright)),
      ytop <- (.25 * obs_dat) / max(obs_dat) * diff(y_lim) + y_lim[1],
      col = rgb(0, 0, 0, .2),
      border = rgb(1, 1, 1, 0.25),
      lwd = par()$lwd * 2)
  }
})
@

<<def_fig.cap_bar_expla, echo=FALSE>>=
fig.cap_bar_expla <- " Grey transparent bars indicates the number of individuals at risk for the specific model version. Heights are only comparable within the model versions."
@

<<subset_EKF_xtra_vs_wo, par_3x3 = TRUE, fig.cap=paste0("Comparison of results with and without extra iterations in the correction step with the EKF. The red curve is the estimate with extra iterations.", fig.cap_bar_expla),cache=FALSE>>=
for(i in 1:9){
  # Shown in figure \ref&fig:subset_EKF_xtra_vs_wo;
  plot(ddfit, cov_index = i)
  plot(ddfit_xtr, cov_index = i, add = TRUE, col = "red")
  add_hist(i) # Function defined in this paper
}
@

The plot is is in figure~\ref{fig:subset_EKF_xtra_vs_wo}. The \code{add_hist} is a function for the specific data set that adds the bars for the number of individuals at risk in each interval. For some of the parameters, the two plots differ noticeably, particulalry the factors levels with a sparse amount of data (observations and/or failures) in some periods (cf. table~\ref{tab:modelDat}). % CHECK: there is a difference and it is noticeable when you look at the table that it is the levels with sparse data
Another difference is the number of iterations of the EM-algorithm using each method (shown below). The method with extra iterations in the correction step takes fewer EM-iterations, which explains the comparable computation time.

<<show_n_iter>>=
c(one_it = ddfit$n_iter, more_its = ddfit_xtr$n_iter)
@

<<EKF_cleanup, echo = FALSE>>=
rm(ddfit_xtr)
@


\subsection{Unscented Kalman filter}\label{subsec:UKF}
The unscented Kalman filter (UKF), introduced by \cite{Julier97}, is similar to Monte Carlo methods but uses deterministically selected state vectors and weights instead of random draws. The vectors and weights are respectively known as \emph{sigma points} and \emph{sigma weights}. The UKF potentially provides a better approximation of the nonlinear dynamics than does the linear approximation used in the EKF. The UKF is also less computatioally expensive than Monte Carlo methods. Further, the UKF does not require computation of the Jacobian. The first two advantages are useful in this package, whereas the third is not as important since deriving and computing the Jacobian is not complicated for the models. The method is shown in algorithm~\ref{alg:UKF}.

\begin{algorithm}
\caption{The unscented Kalman filter (UKF) where $\displaystyle\emNotee{\mat{V}}{t}{t - 1}^{1/2}$ denotes the square root matrix of $\emNotee{\mat{V}}{t}{t - 1}$ and $\left(\displaystyle\emNotee{\mat{V}}{t}{t - 1}^{1/2}\right)_j$ denotes the $j$'th column of the square root matrix. $\diag{\cdot}$ gives a diagonal matrix with the entries of the argument vector in the diagonal. $q$ is the dimension of the state vector. $\vec{W}^{(\cdot)}$ is the vector with elements $W^{(\cdot)}_0,W^{(\cdot)}_1,\dots,W^{(\cdot)}_{2q}$.}\label{alg:UKF}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0, \vec{a}_0,$ $\mat{X}_1,\dots,\mat{X}_d,$ $\vec{y}_1,\dots,\vec{y}_d,$ $R_1,\dots,R_d$
\Statex Hyperparameters $\alpha$, $\beta$ and $\kappa$
\State Set $\emNotee{\vec{a}}{0}{0} = \vec{a}_0$ and $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$
\Statex Compute \emph{sigma weights} with $\lambda = \alpha^2 (q + \kappa) - q$
\State $W_0^{[m]} = \frac{\lambda}{q + \lambda}$\label{alg:UKF:weightsSta}
\State $W_0^{[c]} = \frac{\lambda}{q + \lambda} + 1 - \alpha^2 + \beta$
\State $W_0^{[cc]} = \frac{\lambda}{q + \lambda} + 1 - \alpha$
\State $W_j^{[m]} = W_j^{[c]} = \frac{1}{2(q+\lambda)}, \qquad j = 1,\dots, 2q$\label{alg:UKF:weightsSto}
\For{$t=1,2,\dots,d$}
\Procedure{Prediction step}{}
\State $\emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}$
\State $\emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^\top + \mat{R}\mat{Q}\mat{R}^\top$
\EndProcedure
\Procedure{Correction step}{}
\StateXX Compute \emph{sigma points}
\State $\begin{aligned}
  &\hvec{a}_0 = \emNotee{\vec{a}}{t}{t-1} \\
  &\hvec{a}_{j} = \emNotee{\vec{a}}{t}{t-1} + \sqrt{q + \lambda} \left(\emNotee{\mat{V}}{t}{t - 1}^{1/2}\right)_j \\
  &\hvec{a}_{j + q} = \emNotee{\vec{a}}{t}{t - 1} - \sqrt{q + \lambda} \left(\emNotee{\mat{V}}{t}{t - 1}^{1/2}\right)_j
\end{aligned} \qquad j = 1,2,\dots, q$ \label{alg:UKF:points}
\StateXX Compute intermediates
\State $\hvec{y}_j = \vec{z}_t \left(\hvec{a}_j \right) \qquad j = 0,1,\dots, 2q$\label{alg:UKF:expecMean}
\State $\hmat{Y} = (\hvec{y}_0, \dots, \hvec{y}_{2q})$
\State $\overline{\vec{y}} = \sum_{j = 0}^{2q} W_j^{[m]} \vec{y}_j$\label{alg:UKF:mean}
\State $\Delta\hmat{Y} = \hmat{Y} - \overline{\vec{y}} \vec{1}^\top$
\State $\hmat{H} = \xi\mat{I} + \sum_{j=0}^{2q} W_j^{[c]}\mat{H}_t(\hvec{a}_j)$ \label{alg:UKF:obsCov}
\State $\Delta\hmat{A} = (\hvec{a}_0, \dots, \hvec{a}_{2q}) - \emNotee{\vec{a}}{t}{t-1}\vec{1}^\top$
\State $\tvec{y} = \Delta \hmat{Y}^\top \hmat{H}^{-1}(\vec{y}_t - \overline{\vec{y}})$\label{alg:UKF:residual}
\State $\mat{G} = \Delta\hmat{Y}^\top\hmat{H}^{-1}\Delta\hmat{Y}$\label{alg:UKF:G}
\State $\vec{c} = \tilde{\vec{y}} - \mat{G}\left( \diag{\vec{W}^{(m)}}^{-1} + \mat{G}\right)^{-1} \tvec{y}$ \label{alg:UKF:InterC}
\State $\mat{L} = \mat{G} - \mat{G}\left( \diag{\vec{W}^{(c)}}^{-1} + \mat{G}\right)^{-1} \mat{G}$ \label{alg:UKF:InterG}
\StateXX Compute updates
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1} + \Delta\hmat{A}\diag{\vec{W}^{(cc)}}\vec{c}$
\State $\emNotee{\mat{V}}{t}{t} = \emNotee{\mat{V}}{t}{t - 1} -
    \Delta\hmat{A}\diag{\vec{W}^{(cc)}}\mat{L}\diag{\vec{W}^{(cc)}}\Delta\hmat{A}^\top$
\EndProcedure
\EndFor
\end{algorithmic}
\end{algorithm}

\code{ddhazard} uses the Cholesky decomposition for the square root matrix $\displaystyle\emNotee{\mat{V}}{t}{t - 1}^{1/2}$. The hyperparameters on which the sigma points and sigma weights depend can have values $0 < \alpha \leq 1$, $\kappa\in\mathbb{R}$ and $\beta\in\mathbb{R}$ under the restriction that $q + \lambda = \alpha^2(q + \kappa) \geq 0$. I will show a small example to illustrate the intuition of the sigma points in~\citeAlgLine{alg:UKF:points}{alg:UKF} and sigma weights in~\citeAlgLineTo{alg:UKF:weightsSta}{alg:UKF:weightsSto}{alg:UKF}. Suppose we are at time $t$ of the correction step of the filter with with a two dimensional state equation, $q = 2$. Further, assume that we have

\begin{equation}\label{eqn:UKFEx}
\emNotee{\vec{a}}{t}{t - 1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \qquad
  \emNotee{\mat{V}}{t}{t - 1} = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}, \qquad
  \emNotee{\mat{V}}{t}{t - 1}^{1/2} = \begin{pmatrix} 1.41 & 0.707 \\ 0 & 0.707 \end{pmatrix}
\end{equation}

Then the following hyperparameters leads to the following weights:

\begin{equation}\label{eqn:UKFParamEx}
\begin{aligned}
  (\alpha,\beta,\kappa) = (1,0,1) &\quad \Rightarrow &&
    \quad \Lparen{W_0^{[m]}, W_1^{[m]}, \dots, W_{2q}^{[m]}} =
      (1/3,1/6,\dots,1/6) \\
  (\alpha,\beta,\kappa) = (1/3,0,1) &\quad \Rightarrow &&
    \quad \Lparen{W_0^{[m]}, W_1^{[m]}, \dots, W_{2q}^{[m]}}  =
     (-1,1/2,\dots,1/2) \\
\end{aligned}
\end{equation}

% CHECK: Parameters above match with code

<<sigma_pts,echo=FALSE, par_1x1=TRUE, fig.cap = "Illustration of sigma points in the example from Equation~\\eqref{eqn:UKFEx}. The dashed lines are the contours of the density given by $\\emNotee{\\vec{a}}{t}{t - 1}$ and $\\emNotee{\\mat{V}}{t}{t - 1}$. The full lines are the direction given by the columns of the Cholesky decomposition. The filled circles are sigma points with $(\\alpha,\\beta,\\kappa) = (1,0,1)$ and the open circles are the sigma points with $(\\alpha,\\beta,\\kappa) = (1/3,0,1)$. The point at $(0,0)$ is a sigma point for both sets for hyperparameters.",cache=FALSE>>=
#####
# Draw points
library("mvtnorm")
set.seed(7912351)
x.points <- seq(-3, 3,length.out=100)
y.points <- x.points
z <- matrix(0,nrow=100,ncol=100)
mu <- c(0,0)
sigma <- matrix(c(2,1,1,1),nrow=2)
for (i in 1:100) {for (j in 1:100) {
  z[i,j] <- dmvnorm(c(x.points[i],y.points[j]),
                    mean=mu,sigma=sigma)
}}

#####
# Plot contours
plot(c(-3, 3), c(-3, 3), xlab = "", ylab = "", type = "n",
     xlim = c(-3, 3), ylim = c(-3, 3))
contour(x.points, y.points, z, nlevels = 10,
        drawlabels = FALSE, axes = FALSE,
        frame.plot = FALSE, add = TRUE,
        lty = 3)

#####
# Compute Cholesky decomposition and add lines
decomp <- chol(sigma)
abline(h = 0)
abline(a = 0, b = decomp[1, 2] / decomp[2, 2])

#####
# Add two sets of sigma points
q <- 2
l1 <- 1
l2 <- -1

pts <- rbind(
  c(0, 0),
  sqrt(q + l1) * t(decomp),
  - sqrt(q + l1) * t(decomp),
  sqrt(q + l2) * t(decomp),
  - sqrt(q + l2) * t(decomp))

points(pts[, 1], pts[, 2],
       pch = c(rep(16, 5), rep(1, 4)), cex = par()$cex * 3)
@

Decreasing $\alpha$ increases the absolute size of the weights and can lead to a negative weight on the zero sigma point, $\hvec{a}_0$. $\alpha$ also controls the spread of the sigma points through the factor $\alpha \sqrt{q + \kappa}$ in~\citeAlgLine{alg:UKF:points}{alg:UKF}. Decreasing $\alpha$ decreases the spread of the sigma points, as figure~\ref{fig:sigma_pts} illustrates. The filled circles are the sigma points with $(\alpha,\beta,\kappa) = (1,0,1)$, and the open circles are the sigma points with $(\alpha,\beta,\kappa) = (1/3,0,1)$.

If $\alpha$ is small the weight of the zero sigma point can be negative, $W_0^{[m]}<0$. While it is not immediately clear from algorithm~\ref{alg:UKF}, this can cause computational issues, as~\cite{menegaz16} points out. This is easily seen in~\cite{Julier04} formulation of the UKF, where the covariance matrix of the observational equation can fail to be positive definite when the weight of the zero sigma point is less than zero (see the ddhazard vignette). Thus, we may select a specific value of $W_0^{[m]}>0$ by setting $\kappa = q (1 + \alpha^2 (W_0^{[m]} -1)) / (\alpha^2 (1 - W_0^{[m]}))$.

\code{ddhazard} uses the three hyperparameter UKF given by \cite{Wan00}. Many different UKFs have been suggested with different hyperparameters, algorithms, and sigma points (see \cite{menegaz16} for a comparison of different forms of UKFs in the literature). Algorithm~\ref{alg:UKF} is derived with the weight specification in~\cite{menegaz16}, the UKF from~\cite{Wan00}, and by applying the Woodbury matrix identity.

Algorithm~\ref{alg:UKF} involves at most products of $q\times n_t$ and $n_t \times q$ matrices, inversion of $q\times q$ matrices, and an inversion of $\hmat{H}$, which is a diagonal matrix that can be computed in $\bigO{n_t}$ time. Thus, the filter has a computational complexity of $\bigO{n_tq^2 + q^3}$. The results are not computed in parallel in the current of \code{ddhazard}. I add the identity matrix times $\xi$ to reduce the effect of observation predicted near the boundary of the outcome space in~\citeAlgLine{alg:UKF:obsCov}{alg:UKF} as in the EKF. I will end this section on the UKF with an example.

\subsubsection{Example with UKF}
<<define_ekf_low_Q_0, echo = FALSE>>=
ukf_ex_Q_0 <- 0.1
@


One problem with the UKF compared with the EKF is its greater sensitivity to the choice of $\mat{Q}_0$. The reason is that $\mat{Q}_0$ is used in~\citeAlgLine{alg:UKF:points}{alg:UKF} to compute the first set of sigma points at time $t=1$. I will illustrate this in the following paragraphs. I fit the model below and plot the predicted parameters. I set $\mat{Q}_0$ to a diagonal matrix but with larger entries than before. I specify that I want the UKF by setting the element \code{method = "UKF"} in the list to the \code{control} argument of \code{ddhazard}. Figure~\ref{fig:ukf_large_Q_0} shows the result. Figure~\ref{fig:ukf_small_Q_0} shows the same model but with $\mat{Q}_0$'s diagonal entries equal to $\Sexpr{ukf_ex_Q_0}$. The code is omitted for brevity. The latter figure is comparable to what we have seen previously. My experience is that we need to select a matrix that has \emph{large} but not \emph{too large} elements in the diagonal. This is consistent with the findings in \cite{Xiong06}, although their setting is different from the settings here.

% CHECK: arguments match
% CHECK: claims above is correct

<<ukf_large_Q_0_comp>>=
ddfit_ukf <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(10, 24 - 1), # Larger value
  Q = diag(.01, 24 - 1),
  by = 1,
  max_T = 60,
  control = list(
    method = "UKF",       # Use the UKF
    eps = .01))           # Decreased to get a fit
@

<<ukf_large_Q_0_change_fac_names, echo = FALSE>>=
ddfit_ukf <- get_pretty_model_factors(ddfit_ukf)
@


<<ukf_large_Q_0, par_3x3=TRUE, fig.cap = "Predicted parameters with the UKF used on the hard disk failure dataset where $\\mat{Q}_0$ has large entries in the diagonal.",cache=FALSE>>=
# Shown in figure \ref&fig:ukf_large_Q_0;
plot(ddfit_ukf, cov_index = 1:9)
@

<<ukf_small_Q_0_est, echo = FALSE>>=
#####
# Re-fit with lower Q_0 entries
new_call <- ddfit_ukf$call
new_call$Q_0 <- diag(ukf_ex_Q_0, 23)
ctrl <- eval(new_call$control)
ctrl$eps <- .001
new_call$control <- ctrl
ddfit_ukf <- eval(new_call)
@

<<ukf_small_Q_0, par_3x3=TRUE, echo = FALSE,fig.cap = paste0("Similar plot to figure~\\ref{fig:ukf_large_Q_0} but where the diagonal entries of $\\mat{Q}_0$ are $", ukf_ex_Q_0, "$. The black curve is the estimates from the the EKF with one iteration in the correction step.", fig.cap_bar_expla),cache=FALSE>>=
for(i in 1:9){
  plot(ddfit, cov_index = i)
  plot(ddfit_ukf, cov_index = i, add = TRUE, col = "red")
  add_hist(i)
}
@

In contrast, the EKF with one iteration in the correction step can have large entries in the diagonal of $\mat{Q}_0$ as in the examples in~\cite{Fahrmeir92}. I illustrate this by refitting the \code{ddfit} object using the \code{call} element of the object after altering the \code{Q_0} argument:

<<show_EKF_can_have_large_comp>>=
new_call <- ddfit$call
new_call$Q_0 <- diag(100000000, 23)
ddfit_large_Q_0 <- eval(new_call)
@

<<show_EKF_can_have_large_change_fac_names, echo = FALSE>>=
ddfit_large_Q_0 <- get_pretty_model_factors(ddfit_large_Q_0)
@

<<show_EKF_can_have_large, par_3x3=TRUE, fig.cap = paste0("Predicted parameters with the EKF with one iteration in the correction used on the hard disk failure dataset with $\\mat{Q}_0$ has low entries (", ddfit$Q_0[1,1], ") and large entries (", ddfit_large_Q_0$Q_0[1,1], ") in the diagonal. The red curves are the estimates with large entries in the diagonal.", fig.cap_bar_expla),cache=FALSE>>=
for(i in 1:9){
  # Shown in figure \ref&fig:show_EKF_can_have_large;
  plot(ddfit_large_Q_0, cov_index = i, col = "red")
  plot(ddfit, cov_index = i, add = TRUE)
  add_hist(i)
}
@

% CHECK: The claim below is true

The plot is shown in figure~\ref{fig:show_EKF_can_have_large}. The parameters are close for the parts where we have data, as we can see this in table~\ref{tab:modelDat} and figure~\ref{fig:show_EKF_can_have_large}.

<<ukf_cleanup, echo = FALSE>>=
rm(ddfit_large_Q_0, ddfit_ukf)
@

\subsection{Sequential approximation of the posterior modes}\label{subsec:postApprox}
Another idea is to replace the means in the filters with the modes in each correction step. That is, we are still looking for a method for~\citeAlgLine{alg:EM:filter}{alg:EM}. We perform the same prediction step as with the EKF and UKF, although we change the correction step from finding the mean to finding the mode. In making this replacement, we need must the minimum of Equation~\eqref{eqn:modeExact}, followed by an update of the covariance matrix.
\begin{equation}\label{eqn:modeExact}
\emNotee{\vec{a}}{t}{t} = \argminu{\vec{\alpha}}
  - \log \proppCond{\vec{\alpha}}{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
    -\sum_{i \in R_t} \log\proppCond{y_{it}}{\vec{\alpha}}
\end{equation}

One way of finding an approximate minimum is to replace Equation~\eqref{eqn:modeExact} with $n_t = \vert R_t \vert$ rank-one updates of the form in Equation~\eqref{eqn:modeApprox} and an update of the covariance matrix. I will use a superscript to indicate the previous result from the rank-one update.

\begin{equation}\label{eqn:modeApprox}
\emNote{\vec{a}}{t}{t}{k} = \argminu{\vec{\alpha}}
  -\log \proppCond{\vec{\alpha}}{\emNote{\vec{a}}{t}{t}{k-1}, \emNote{\mat{V}}{t}{t}{k-1}}
  -\log\proppCond{y_{it}}{\vec{\alpha}}
\end{equation}

I will refer to this method as the sequential mode approximation (SMA). Algorithm~\ref{alg:approxMode} and~\ref{alg:approxModeChol} accomplish this. The latter replaces the correction step in the former by propagating a Cholesky decomposition of the information matrix but is otherwise identical. The advantage of using the Cholesky decomposition in algorithm~\ref{alg:approxModeChol} is that we ensure that the covariance matrix is positive semi-definite. \CiteAlgLine{alg:approxModeChol:LInvUpdate}{alg:approxModeChol} can be inverted in $\bigO{q^2}$ because $\mat{L}$ is a triangular matrix. Further, the rank-one update of the Cholesky decomposition in~\citeAlgLine{alg:approxModeChol:LUpdate}{alg:approxModeChol} is done in~$\bigO{q^2}$. Lastly, I exploit that $\tmat{L}$ is a triangular matrix to reduce the computational cost of the matrix and vector products. However, the correction step with algorithm~\ref{alg:approxModeChol} is slower. The Newton-Raphson method is used to find the constant $v$ in \citeAlgLine{alg:approxMode:findConst}{alg:approxMode} and \citeAlgLine{alg:approxModeChol:findConst}{alg:approxModeChol}. $\proppCond{y_{it}}{\vec{\alpha}_t^\top \vec{x}_{it} = b}$ is log-concave in $b$ with a finite upper bound for the models currently implemented in \code{ddhazard} so the Newton-Raphson method finds the unique minimum. Both methods are $\bigO{n_tq^2}$ in each prediction and correction step.

\begin{algorithm}
\caption{Sequential approximation of the posterior mode. Left-arrow, $\leftarrow$, indicates an update instead of an equality.}\label{alg:approxMode}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0, \vec{a}_0,$ $\mat{X}_1,\dots,\mat{X}_d,$ $\vec{y}_1,\dots,\vec{y}_d,$ $R_1,\dots,R_d$
\Statex Learning rate $\zeta_0$
\Statex Set $\emNotee{\vec{a}}{0}{0} = \vec{a}_0$ and $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$
\For{$t=1,2,\dots,d$}
\Procedure{Prediction step}{}
\State $\emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}$
\State $\emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^\top + \mat{R}\mat{Q}\mat{R}^\top$
\EndProcedure
\Procedure{Correction step}{} \label{alg:approxMode:correction}
\StateXX Set $\emNote{\mat{V}}{t}{t}{0} = \emNotee{\mat{V}}{t}{t - 1}$, $k = 0$ and $\emNote{\vec{a}}{t}{t}{0} = \emNotee{\vec{a}}{t}{t - 1}$
\For{$i \in R_t$}
\State $k  \leftarrow k + 1$
\State $d_1 = \frac{1}{\vec{x}_{it}^\top \emNote{\mat{V}}{t}{t}{k-1}\vec{x}_{it}}$
\State $d_2 = \vec{x}_{it}^\top \emNote{\vec{a}}{t}{t}{k-1}$
\State $ v = \argminu{b} b^2 \frac{1}{2}d_1 - b d_1d_2 - \log\proppCond{y_{it}}{\vec{\alpha}_t^\top \vec{x}_{it} = b}$\label{alg:approxMode:findConst}
\State $g =  -\left. \frac{\log \proppCond{y_{it}}{\vec{x}_{it}^\top \vec{\alpha} = b}}{\partial b^2} \right|_{b = v}$
\State $\emNote{\vec{a}}{t}{t}{k} = \emNote{\vec{a}}{t}{t}{k-1} - (d_1 - v) d_2 \zeta_0 \emNote{\mat{V}}{t}{t}{k-1}\vec{x}_{it}$
\State $\emNote{\mat{V}}{t}{t}{k} = \emNote{\mat{V}}{t}{t}{k-1} - \frac{\emNote{\mat{V}}{t}{t}{k-1}\vec{x}_{it} g \vec{x}_{it}^\top\emNote{\mat{V}}{t}{t}{k-1}}{1 + g / d_1}$
\EndFor
\StateXX Set $\emNotee{\vec{a}}{t}{t} = \emNote{\vec{a}}{t}{t}{n_t}$ and $\emNotee{\mat{V}}{t}{t} = \emNote{\mat{V}}{t}{t}{n_t}$
\EndProcedure
\EndFor
\end{algorithmic}
\end{algorithm}

A disadvantage of SMA is that it is sequential and all matrix and vector products are in dimension $q\times q$ and $q$. Thus, doing the computations in parallel unless $q$ is large yields little gain if any. Moreover, the results depends on the order of the risk set. For this reason, the risk sets are permuted once before running the algorithm. This can be avoided by setting \code{permu = FALSE} to the \code{control} argument of \code{ddhazard}. One advantage is that we do not need to compute an expected value in each interval ($h(\eta)$ in~\citeAlgLine{alg:EKF:scoreVec}{alg:EKF} and $\overline{\vec{y}}$ in~\citeAlgLine{alg:UKF:mean}{alg:UKF}) with the SMA but instead work with the likelihood. The latter is particularly useful for the continuous time model I cover in section~\ref{subsec:contTime}, as I avoid the definition of an outcome variable.

\begin{algorithm}
\caption{Alternative correction step in the procedure at~\citeAlgLine{alg:approxMode:correction}{alg:approxMode} with a Cholesky decomposition. Left-arrow, $\leftarrow$, indicates an update instead of an equality.}\label{alg:approxModeChol}
\begin{algorithmic}[1]\raggedright
\State Compute the Cholesky decomposition $\mat{L}\mat{L}^\top = \Lparen{\emNotee{\mat{V}}{t}{t-1}}^{-1}$ and $\tmat{L}=\Lparen{\mat{L}^{-1}}^\top$\label{alg:approxMode:setup}
\For{$i \in R_t $}
\State $k \leftarrow k + 1$
\State $\tvec{x}_{it} = \tmat{L}^\top\vec{x}_{it}$
\State $d_1 = \frac{1}{\tvec{x}_{it}^\top \tvec{x}_{it}}$
\State $d_2 = \vec{x}_{it}^\top \emNote{\vec{a}}{t}{t}{k-1}$
\State $v = \argminu{b} b^2 \frac{1}{2}d_1 - b d_1d_2 - \log\proppCond{y_{it}}{\vec{\alpha}_t^\top \vec{x}_{it} = b}$\label{alg:approxModeChol:findConst}
\State $g =  -\left. \frac{\log \proppCond{y_{it}}{\vec{x}_{it}^\top \vec{\alpha} = b}}{\partial b^2} \right|_{b = v}$
\State $\emNote{\vec{a}}{t}{t}{k} = \emNote{\vec{a}}{t}{t}{k-1} - (d_1 - v) d_2 \zeta_0 \tmat{L}\tvec{x}_{it}$
\State $\mat{L}\mat{L}^\top \leftarrow \mat{L}\mat{L}^\top + \vec{x}_{it} g \vec{x}_{it}^\top$\label{alg:approxModeChol:LUpdate}
\State $\tmat{L}=\Lparen{\mat{L}^{-1}}^\top$\label{alg:approxModeChol:LInvUpdate}
\EndFor
\Statex Set $\emNotee{\vec{a}}{t}{t} = \emNote{\vec{a}}{t}{t}{n_t}$ and $\emNotee{\mat{V}}{t}{t} = \tmat{L}\tmat{L}^\top$
\end{algorithmic}
\end{algorithm}

\subsection{Global mode approximation}\label{subsec:GMA}
We can also minimize Equation~\eqref{eqn:modeExact} directly. I denote this as the global mode approximation (GMA). This is equivalent to an L2 penalized generalized linear model (GLM) in every iteration, because we use only models from the exponential family. This can be done with the usual iteratively reweighed ridge regression. Every iteration can be done in $\bigO{n_tq^2 + q^3}$. I will show the estimation in the following paragraphs with a Newton method. First, let

\begin{equation}
\tilde h(\vec{\alpha}) =  - \log \proppCond{\vec{\alpha}}{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
    -\sum_{i \in R_t} \log\proppCond{y_{it}}{\vec{\alpha}}
\end{equation}

Next, denote the gradient by $\tvec{g}(\vec{\alpha})$ and the Hessian by $\tmat{G}$. They are given by

\begin{equation}\begin{aligned}
	 &\tilde h(\vec{\alpha}) =  - \log \proppCond{\vec{\alpha}}{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
    -\sum_{i \in R_t} \log\proppCond{y_{it}}{\vec{\alpha}} \\
%
	&\begin{aligned}
	\tvec{g}(\vec{\alpha}) =  \tilde h'(\vec{\alpha})
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} \Lparen{\vec{\alpha} - \emNotee{\vec{a}}{t}{t-1}}
      - \left. \sum_{i \in R_t} \frac{\partial\log\proppCond{y_{it}}{\vec{\alpha}'}}{\partial \vec{\alpha}'} \right|_{\vec{\alpha}' = \vec{\alpha}} \\
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} \Lparen{\vec{\alpha} - \emNotee{\vec{a}}{t}{t-1}}
		  - \mat{X}_t^\top \underbrace{\algGMApPrime}_{c'(\vec{\alpha})} \\
%
	\tmat{G}(\vec{\alpha}) = \tilde h''(\vec{\alpha})
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} - \left. \sum_{i \in R_t} \frac{\partial^2\log\proppCond{y_{it}}{\vec{\alpha}'}}{\partial \vec{\alpha}'\partial \Lparen{\vec{\alpha}'}^\top} \right|_{\vec{\alpha}' = \vec{\alpha}} \\
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} - \mat{X}_t^\top \underbrace{\algGMApPrimePrime}_{c''(\vec{\alpha})}\mat{X}_t
\end{aligned}\end{aligned}\end{equation}

Thus, the update equation is
\begin{equation}
\begin{aligned}
\vec{a}^{(k)} &= \vec{a}^{(k - 1)} + \zeta_0 \Lparen{\tmat{G}(\vec{a}^{(k - 1)})}^{-1}\Lparen{-\tmat{g}(\vec{a}^{(k - 1)})} \\
%	&= \Lparen{\tmat{G}(\vec{a}^{(k - 1)})}^{-1}\Lparen{
%		- \mat{X}_t^\top  c''(\vec{\alpha}^{(k-1)}) \mat{X}_t  \vec{a}^{(k - 1)}
%		+ (1-\zeta_0)   \emNotee{\mat{V}}{t}{t-1}^{-1} \vec{a}^{(k-1)}
%		+ \emNotee{\mat{V}}{t}{t-1}^{-1} \emNotee{\vec{a}}{t}{t-1}
%		+ \zeta_0 \mat{X}_t^\top c'(\vec{\alpha}^{(k-1)})} \\
	& \algGMAscore{\ =}
\end{aligned}
\end{equation}

Algorithm~\ref{alg:GlobalMA} shows the final algorithm for the correction step with the GMA.

\begin{algorithm}
\caption{Correction step with global mode approximation by Newton Raphson.}\label{alg:GlobalMA}
\begin{algorithmic}[1]\raggedright
\Statex Set $\vec{a}^{(0)} =\emNotee{\vec{a}}{t}{t-1}$ and define:
\Statex $c'(\vec{\alpha}) = \algGMApPrime$
\Statex $c''(\vec{\alpha}) = \algGMApPrimePrime$
\Repeat
\State %
$\algGMAscore{\vec{a}^{(k)} =}$\label{alg:GlobalMA:Update}
\UntilElse{$\LVert{\vec{a}^{(k)} - \vec{a}^{(k-1)}}/ (\LVert{\vec{a}^{(k-1)}} + \delta) < \epsilon$}{Set $k\leftarrow k + 1$}
\State $\emNotee{\mat{V}}{t}{t} = \Lparen{\tmat{G}(\vec{a}^{(k - 1)})}^{-1}$
\end{algorithmic}
\end{algorithm}

An alternative to algorithm~\ref{alg:GlobalMA} for the exponential family is to rewrite the original problem to use working responses to get a weighted least squares problem of the form

\begin{equation}\label{eqn:GMAAlt}
\begin{aligned}
&\vec{b} = \mat{X}_t  \vec{a}^{(k-1)} + \vec{h}'\Lparen{\mat{X}_t  \vec{a}^{(k-1)}}^{-1}\Lparen{\vec{y}_t - \vec{h}\Lparen{\mat{X}_t  \vec{a}^{(k-1)}}}   \\
%
&\vphantom{\LVert{\underbrace{\eqnGblModeTerma}_{\tmat{W}}}}
%
\argmin_{\vec{\alpha}}\LVert{
\smash{\underbrace{\eqnGblModeTerma}_{\tmat{C}^{1/2}}}\vphantom{\eqnGblModeTerma}
 \Lparen{
  \smash{\underbrace{\eqnGblModeTermb}_{\tmat{X}_t}}\vphantom{\eqnGblModeTermb} \vec{\alpha}
  - \smash{\underbrace{\eqnGblModeTermc}_{\tvec{b}}}\vphantom{\eqnGblModeTermc}}}
\end{aligned}\end{equation}%
%
where $\vec{b}$ is the working responses, $\vec{h}$ temporarily denotes the inverse link function, and the derivative $\vec{h}'$ is with respect to the state vector $\vec{\alpha}$. The inverse link function $\vec{h}$ implicitly depends on the risk set at time $t$. I show in the ddhazard vignette that one iteration of solving~\eqref{eqn:GMAAlt} is equivalent to the EKF with a single iteration in the correction step.

Algorithm~\ref{alg:GlobalMA} is sensitive to the choice of $\mat{Q}_0$. In an extreme example, we would have no events in the first interval and only an intercept. Setting $\mat{Q}_0$ to a diagonal matrix with large entries (in this case $\mat{Q}_0$ is a scalar) implies almost no restrictions on the intercept. Thus, selecting a value tending towards minus infinity would be optimal. Only $c''(\vec{\alpha})$ and $c'(\vec{\alpha})$ in algorithm~\ref{alg:GlobalMA} are computed in parallel with \pkg{OpenMP}. Building with a multithreaded \pkg{BLAS} can decrease the computation time of $\mat{X}_t^\top c''(\vec{\alpha}) \mat{X}_t$ along with the other matrix and vector products.

The global mode approximation is somewhat similar that in~\citet[Section 10.6]{durbin12}. The major difference is that \cite{durbin12} make the Taylor expansion \emph{before} running the filter using the current estimate of $\vec{\alpha}_0,\vec{\alpha}_1,\dots,\vec{\alpha}_d$. In contrast, the GMA method makes the expansion at each correction step \emph{within} the filter using the current estimate of $\emNotee{\vec{a}}{t}{t-1}$. The package \pkg{KFAS} implements the method in~\citet[Section 10.6]{durbin12}. They further ease the computation in \pkg{KFAS} by using the sequential method for the correction step for the Kalman filter described in~\cite{Koopman00}. % Lastly, if the negative value under the minimization operator in Equation~\eqref{eqn:modeExact} is not log-concave then the global instead of sequential optimization can be done by the second method in~\cite{Durbin00} or by the methods in~\cite{So03}. Similar changes can be made to the GMA method presented here.

\subsubsection{Examples with SMA and GMA}
I will use the hard disk failures data set to compare the SMA and GMA methods with the EKF with a single iteration in the correction step. I illustrate this below by estimating the model with the SMA method, GMA method and EKF method. I use the correction step with the Cholesky decomposition in algorithm~\ref{alg:approxModeChol} by setting the \code{posterior_version = "cholesky"} in the list passed to the \code{control} argument.

<<echo=FALSE, cache = FALSE>>=
set.seed(914587)
@

% CHECK: arguments match

<<fit_SMA_hd_fail>>=
ddfit_SMA <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  by = 1,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(1, 23),
  Q = diag(0.01, 23),
  control = list(
    eps = 0.001,
    method = "SMA",                  # Use SMA
    posterior_version = "cholesky")) # The Cholesky method in algorithm \ref&alg:approxModeChol;
@

% CHECK: arguments match

<<fit_GMA_hd_fail>>=
ddfit_GMA <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  by = 1,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(1, 23),
  Q = diag(0.01, 23),
  control = list(
    eps = 0.001,
    method = "GMA"))                 # Use GMA instead
@

Figure~\ref{fig:EKF_vs_SMA_n_GMA} shows the three sets of predicted parameters where the black lines are the EKF parameters, the gray lines are the SMA parameters and the red lines are the GMA parameters. The parameters in figure~\ref{fig:EKF_vs_SMA_n_GMA} appear similar. The SMA can have large entries in the diagonal of $\mat{Q}_0$ like those in the EKF with one iteration, as the following fit shows:

<<EKF_vs_SMA_n_GMA, par_3x3 = TRUE, fig.cap= paste0("Predicted parameters using the EKF, the GMA and the SMA for the hard disk failure data set. The gray lines are the parameters from the SMA, red lines are parameters from the GMA and the black lines are the parameters form the EKF.", fig.cap_bar_expla), echo = FALSE, cache = FALSE>>=
for(i in 1:9){
  plot(ddfit, cov_index = i)
  plot(ddfit_SMA, cov_index = i, col = "gray40", add = T)
  plot(ddfit_GMA, cov_index = i, col = "red", add = T)
  add_hist(i)
}
@

<<SMA_seed_large_Q0, echo=FALSE, cache = FALSE>>=
set.seed(914587)
@

<<SMA_w_large_Q_0>>=
new_call <- ddfit_SMA$call
new_call$Q_0 <- diag(100000000, 23)
ddfit_SMA_large_Q_0 <- eval(new_call)
@

<<SMA_w_large_Q_0_plot, par_3x3=TRUE, fig.cap = paste0("Similar plot to figure~\\ref{fig:show_EKF_can_have_large} but with the SMA instead. The red curves are the estimates with the SMA with large entries in the diagonal of $\\mat{Q}_0$. The black curves are the same fit using the EKF as in figure~\\ref{fig:show_EKF_can_have_large}.", fig.cap_bar_expla), echo = FALSE>>=
ddfit_SMA_large_Q_0 <- get_pretty_model_factors(ddfit_SMA_large_Q_0)
for(i in 1:9){
  # Shown in figure \ref&fig:SMA_w_large_Q_0_plot;
  plot(ddfit_SMA_large_Q_0, cov_index = i, col = "red")
  plot(ddfit, cov_index = i, add = TRUE)
  add_hist(i)
}
@

Figure~\ref{fig:SMA_w_large_Q_0_plot} shows the first set of predicted parameters.

<<SMA_n_GMA_cleanup, echo = FALSE>>=
rm(ddfit_SMA_large_Q_0, ddfit_SMA, ddfit_GMA)
@


\subsection{Summary on filters}
All the filters have computational complexity $\bigO{n_t}$. Thus, the final EM-algorithm in algorithm~\ref{alg:EM} is $\bigO{n_t}$ as the computation time of the specified parts of~\ref{alg:EM} is independent of $n_t$. I summarize the pros and cons of the EKF, UKF, SMA, and GMA in tables~\ref{tab:proConsEKF},~\ref{tab:proConsUKF},~\ref{tab:proConsSMA}, and~\ref{tab:proConsGMA}, respectively. Some of the points assume that the size of the risk set, $n_t$, is much greater than the dimension of the state vector, $q$.

\WiTbl{
\textbf{Pros of EKF} & \\
\hline
\embParallel{}. \\
\hline
 \notQsens{} \\
\textbf{Cons of EKF} & \\
\hline
Linearisation & The linearisation may be a poor approximation.
}{The pros and cons for the EKF with \emph{one iteration} in the correction step.\label{tab:proConsEKF}}

\WiTbl{
\textbf{Pros of UKF} & \\
\hline
Approximation & Potentially better approximation than the EKF. \\
\hline
Parallel & The matrix and vector products can be computed in parallel with a multithreaded \pkg{BLAS}. Moreover, parts of the computations could be computed in parallel although the current implementation does not do this.  \\
\textbf{Cons of UKF} & \\
\hline
\Qsens{} \\
\hline
Hyperparameters & Additional hyperparameters $(\alpha,\beta,\kappa)$ must be specified. My experience is that $(\alpha,\beta) = (1,0)$ tends to do well with $\kappa > 0$ to ensure a positive weight on the zero-ith sigma point.
}{The pros and cons for the UKF.\label{tab:proConsUKF}}


\WiTbl{
\textbf{Pros of SMA} & \\
\hline
  \prosLikelihood{} \\
\hline
  \notQsens{} \\
\textbf{Cons of SMA} & \\
\hline
  Sequential & The updates are sequential and thus do not benefit from a parallel implementation. \\ \hline
  Ordering & The final outcome will depend on the order of the risk sets.
}{The pros and cons for the SMA.\label{tab:proConsSMA}}

\WiTbl{
\textbf{Pros of GMA} & \\
\hline
  \prosLikelihood{} \\
\hline
\embParallel{}, although this is it is only partly implemented. The matrix and vector products can be computed in parallel with a multithreaded \pkg{BLAS}. \\
\textbf{Cons of GMA} & \\
\hline \Qsens{}
}{The pros and cons for the GMA. The EKF with more iteration in the correction step is similar as shown in the ddhazard vignette. Thus, some of the points also apply to algorithm~\ref{alg:EKFextra}.\label{tab:proConsGMA}}

\subsection{Constant effects}
In some applications, constant (time-invariant) parameters may be relevant. Two methods are implemented to estimate such parameters: one that makes the estimation in the E-step and one that estimates the parameters by a zero-order Taylor expansion in the M-step.

\subsubsection{E-step method}
A common way of estimating fixed parameters in filtering (see e.g., \cite{Harvey79}) is to set the entries of the rows and columns of $\mat{Q}$ for the fixed parameters to zero and the corresponding diagonal entries of $\mat{Q}_0$ to large values. This approach is also used by~\cite{Fahrmeir92} with the EKF.

\subsubsection{M-step method}
The other method is to estimate the fixed parameters in the M-step. For this section, I define $\vec{\gamma}$ as the fixed parameters, $\vec{x}_{(\alpha)it}$ as the covariates corresponding to the time-varying parameters, and $\vec{x}_{(\gamma)it}$ as the covariates corresponding to the fixed parameters. The fixed parameters multiplied by the corresponding covariates act as offsets in the filters because the linear predictor is $\vec{x}_{(\alpha)it}^\top \vec{\alpha}_t + \vec{x}_{(\gamma)it}^\top \vec{\gamma}$, where $\vec{\gamma}$ is fixed. Moreover, the formulas for $\vec{a}_0$ and $\mat{Q}$ are in the M-step are not affected because the only relevant terms for fixed effects in the M-step is the last line of the log likelihood in Equation~\eqref{eqn:logLikeFirst}. However, the optimization is not easily solved exactly in the M-step for the fixed parameters. The log likelihood we need to maximize in the M-step is

\begin{equation}\label{eqn:fixedWithExpec}
\argmaxu{\vec{\gamma}}\expecpCond{
  \sum_{t = 1}^d \sum_{i \in R_t} l_{it}({\vec{\alpha}_t}, \vec{\gamma})}{
\emNotee{\vec{a}}{1}{d}, \emNotee{\mat{V}}{1}{d}, \vec{y}_1, \dots,
\emNotee{\vec{a}}{d}{d}, \emNotee{\mat{V}}{d}{d}, \vec{y}_d}
\end{equation}

where I temporarily add an additional argument in the log likelihood terms, $l_{it}$, for the fixed effects. Equation~\eqref{eqn:fixedWithExpec} is not easy to optimize. What further complicates it is that $\emNotee{\vec{a}}{0}{d}, \emNotee{\mat{V}}{0}{d}, \dots,
\emNotee{\vec{a}}{d}{d}, \emNotee{\mat{V}}{d}{d}$ are modes and not means when we use the EKF, SMA, or GMA. I make a zero-order Taylor expansion around the $\emNotee{\vec{a}}{1}{d}, \dots, \emNotee{\vec{a}}{d}{d}$ in the current implementation to get

\begin{equation}
\begin{aligned}
  \argmaxu{\vec{\gamma}}\expecpCond{
  \sum_{t = 1}^d \sum_{i \in R_t} l_{it}({\vec{\alpha}_t}, \vec{\gamma})}{
\emNotee{\vec{a}}{1}{d}, \emNotee{\mat{V}}{1}{d}, \vec{y}_1 \dots,
\emNotee{\vec{a}}{d}{d}, \emNotee{\mat{V}}{d}{d}, \vec{y}_d} \hspace{-120pt}& \\
  & \approx \argmaxu{\vec{\gamma}} \sum_{t = 1}^d \sum_{i \in R_t} l_{it}(\emNotee{\vec{a}}{t}{d}, \vec{\gamma})
\end{aligned}
\end{equation}

which is also a first-order expansion, as the first order term cancels out. One advantage of doing this is that the problem can be solved with regular methods for GLMs when the model is from the exponential family. However, the design matrix will be big, as each individual will yield multiple rows because of different offsets from the time-varying parameters given by $\vec{x}_{(\alpha)it}^\top \emNotee{\vec{a}}{t}{d}$. To overcome this problem, I use the same \proglang{Fortran} code  from~\cite{Miller92} to do a series of rank-one updates of the QR-decomposition to solve the iteratively re-weighted least squares. This is the same approach as in the package \pkg{biglm}~\citep{biglm}. The computational complexity of each update is $\bigO{c^2}$, where $c$ is the dimension of $\vec{\gamma}$.

\subsection{Second-order random walk}
I will end this part of the paper by using the E-step method for estimating fixed parameters. Further, I will illustrate the use of the second-order random walk. I estimate the model below where the factor levels for the hard disk version follow a second-order random walk and the spline for the SMART 12 attribute is fixed. I get the second-order random walk for the factor levels by setting the argument \code{order = 2}. I specify which terms are fixed by wrapping the terms in the formula the \code{ddFixed} function. The fixed effect estimation method is selected as the E-step method by setting \code{fixed_terms_method = "E_step"} in the list passed to the \code{control} argument. To avoid divergence, I decrease the learning rate by setting the \code{LR} element in the list passed to the \code{control} argument.

% CHECK: Arguments for spline match. Make sure the argument in the two calls match

<<order_2_est>>=
# Define new formula
frm_fixed <-
  Surv(tstart, tstop, fails) ~ -1 + model +
  ddFixed(ns(smart_12, knots = c(3, (1:2) * 10), Boundary.knots = c(0, 30)))

ddfit_fixed_E <- ddhazard(
  formula = frm_fixed,
  data = hd_dat_sub,
  by = 1,
  order = 2,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(10, 32),
  Q = diag(0.01, 16),
  control = list(
    method = "GMA",
    NR_eps = .001,
    eps = 0.001,
    LR = .2,
    fixed_terms_method = "E_step")) # Use E-step method
@


<<order_2_plot, echo = FALSE, par_3x3 = TRUE, fig.cap=paste0("Predicted parameters for some of the factor levels with the second order random walk. The red lines are parameters with fixed effects estimated in the E-step and the black lines is the first order random walk where all parameters are time-varying.", fig.cap_bar_expla),cache=FALSE>>=
ddfit_fixed_E <- get_pretty_model_factors(ddfit_fixed_E)

for(i in 1:9){
  # Shown in figure \ref&fig:order_2_plot;
  plot(ddfit_fixed_E, cov_index = i, col = "red")
  plot(ddfit, cov_index = i, add = TRUE)
  add_hist(i)
}
@

<<order_2_plot_fixed, echo = FALSE, par_1x1 = TRUE, fig.cap="Fixed effects estimates for the SMART 12 attribute on the linear predictor scale using the E-step method.",cache=FALSE>>=
expr <- tail(colnames(attr(terms(ddfit_fixed_E$formula), "factors")), 1)
x <- with(
  list(smart_12 = (x_org <- seq(0, 70, length.out = 1000))),
  eval(parse(text = expr)))

spline_E <- drop(x %*% ddfit_fixed_E$fixed_effects)

plot(x_org, spline_E,
     type = "l", xlab = "SMART 12 attribute", ylab = "Linear predictor term")
@


Figure~\ref{fig:order_2_plot} shows the predicted factor levels for the hard disk version, and figure~\ref{fig:order_2_plot_fixed} plots the spline estimate. The spline estimate seems consistent with figure~\ref{fig:subset_smart_12_plot}. A steeper increase is followed by a more gradual increase. I also tried the M-step estimation method. It gave quite a different result for the data set that did not seems sensible. An explanation for the difference between the methods could be the time-dependence of the SMART 12 attribute values shown in figure~\ref{fig:smart_12_through_time}. In my experience, the difference between the two methods of estimating fixed effects is not as big as in this example in most cases. For example, you can run the Shiny app with the package by calling \code{ddhazard_app}. This provides a simulation example where, among other things, you can estimate with the two methods for fixed effects.
<<smart_12_through_time, echo = FALSE, par_1x1 = TRUE, fig.cap="Quantiles of the SMART 12 attribute through time. The quantiles are at 5\\%, 10\\%, \\ldots, 95\\%. This is the data set without the ST3000DM001 version.",cache=FALSE>>=
#####
# Find quantiles through time
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ smart_12,
  data = hd_dat_sub, by = 1, max_T = 60, use_weights = F,
  id = hd_dat_sub$serial_number)
quants <- tapply(
  tmp_dat$X$smart_12, tmp_dat$X$t, quantile, probs = (1:19)/20)
quants <- do.call(cbind, quants)

#####
# Plot
x <- as.numeric(colnames(quants))
plot(range(x), range(quants), type = "n", xlab = "Month", ylab = "SMART 12 attribute")
col <- rgb(0, 0, 0, alpha = .1)
for(i in 1:floor(nrow(quants) / 2)){
  lb <- quants[i, ]
  ub <- quants[nrow(quants) - (i - 1), ]
  polygon(c(x, rev(x)), c(ub, rev(lb)), col = col, border = NA)
}

#####
# Cleanup
rm(tmp_dat)
@


\section{Continuous time model}\label{sec:mod}

\subsection{Dynamic discrete time model}\label{subsec:logi}
The dynamic discrete time model is where we use the log likelihood terms, $l_{it}(\vec{\alpha}_t)$, as shown in Equation~\eqref{eqn:binModelLikeliFirst} where $h$ is the inverse logistic function, $h(x) = \exp (x) / (1 + \exp(x))$. This model is suited for situations where the events occur at discrete times and the covariates change at discrete times. For example, in corporate default prediction, where covariates are values from the financial statements that are reported on a yearly or quarterly basis. However, this is not the case for the hard disk data set. The hard disk data is not reported on monthly precision but on hourly precision. I print the first 10 rows here to illustrate this:

<<hd_dat_ex_setup, echo = FALSE>>=
#####
# Alter digits for the next print
old_digits <- getOption("digits")
options(digits = 5)
@


<<hd_dat_ex>>=
hd_dat_sub[
  1:10, c("serial_number", "model", "tstart", "tstop", "smart_12")]
@

<<hd_dat_ex_cleanup, echo = FALSE>>=
options(digits = old_digits)
@

I will explain how the \code{ddhazard} implementation deals with data sets that are not discrete with the discrete time model. To explain the implementation, I redefine  $\vec{x}_{ij}$ as the covariate vector for individual $i$ that is valid in period $(t_{i,j-1},t_{ij}]$. Next, I redefine the discrete risk set $R_t$ as

\begin{equation}\label{eqn:discreteRiskSetReDef}
R_t = \Lbrace{(i,j)\in \{1,\dots, n\}\times\mathbb{Z}:\, t_{i,j-1} \leq t - 1 < t_{i,j} \wedge D_{it} = 0}
\end{equation}

Further, I redefine

\begin{equation}\label{eqn:binModelLikeliSecond}
l_{ijt}(\vec{\alpha}_t) = y_{it} \log h(\vec{x}_{ij}^\top \vec{\alpha}_t) + (1 - y_{it})
	\log \Lparen{1 - h(\vec{x}_{ij}^\top \vec{\alpha}_t)}
\end{equation}

The methods must be updated to loop over $(i,j)\in R_t$ and use $\vec{x}_{ij}$ instead of $\vec{x}_{it}$ but are otherwise identical. I will present the following example to illustrate the impact of discrete risk sets in Equation~\eqref{eqn:discreteRiskSetReDef}. Suppose we look at interval $d - 1$ and $d$ (the last two intervals) in a model with time-varying covariates. Further, let both the event times and the point at which we observe new covariates happen at continuous points in time. Figure~\ref{fig:binning_fig} illustrates such a situation. Each horizontal line represents an individual. A cross represents when the covariate values change for the individual and a filled circle represents an event that has happened for the individual. Lines that end with an open circle are right censored. The vertical dashed lines in the figure represent the time-interval borders. The first vertical line from the left is the start of interval $d - 1$, the second vertical line is where interval $d - 1$ ends, and interval $d$ starts and the third vertical line is where interval $d$ ends.

<<binning_fig, echo=FALSE, results="hide", fig.cap = "Illustration of a data set with 7 individuals with time-varying covariates. Each horizontal line represents an individual. Each number indicates a start time and/or stop time in the initial data. A cross indicates that new covariates are observed while a filled circle indicates that the individual has an event. An open circle indicates that the individual is right censored. Vertical dashed lines are time interval borders. The symbols for the covariate vectors and stop times are shown for observation a.", fig.height=3.5, fig.width=6, par_1x1 = TRUE,cache=FALSE>>=
# Alter par values
par(mar = c(1, 5, 1, 2), cex = par()$cex * 1.33, xpd=TRUE)

# Make dummy plot
plot(c(0, 4), c(0, 1), type="n", xlab="", ylab="", axes = F)

# Add interval lines and interval text
abline(v = c(0.5, 1.5, 2.5), lty = 2)

text(1, 0.01, expression(paste("Interval ", d - 1)), adj = .5)
text(2, 0.01, expression(paste("Interval ", d)), adj = .5)

#####
# Setup for the observations
n_series = 7
y_pos = seq(0, 1, length.out = n_series + 2)[-c(1, n_series +2)]

# Each element is an observation
# First column is the x coordinates
# Second column is the pch values
x_vals_and_point_codes <-
  list(
    structure(c(0.017182620242238, 1.3,
                4, 1),
              .Dim = c(2L, 2L)),
    structure(c(0.0665940539911389, 0.8, 1.9, 2.20536063500913,
                4, 4, 4, 16),
              .Dim = c(4L, 2L)),
    structure(c(0.406624712632038, 1.13612816265784,
                4, 16),
              .Dim = c(2L, 2L)),
    structure(c(0.092221238068305, 0.8, 2.2, 3, 3.7,
                4, 4, 4, 4, 1),
              .Dim = c(5L, 2L)),
    structure(c(0.285507099120878, 1, 2.12480141618289,
                4, 4, 16),
              .Dim = c(3L, 2L)),
    structure(c(2.15463116460014,2.33,
                4, 16), .Dim = c(2L, 2L)),
    structure(c(0.254905348294415,1.4, 2.1, 3.68174365991727,
                4, 4, 4, 16),
              .Dim = c(4L, 2L)))

# Add the observations to the plot
for(i in seq_along(x_vals_and_point_codes)){
  vals <- x_vals_and_point_codes[[i]]
  y = y_pos[i]
  xs = vals[, 1]
  n_xs = length(xs)

  # add lines
  segments(xs[-n_xs], rep(y, n_xs - 1),
           xs[-1], rep(y, n_xs - 1))

  # Add points
  points(xs, rep(y, n_xs), pch = vals[, 2],
         cex = ifelse(vals[, 2] == 1, par()$cex * 2.5, par()$cex))

  # Add numbers
  text(xs, rep(y + .05, n_xs), as.character(1:n_xs))

  # Add extra text to first observation
  if(i == length(x_vals_and_point_codes)){
    for(j in 1:(n_xs - 1)){
      text(xs[j], y + +.12,
           substitute(paste(bold(x)[ajp], ", (", t[aj], ",",  t[ajp], "]"),
                      list(aj = paste0("a", j - 1),
                           ajp = paste0("a", j))),
           cex = par()$cex * 1.2)
    }
  }
}

# Add letters
x <- sapply(x_vals_and_point_codes, "[", 1, 1)
x <- pmin(x - .1, .4)
text(rev(x), rev(y_pos), letters[1:n_series], cex = par()$cex * 1.5)
@

% CHECK: Text here match with text in the start

I will use observation a in figure~\ref{fig:binning_fig} to illustrate the risk set in Equation~\eqref{eqn:discreteRiskSetReDef}. The covariate vector used in interval $d-1$ is $\vec{x}_{a1}$ as $t_{a0} < d - 1 < t_{a1}$. By similar arguments, the covariate vector in interval $d$ is $\vec{x}_{a2}$. One consequence is that we use covariates from 1 for individuals a, c, d, and f  for the entire period of interval $d - 1$, even though the covariates change at 2. Furthermore, g is not in either interval, as we know only that it survives parts of interval $d - 1$. Lastly, we never include b as we do not know its covariate vector at the start of interval $d$.

\subsection{Continuous time model}\label{subsec:contTime}
The continuous  time model implemented in \code{ddhazard} assumes that

\begin{itemize}
\item we have instantaneous hazards given by $\exp(\vec{x}_{i}(t)^\top\vec{\alpha}(t))$.
\item parameters change at the end of time intervals, i.e., $\vec{\alpha}(t) = \vec{\alpha}_{\lceil t\rceil}$ where $\lceil t\rceil$ gives the ceiling of $t$. This is the same as for the dynamic discrete time model as illustrated in figure~\ref{fig:binning_fig} where the parameters change at the vertical lines.
\item The individuals covariates change at discrete times, i.e., $\vec{x}_{i}(t) = \vec{x}_{ij}$ where $j = \{ k:\  t_{i,k-1} < t \leq t_{i,k}  \}$. In figure~\ref{fig:binning_fig}, the covariates change at the crosses.
\end{itemize}

The instantaneous hazard changes when either the individual's covariates changes or the parameters change. Thus, an individual's event time is piecewise constant exponentially distributed given the state vectors. The log likelihood of individual $i$ having an event at time $t_i$ is

\begin{equation}\label{eqn:condPropEvent}
\log\Lparen{\likepCond{t_i}{\vec{\alpha}_0,\dots,\vec{\alpha}_d}} =
  \vec{x}_i(t_i)^\top\vec{\alpha}(t_i)
  -\int_0^{t_i}\exp\Lparen{\vec{x}_i(u)^\top\vec{\alpha}(u)}\, du
\end{equation}

where $\likep{\cdot}$ denotes the likelihood. Because of our assumptions, the complete data log likelihood in Equation~\eqref{eqn:logLikeFirst} simplifies to

\begin{equation}
\begin{aligned}
\mathcal{L}\Lparen{\mat{Q},\mat{Q}_0, \vec{a}_0} = &
  - \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^\top \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^\top \mat{Q}^{-1} \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\
	&  - \frac{1}{2} \log\deter{\mat{Q}_0} - \log\frac{d}{2} \deter{\mat{Q}} \\
  &+ \sum_{t=1}^d\sum_{(i,j) \in \mathcal{R}_t} l_{ijt}(\vec{\alpha}_t) + \dots
\end{aligned}
\end{equation}

where

\begin{equation}
\begin{aligned}
&y_{ijt} = 1_{\left\{T_i \in (t_{i,j-1}, t_{ij}]\, \wedge\, t - 1 < t_{ij} \leq t \right\}} \\
%
&l_{ijt}(\vec{\alpha}_t) = y_{ijt}\vec{x}_{ij}^\top\vec{\alpha}_t
  - \exp\Lparen{\vec{x}_{ij}^\top\vec{\alpha}_t}
  \Lparen{\min\{ t, t_{ij} \} - \max\{ t - 1, t_{i,j-1} \}}
\end{aligned}
\end{equation}

The $l_{ijt}$ terms are a simplification of Equation~\eqref{eqn:condPropEvent}, where I assume that both the covariates $\vec{x}_i(t)$ and parameters $\vec{\alpha}(t)$ are piecewise constant. $y_{ijt}$ is a generalization of Equation~\eqref{eqn:binFirst} that indicates whether individual $i$ experiences an event with \emph{the $j$’th covariate vector} in interval $t$. Further, $\mathcal{R}_t$ is the \emph{continuous risk set} given by

\begin{equation}
\mathcal{R}_t = \Lbrace{(i,j) \in \{1,\dots,n\}\times \mathbb{Z} :\, t_{i,j-1} < t + 1 \wedge t_{ij} \geq t \wedge D_{it} = 0}
\end{equation}

The two conditions in $\mathcal{R}_t$ are that the observation must start before the intervals end ($t_{i,j-1} < t + 1$) and end after the intervals start ($t_{ij} \geq t$). I will use observation a in figure~\ref{fig:binning_fig} as example. Observation a has two covariate vectors in interval $d - 1$. The first is $\vec{x}_{a1}$ as $t_{a0} < d$ and $t_{a1} > d - 1$. Similar arguments apply for the covariate vector $\vec{x}_{a2}$.

The above is easily implemented with the SMA and GMA as we work directly with the likelihoods, $l_{ijt}$, in~\citeAlgLine{alg:approxMode:findConst}{alg:approxMode},~\citeAlgLine{alg:approxModeChol:findConst}{alg:approxModeChol}, and~\citeAlgLine{alg:GlobalMA:Update}{alg:GlobalMA}. However, the EKF and UKF require that we have an outcome variable with a link function in~\citeAlgLine{alg:EKF:scoreVec}{alg:EKF},~\citeAlgLine{alg:EKFextra:scoreVec}{alg:EKFextra}, and~~\citeAlgLine{alg:UKF:expecMean}{alg:UKF}. The ddhazard vignette shows how to define such outcome variables.

\subsubsection{Example with continuous model}
As mentioned in section~\ref{subsec:logi}, the start and stop times in the hard disk failure data set are in fractions of months with on hourly hourly precision. Thus, I can use the continuous model. I fit the model below using the right clipped time variable with a jump-term outcome variable. The plot of the first estimated factor levels is shown in figure~\ref{fig:cont_plot}.

% CHECK: arguments match with previous

<<cont_fit>>=
ddfit_cont <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  model = "exp_clip_time_w_jump", # Change model from default
  by = 1,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(1, 23),
  Q = diag(0.01, 23),
  control = list(
    NR_eps = 0.001,  # EKF with extra iterations
    eps = 0.001,
    LR = .8,
    method = "EKF")) # Use EKF
@

<<cont_plot, echo=FALSE, par_3x3 = TRUE, fig.cap="Predicted factor levels parameters with the model using the right clipped time variable with a jump term.",cache=FALSE>>=
ddfit_cont <- get_pretty_model_factors(ddfit_cont)
plot(ddfit_cont, cov_index = 1:9)
@

<<cont_cleanup, echo = FALSE>>=
rm(ddfit_cont)
@

The results are comparable to what we have seen previously (e.g., see figure~\ref{fig:subset_EKF_xtra_vs_wo} where I also used the EKF with extra iterations in the correction step but with the discrete time model).

\section{Simulations}\label{sec:sims}
<<run_sim_exp, echo = FALSE,cache = FALSE>>=
# Although caching is used, I also make an additional copy of the results
# as this part takes a while and to avoid re-computations in case of some
# minor change in the code here or similar
library("tcltk")

# Check if results are already computed
result_file <- "results_from_simulation.Rds"
sim_env_file <- "sim_env.Rds"
do_not_compute <-
  file.exists(result_file) &&
  file.exists(sim_env_file) &&
  (!tclvalue(tkmessageBox(
    title = "Rerun", message = "Want to rerun the simulation?",
    type = "yesno")) == "yes")

if(!do_not_compute){
  with(sim_env <- new.env(), {
    #####
    # Function to make sampling go quicker
    get_exp_draw <- with(environment(ddhazard), get_exp_draw())
    get_unif_draw <- with(environment(ddhazard), get_unif_draw())
    get_norm_draw <- with(environment(ddhazard), get_norm_draw())

    #####
    # Define simulation function
    sim_func <- function(
      n_series, n_vars = 10L, t_0 = 0L, t_max = 30L, cov_params = 1,
      re_draw = T, beta_start = rnorm(n_vars), intercept_start,
      sds = rep(1, n_vars + 1), run_n = 1){
      # Make output matrix
      n_row_max <- n_row_inc <- 10^5
      res <- matrix(
        NA_real_, nrow = n_row_inc, ncol = 4 + n_vars,
        dimnames = list(NULL, c("id", "tstart", "tstop", "event",
                                paste0("x", 1:n_vars))))
      cur_row <- 1

      if(re_draw){
        get_unif_draw(re_draw = T)
        get_exp_draw(re_draw = T)
        get_norm_draw(re_draw = T)
      }

      if(length(beta_start) == 1)
        beta_start <- rep(beta_start, n_vars)

      # draw states
      betas <- matrix(get_norm_draw((t_max - t_0 + 1) * (n_vars + 1)),
                      ncol = n_vars + 1, nrow = t_max - t_0 + 1)
      betas <- t(t(betas) * sds)
      betas[1, ] <- c(intercept_start, beta_start)
      betas <- apply(betas, 2, cumsum)

      # covariate simulation expression
      cov_exp <- expression(cov_params * get_norm_draw(n_vars))

      #####
      # Simulate
      for(id in 1:n_series){
        interval_start <- tstart <- tstop <-
          max(floor(get_unif_draw(1) *  2 * t_max) - t_max, 0L)
        repeat{
          tstop <- tstop + 5L
          if(tstop >= t_max)
            tstop <- t_max

          x_vars <- eval(cov_exp)
          l_x_vars <- c(1, x_vars) # add intercept

          tmp_t <- tstart
          while(tmp_t <= interval_start &&  interval_start < tstop){
            # Plus 2 for the coefficient vector at time zero and interval_start
            # is t - 1
            exp_eta <- exp(.Internal(drop(
              betas[interval_start + 2, ] %*% l_x_vars)))
            event <- exp_eta / (1 + exp_eta) > get_unif_draw(1)

            interval_start <- interval_start + 1L
            if(event){
              tstop <- interval_start
              break
            }

            tmp_t <- tmp_t + 1L
          }

          res[cur_row, ] <- c(id, tstart, tstop, event, x_vars)

          if(cur_row == n_row_max){
            # We need to add more rows to the ouput matrix
            n_row_max <- n_row_max + n_row_inc
            res = rbind(res, matrix(NA_real_, nrow = n_row_inc, ncol = 4 + n_vars))
          }
          cur_row <- cur_row + 1

          if(event || tstop >= t_max)
            break

          tstart <- tstop
        }
      }

      list(res = as.data.frame(res[1:(cur_row - 1), ]), betas = betas)
    }

    sim_func <- compiler::cmpfun(sim_func)

    #####
    # Define parameters
    # Some have to elements; one for each simulation experiment

    # General parameters
    n_series <- 2^(9:20)
    n_vars <- c(20, 20)
    t_max <- 30L
    intercept_start <- -3.5
    beta_sd <- c(.33, .33)
    intercept_sd <- 0.1
    Q_0_arg <- 1e5
    Q_arg <- 0.01
    denom_term <- 0.00001
    LR <- 1
    n_max <- 9
    eps <- 0.01
    cov_params <- list(c("sigma" = 1), c("sigma" = .33))

    # SMA and EKF parameters
    NR_eps = 0.1
    SMA_meth = "woodbury"
    Q_0_small <- 1

    # Number of trials and seeds
    n_sims <- 11
    set.seed(4368560)
    seeds <- sample.int(n_sims)

    # UKF parameters
    ukf_alpha <- 1
    ukf_w0 <- 0.0001
    ukf_beta <- 0
    ukf_max <- 2^18
    ukf_kappa <- (2 * n_vars + 1) *
      (1 + ukf_alpha^2 * (ukf_w0 - 1)) / (ukf_alpha^2 * (1 - ukf_w0))
    ukf_Q_0 <- .01

    # Sanity check
    m <- 2 * n_vars + 1
    lambda <- ukf_alpha^2 * (m + ukf_kappa) - m
    stopifnot(all.equal(lambda / (m + lambda), rep(ukf_w0, 2)))
    rm(m, lambda)

    # Number of threads to use
    n_threads <- max(parallel::detectCores() - 1, 2)
    options(ddhazard_max_threads = n_threads)

    # Define result array
    results <- array(
      NA_real_, dim = c(2, length(seeds), length(n_series), 5, 3),
      dimnames = list(
        NULL, NULL, NULL,
        c("EKF", "EKFx", "UKF", "SMA", "GMA"),
        c("elapsed", "MSE", "niter")))

    #####
    # Function to get estimates
    get_fit <- eval(bquote(
      function(data, method, run_n = 1){
        gc() # Make sure garbage collection is run before

        try({
          time <- system.time(fit <- ddhazard(
            Surv(tstart, tstop, event) ~ . - tstart - tstop - event - id,
            data = data, by = 1L, max_T = .(t_max), id = data$id,
            Q_0 = diag(
              ifelse(method %in% c("GMA", "EKFx"),
                     .(Q_0_small),
                     ifelse(method == "UKF" ,
                            .(ukf_Q_0), .(Q_0_arg))),
              .(n_vars)[run_n] + 1),
            Q = diag(.(Q_arg), .(n_vars)[run_n] + 1),
            control = list(
              LR = if(method == "EKF") .(LR) else 1,
              method = stringr::str_replace(method, "x", ""),
              alpha = .(ukf_alpha), beta = .(ukf_beta),
              kappa = .(ukf_kappa)[run_n], denom_term = .(denom_term),
              save_risk_set = F, save_data = F,
              LR_max_try = 1, # we only try one learning rate
              n_max = .(n_max),
              eps = .(eps),
              posterior_version = .(SMA_meth),
              NR_eps =  if(method == "EKFx") NR_eps else NULL
            )))["elapsed"]

          return(list(fit = fit, time = time))
        })

        return(NULL)
      }))

    #####
    # Function to get MSE
    mse_func <- function(betas, fit)
      mean((betas[-1, ] - fit$state_vecs[-1, ])^2) # remove the first entry
                                                   # which is just a same as
                                                   # period one estimate

    ######
    # Run experiment
    for(run_n in 1:2){
      for(i in seq_along(seeds)){
        s <- seeds[i]
        for(j in seq_along(n_series)){
          print(paste0("Using seed ", i, " with number of series index ", j,
                       " in run ", run_n))
          n <- n_series[j]
          set.seed(s)

          # Simulate
          sims <- sim_func(
            n_series = n, n_vars = n_vars[run_n], t_max = t_max,
            intercept_start = intercept_start,
            cov_params = cov_params[[run_n]],
            sds = c(intercept_sd, rep(beta_sd[run_n], n_vars[run_n])),
            run_n = run_n)

          # EKF
          out <- get_fit(data = sims$res, "EKF", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "EKF", "elapsed"] <- out$time
            results[run_n, i, j, "EKF", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "EKF", "niter"] <- out$fit$n_iter
          }

          # EKFx
          out <- get_fit(data = sims$res, "EKFx", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "EKFx", "elapsed"] <- out$time
            results[run_n, i, j, "EKFx", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "EKFx", "niter"] <- out$fit$n_iter
          }

          # UKF
          if(n <= ukf_max){
            out <- get_fit(data = sims$res, "UKF", run_n)
            if(!is.null(out)){
              results[run_n, i, j, "UKF", "elapsed"] <- out$time
              results[run_n, i, j, "UKF", "MSE"] <- mse_func(sims$betas, out$fit)
              results[run_n, i, j, "UKF", "niter"] <- out$fit$n_iter
            }
          }

          # SMA
          out <- get_fit(data = sims$res, "SMA", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "SMA", "elapsed"] <- out$time
            results[run_n, i, j, "SMA", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "SMA", "niter"] <- out$fit$n_iter
          }

          # GMA
          out <- get_fit(data = sims$res, "GMA", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "GMA", "elapsed"] <- out$time
            results[run_n, i, j, "GMA", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "GMA", "niter"] <- out$fit$n_iter
          }

          print(results[run_n, i, j,,])
        }

        print(results[run_n, i,,,])
        rm(out, sims)
      }
    }
  })

  # Take copy of results and cleanup
  results <- sim_env$results
  rm(i, j, s, get_fit, mse_func, envir = sim_env)

  # Save for later
  saveRDS(sim_env, file = sim_env_file)
  saveRDS(results, file = result_file)
} else {
  # Load results
  sim_env <- readRDS(sim_env_file)
  results <- readRDS(result_file)
}
@

In this section, I will simulate data using the first-order random walk model and illustrate the computation time and mean square error of the predicted parameters as the number of individuals increases. The simulation is done as follows. I use a first-order random walk for the parameters with \Sexpr{sim_env$n_vars[1] + 1} parameters. The intercept starts at \Sexpr{sim_env$intercept_start}, and the other parameters start at points drawn from the standard normal distribution. I set the intercept to a low value to decrease the baseline likelihood of an event in every interval. I let covariance matrix $\mat{Q}$ be a diagonal matrix whose first diagonal entry equals $\Sexpr{sim_env$intercept_sd}^2$ (the intercept) and $\Sexpr{sim_env$beta_sd[1]}^2$ for rest of the diagonal entries. The standard deviation is chosen lower for the intercept to ensure that the intercept does not change too much with high probability. Figure~\ref{fig:sim_coefficients_ex} provides an example of a draw of parameters.

<<sim_coefficients_ex, echo=FALSE, results="hide", fig.cap = "Example of parameters in the simulation experiment. The black curve is the intercept and the gray curves are the parameters for the covariates.", par_1x1 = TRUE, cache = FALSE>>=
# Plot one example
set.seed(50044010)
with(sim_env, {
  sims <- sim_func(
          n_series = 100, n_vars = n_vars[1], t_max = t_max,
          intercept_start = intercept_start,
          sds = c(intercept_sd, rep(beta_sd[1], n_vars[1])))

  matplot(sims$betas, type = "l", lty = 1,
          col = c("black", rep("gray40", n_vars[1])),
          xlab = "Time", ylab = "Parameter")
})
@


I then simulate a different number of individuals with $n = 2^{\Sexpr{min(log(sim_env$n_series, 2))}}, 2^{\Sexpr{min(log(sim_env$n_series, 2)) + 1L}}, \dots, 2^{\Sexpr{max(log(sim_env$n_series, 2))}}$ in each trial. Each individual is right censored at time $\Sexpr{sim_env$t_max}$, and I set the interval lengths to $1$. Further, I simulate random delayed entry. We randomly start to observe each individual at time ${0,1,\dots,\Sexpr{sim_env$t_max - 1L}}$ with a 50\% chance of $0$ and uniform chance on the other points. This mimics a situation like corporate default prediction where we use the calendar time as the time scale. A firm may first be incorporated a while into the study, and thus the firm has delayed entry.

Each individual has time-varying covariates that change after five periods. Thus, if an individual starts at time $2$, his covariate vector changes at time $7, 12, \dots, 27$. The covariates are drawn from an iid standard normal distribution. For each value of $n$, I make $\Sexpr{sim_env$n_sims}$ replications. I estimate the UKF model only up to $n = 2^{\Sexpr{log(sim_env$ukf_max, 2)}}$ because of the computation time. Further, I set the UKF hyperparameters to $(\alpha,\beta,\kappa) = (\Sexpr{sim_env$ukf_alpha},\Sexpr{sim_env$ukf_beta},\Sexpr{sim_env$ukf_kappa[1]})$, which yields $W_0^{[m]} = \Sexpr{sim_env$ukf_w0}$. $\mat{Q}_0$ for the EKF with extra iterations and the GMA is a diagonal matrix with entries $\Sexpr{sim_env$Q_0_small}$. The UKF has $\Sexpr{sim_env$ukf_Q_0}$ as the diagonal entries. The EKF without extra iterations and the SMA have $\Sexpr{sim_env$Q_0_arg}$ in the diagonal entries of $\mat{Q}_0$. All the filters set the starting value $\mat{Q}$ as a diagonal matrix with $\Sexpr{sim_env$Q_arg}$ in the diagonal elements. All the methods take at most $\Sexpr{sim_env$n_max + 1}$ iterations of the EM-algorithm if the convergence criteria is not previous met.

% CHECK: comment below

<<tbl_sim_stats, results='asis',echo=FALSE,cache=FALSE>>=
# Compute means and medians
medians <- apply(results, c(1, 3:5), median, na.rm = T)
means <- apply(results, c(1, 3:5), mean, na.rm = T)

# Define output table
tbl_sum <- matrix(
  NA_real_, nrow = 2, ncol = dim(results)[4],
  dimnames = list(
    c("Run time", "Log-log slope"),
    dimnames(results)[[4]]))

# Enter run times
tbl_sum["Run time", ] <-
  apply(medians[1, , , "elapsed"], 2, max, na.rm = T)

# Compute log-log regression slope
log_reg_cut_off <- 2^14
for(n in dimnames(tbl_sum)[[2]]){
  tmp <- results[1, , , n, "elapsed"]
  tbl_sum["Log-log slope", n] <-
    lm(log(c(t(tmp[, log_reg_cut_off<= sim_env$n_series]))) ~
       log(rep(sim_env$n_series[sim_env$n_series >= log_reg_cut_off],
               sim_env$n_sims)))$coefficient[2]
}

# Change column name for EKFx
colnames(tbl_sum)[colnames(tbl_sum) == "EKFx"] <- "EKF with extra iterations"

# Make table and print
xtable(tbl_sum, digits = getOption("digits"),
       caption = paste0("Summary information of the computation time in the simulation study. The first row prints the median runtime for largest number of individuals. The UKF is only up to $n=",
                        sim_env$ukf_max, "$. The second row shows the slope of the log computation time regressed on the log number of individuals for $n\\geq ",
                        log_reg_cut_off, "$."),
       label = "tab:runSummaryStats")
@

The simulations are run on a laptop running Windows 10 with an Intel\textsuperscript{\textregistered}~core\texttrademark~i7-6700hq cpu @ 2.60ghz processor and 16 GB ram. The 64-bit Rtools 34 is used to build \proglang{R} and the \pkg{dynamichazard} package. Figure~\ref{fig:sim_comp_time} shows the medians and means of the computation time. Table~\ref{tab:runSummaryStats} displays the median computation time for the largest value of $n$ along with the regression slope of the log computation time regressed on the log number of individuals. All methods have a slope at or below 1, reflecting the $\bigO{n_t}$ computation time. The slope for the EKF may be lower because of the overhead of the parallel computation overhead. All computation times include the time required to set up the data frame and run a weighted GLM to get starting values for $\vec{\alpha}_0$. The setup time is equal for all methods.

The computation time is comparable for the SMA and the GMA. This may be explained by a floating-point operations (FLOP) count of the matrix and vector product effected by the number of observations. I use the method derived by the Woodbury matrix identity in algorithm~\ref{alg:approxMode} for the SMA in the simulation study. The FLOP count for this algorithm is $(6q + 4q^2)n_t$ at each correction step. In contrast, the GMA method shown in algorithm~\ref{alg:GlobalMA} has a count of $(6q + 2q^2)n_t$ for a single iteration in the correction step. Thus, the computation time of the GMA and SMA should be roughly equal for large $q$ if we make only two iterations with the GMA in the correction step. A second explanation may be the additional search for the step length in~\citeAlgLine{alg:approxMode:findConst}{alg:approxMode}. A third explanation may be that $\mat{X}_t$ is never formed in continuous memory with the SMA. Instead, the matrix with all data rows is used where only a subset of entries are accessed one at a time. If the latter is important, a similar conclusion applies for the EKF method. The two EKF methods are almost equal in computation time because of additional iterations of the EM-algorithm when only one iteration step is taken in the correction step. This is can be seen from figure~\ref{fig:n_iter_plot}, which shows the median number of iterations of the EM-algorithm.

Figure~\ref{fig:sim_MSE} shows a plot of the mean square error for the parameters. The EKF with one iteration in the correction step does not improve much as $n$ increases. Hence, more iterations seem preferable in this example. Interestingly, the UKF flatlines after a given number of observations. Some points are worth stressing. First, the computation time of the UKF and GMA can be reduced by using a multithreaded \pkg{BLAS} library. I have seen a reduction up to factor $2$ for larger datasets on the setup used in the simulation when \pkg{OpenBLAS} \citep{Xianyi12} is used. Further, one can do more tuning (especially with the UKF) for each data set, which is not done in the present setup.

<<sim_labels_setup_n_more, echo = FALSE>>=
#####
# Plot settings
pchs <- structure(
  c(15, 4, 16, 17, 5),
  names = dimnames(medians)[[3]])
col_medians <- "black"
col_means <- rgb(0, 0, 0, alpha = .5)

#####
# Legend function
.legend <- names(pchs)
.legend[.legend == "EKFx"] <- "EKF w/ extra"

draw_legend <- function(x, y = NULL)
  legend(
    x = x, y = y,
    bty = "n",
    pch = pchs,
    legend = .legend)

#####
# Variable for parts of the fig.cap
sim_fig_cap <-
  "The EKF is the filled squares, the EKF with extra iteration is the crosses, the UKF is the circles, the SMA is the triangles and the GMA is the open square."
@

% CHECK: cap match with symbols

<<sim_comp_time, echo=FALSE, results="hide", fig.cap = paste("Median computation times of the simulations for each method for different values of $n$. The gray symbols to the right are the means.", sim_fig_cap, "The scales are logarithmic so a linear trend indicates that computation time is a power of $n$."), par_1x1 = TRUE, cache = FALSE>>=
# Plot for computation time
# I save it as an epxression to re-use it
time_plot_exp <- expression({
  par(xpd=TRUE)
  with(sim_env, {
    # Medians points
    matplot(
      n_series, medians[marg,,, "elapsed"], log  = "xy", col = col_medians,
      pch = pchs, type = "p", xaxt='n',
      xlab = "Number of individuals", ylab = "Computation time (seconds)")
    axis(1, at = sim_env$n_series)
    draw_legend("topleft")

    # Lines between points
    matlines(n_series, medians[marg,,, "elapsed"], lty = 2, col = col_medians)

    # Mean points
    matplot(
      n_series * 1.15, means[marg,,, "elapsed"], col = col_means,
      pch = pchs, type = "p", add = T)
  })})

marg <- 1
eval(time_plot_exp)
@

<<sim_MSE, echo=FALSE, results="hide", fig.cap = paste("Median mean square error of predicted parameters of the simulations for each method for different values of $n$. The gray symbols to the right are the means.", sim_fig_cap, "The axis are on the logarithmic scale."), par_1x1 = TRUE, cache = FALSE>>=
# Plot for MSE
plot_exp <- expression({
  par(xpd=TRUE, yaxs = "i")
  with(sim_env, {
    # Median points
    matplot(
      n_series, medians[marg,,, "MSE"], log  = "xy", col = col_medians,
      pch = pchs, xaxt='n',
      xlab = "Number of individuals", ylab = "MSE of predicted parameters",
      ylim = c(min(medians[marg,,, "MSE"], na.rm = T),
               max(medians[marg,,, "MSE"], na.rm = T)))
    axis(1, at = sim_env$n_series)

    # Lines between points
    matlines(n_series,medians[marg,,, "MSE"], lty = 2, col = col_medians)

    # Mean points
    matplot(
      n_series * 1.15, means[marg,,, "MSE"], col = col_means,
      pch = pchs, type = "p", add = T)

    draw_legend("bottomleft")
  })
})

marg <- 1
eval(plot_exp)
@

<<n_iter_plot, echo = FALSE, fig.cap = paste0("Median number of iterations of the EM-algorithm. ", sim_fig_cap), par_1x1= TRUE, cache = FALSE>>=
par(xpd=TRUE)

with(sim_env, {
  # Median points
  matplot(n_series, medians[1,,, "niter"],
          pch = pchs, xaxt='n',
          ylim = c(0, 10.5), log  = "x",
          col = col_medians,
          xlab = "Number of individuals", ylab = "Iterations of the EM")
  axis(1, at = sim_env$n_series)

  # Lines between points
  matplot(n_series, medians[1,,, "niter"],
          type = "l", lty = 2,
          col = col_medians, add = T)

  draw_legend("bottomleft")
})
@

<<hist_of_lp_dens, echo = FALSE, par_1x1=TRUE, fig.cap="Estimated density for the linear predictor in the last interval in the first simulation experiment.">>=
# The density is not Gaussian. See https://math.stackexchange.com/a/42761/253239
#####
# Make histogram to illustrate that simulation are "extreme"

tmp_env <- new.env(parent = sim_env)
with(tmp_env,{
  set.seed(6339855)

  lps <- replicate(1e3, {
    # Draw state at time 30
    n_vars <- n_vars[1]
    a0 <- c(intercept_start, eval(formals(sim_func)$beta_start))
    a30 <- mapply(
      rnorm, n = 1, mean = a0,
      sd = sqrt(t_max) * c(intercept_sd, rep(beta_sd[1], n_vars[1])))

    # Draw linear predictors
    n <- 1e3
    q <- n_vars
    X <- rnorm(n * (q + 1), sd = formals(sim_func)$cov_params)
    dim(X) <- c(q + 1, n)

    drop(a30 %*% X)
    })

  # Compute density estimate
  # hist(lp, breaks = 50)
  dens <- density(c(lps))
  plot(dens$x, dens$y, type = "l", ylab = "Density",
       main = "", xlab = "Linear predictor", xlim = range(dens$x), yaxs="i")
})

# Cleanup
rm(tmp_env)
@

The simulation here is ``extreme'' in that the linear predictor can take large absolute values in the last intervals with non-negligible probability. Figure~\ref{fig:hist_of_lp_dens} illustrates this by plotting an estimate of the density of the linear predictor in the last interval when both the state vector and the covariate vector are random as in the simulation.

For the above reason, I perform a second simulation where I draw the covariates from a normal distribution with zero mean and variance $\Sexpr{sim_env$cov_params[[2]]["sigma"]}^2$. Figure~\ref{fig:sim_MSE_altered} shows the mean square errors. The difference between the filters is small in terms of mean square error. Still, it seems that EKF with extra iterations and GMA are preferred given the running times, which are similar to the previous example, as figure~\ref{fig:sim_comp_time_altered} shows. I have found it easier to get what seems like sensible results (smooth parameters curves in a reasonable range) with the EKF with extra iteration in the correction step and the GMA method. Thus, the result of the simulation study give further arguments to use the EKF with extra iteration in the correction step and the GMA method.

<<define_altered_plot_text, echo = FALSE>>=
altered_plot_text <- paste0(
  "but where each element of the covariate vectors are drawn from $N\\Lparen{0, ", sim_env$cov_params[[2]]["sigma"], "^2}$.")
@

<<sim_comp_time_altered, echo=FALSE, results="hide", fig.cap = paste("Similar plot to figure~\\ref{fig:sim_comp_time}", altered_plot_text), par_1x1 = TRUE, cache = FALSE>>=
marg <- 2
eval(time_plot_exp)
@

<<sim_MSE_altered, echo=FALSE, results="hide", fig.cap = paste("Similar plot to figure~\\ref{fig:sim_MSE}", altered_plot_text), par_1x1 = TRUE, cache = FALSE>>=
marg <- 2
eval(plot_exp)
@

\section{Conclusion}\label{sec:conc}
I have covered the EM-algorithm implementation in \pkg{dynamichazard} and the four different filters available with the \code{ddhazard} function, highlighting the pros and cons of the different filters have been highlighted. Further, I have covered the dynamic discrete time model and the continuous time model. I have analyzed a large real-world data set. The simulation study shows that the filters scale well with the number of observations. Further, the  simulation study also showed how the mean square error of the predicted parameters behaves with different numbers of observations.

I have not covered all the \code{S3} methods provided in the \pkg{dynamichazard} package. These include \code{plot}, \code{predict}, \code{hatvalues}, and \code{residuals}. Including weights for the individuals for all the filters is possible. The details hereof are in the ddhazard vignette of this package. Furthermore, bootstrap is implemented in the \code{ddhazard_boot} function. Weights are used in \code{ddhazard_boot} with case resampling, which reduces the computation time. Vignettes provided with the \pkg{dynamichazard} package illustrates the use of the mentioned functions. A demo of the models is available by running \code{ddhazard_app}. I will end by looking at potential further developments.

\subsection{Further developments}

I will summarize some potential future developments of the \pkg{dynamichazard} in this section. First, we could replace the random walk model with a parametric model like an ARMA process for each parameter. This will require additional parameter to be estimated the matrix $\mat{F}$ in Equation~\eqref{eqn:stateEqn}. This can be done in the M-step of the EM-algorithm. The constrained EM-algorithm in the \pkg{MARSS} \citep{Holmes13} package can be used here. The details of the EM-algorithm may be found in~\cite{Holmes13}. The structure of the equations in the constrained algorithm can be used to avoid the Kronecker products, vectorization etc. to get an implementation that has cubic complexity in the number of parameters to be estimated. Further, we could extend the model to allow the user to specify that certain entries of the covariance matrix $\mat{Q}$ are restricted to zero by using the same formulas.

We can extend the methods to sparse time series for the parameters. This have received some attention in the signal processing literature. For example,~\cite{Charles11} explore different penalties in the transition of the state vector for the linear Gaussian observational observational equation and nonlinear Gaussian state equation. The penalties include $L1$, $L2$, and a combination of the two (elastic net).~\cite{Angelosante09} give another example, applying the group-Lasso to the linear Gaussian state space models.

We can implement mixture models to model heterogeneity, estimating mixture probabilities in the M-step. Moreover, both the filters and each iteration of the EM-algorithm could stay at computational complexity of $\bigO{q^3}$ if we assume that the parameter vectors in the mixtures in the state vector are independent of each other between the mixture densities (see e.g. the interacting multiple model filter and smoother in \cite{Hartikainen11}).

Other models can be implemented in survival analysis, such as recurrent events and competing risk (see \cite{Fahrmeir96}). Furthermore, the methods can also be used outside survival analysis. For instance, we could observe real valued outcomes, multinomial outcomes or ordinal outcomes for each individual in each interval. The underlying time can depend on the context (e.g., it could be calender time or time since enrollment).

The current implementation of parallel computation for the EKF is based on shared memory. However, we could instead extend it to a distributed network.~\citet[chapter 3]{rigatos17} introduces different ways of extending this. Two approaches are either to distribute the work in each step of the filter or to run separate filters and aggregate the filters at the end.

An alternative to the filters in the E-step is to use the linearisation method described in~\citet[Section 10.6]{durbin12} mentioned in section~\ref{subsec:GMA}. It would be interesting to implement this approach in the package as well.~\cite{Fahrmeir91} describe an idea similar to the linearisation method in~\citet[Section 10.6]{durbin12}, using a Gauss-Newton and Fisher scoring method.

The methods here can be used as the initial input to the importance sampler with use of antithetic variables and control variables, as shown in~\cite{Durbin00}. This can extend the methods to cover state vectors with heavy-tailed distribution. This approach is implemented in the package \pkg{KFAS} \citep{kfas}.

<<echo = FALSE, comment = "%", results = "asis", cache = FALSE, purl=FALSE>>=
# Log session info to tex file
cat(paste("%", capture.output(sessionInfo())), sep = "\n")

cat("%\n%\n% The date and time is", as.character(Sys.time()), "\n")
@


\section*{Acknowledgments}
I would like to thank Hans-Rudolf Künsch and Olivier Wintenberger for constructive conversations. Furthermore, thanks to Søren Feodor Nielsen for feedback and ideas on most aspects of the \code{dynamichazard} package.

\bibliography{bibliography}

\end{document}
