
\documentclass[article,shortnames]{jss}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
Benjamin Christoffersen\\Copenhagen Business School Center for Statistics
}
\title{\pkg{dynamichazard}: Dynamic Hazard Models using State Space Models}
\Keywords{survival analysis, time-varying coefficients, extended Kalman filter, EM-algorithm, unscented Kalman filter, parallel computing, \proglang{R}, \pkg{Rcpp}, \pkg{RcppArmadillo}}

\Abstract{
State space models provides a computational efficient way to model time-varying coefficients in survival analysis. This is implemented in the \pkg{dynamichazard} package. I cover the methods for estimation and models implemented in \pkg{dynamichazard} along with examples. The models and methods are applied to a large data set with hard disk failures and a simulation study is performed to illustrate the computation time and performance. The methods covered in this paper can be applied to other topics than survival analysis at the same computational cost for general non-linear filtering problems.
}

\Plainauthor{Benjamin Christoffersen}
\Plaintitle{dynamichazard: Dynamic Hazard Models using State Space Models}
\Plainkeywords{survival analysis, time-varying coefficients, extended Kalman filter, EM-algorithm, unscented Kalman filter, parallel computing, R, Rcpp, RcppArmadillo}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
\Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    Benjamin Christoffersen\\
  Copenhagen Business School Center for Statistics\\
  Solbjerg Pl. 3, A4.19, 2000 Frederiksberg, Denmark\\
  E-mail: \href{mailto:bch.fi@cbs.dk}{\nolinkurl{bch.fi@cbs.dk}}\\
  URL: \url{http://bit.ly/2nPbTfK}\\~\\
  }

% Included package by default are: graphicx, color, hyperref, ae, fancyverb and natbib

\usepackage{array}
\usepackage[utf8]{inputenc}
% \usepackage{fancyvrb} % for references inside Verbatim
\usepackage{textcomp} % copy right and trademark
\usepackage{amsmath} \usepackage{bm} \usepackage{amsfonts}
\usepackage{algorithm} \usepackage{algpseudocode} \usepackage{hyperref}
\usepackage{rotating}

% fancyvrb documentation regarding commandchars
%   commandchars (three characters) : characters which define the character which
%   starts a macro and marks the beginning and end of a group; thus lets us introduce
%   escape sequences in verbatim code. Of course, it is better to choose special
%   characters which are not used in the verbatim text! (Default: empty).
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,commandchars=\\\&;}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,commandchars=\\\&;}

% algorithm and algpseudocode definitions
\newcommand\StateX{\Statex\hspace{\algorithmicindent}}
\newcommand\StateXX{\Statex\hspace{\algorithmicindent}\hspace{\algorithmicindent}}
\newcommand\StateXXX{\Statex\hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent}}

\algnewcommand{\UntilElse}[2]{\Until{#1\ \algorithmicelse\ #2}}

\algtext*{EndFor} \algtext*{EndProcedure}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}

\newcommand{\citeAlgLine}[2]{line~\ref{#1} of algorithm~\ref{#2}}
\newcommand{\citeAlgLineTwo}[3]{line~\ref{#1} and~\ref{#2} of algorithm~\ref{#3}}
\newcommand{\citeAlgLineTo}[3]{lines~\ref{#1}-\ref{#2} of algorithm~\ref{#3}}

% Table commands
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcommand{\WiTbl}[2]{{
\renewcommand{\arraystretch}{2}
\begin{table}[h!]
\centering
\begin{tabular}{R{3cm} p{9cm}}
#1
\end{tabular}
\caption{#2}
\end{table}
}}

% Math commands

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%
\newcommand{\Lparen}[1]{\left( #1\right)}
\newcommand{\Lbrace}[1]{\left\{ #1\right\}}
\newcommand{\LVert}[1]{\left\rVert #1\right\lVert}
%
\newcommand{\Cond}[2]{\left. #1 \vphantom{#2} \right\vert  #2}
\newcommand{\propp}[1]{\Prob\Lparen{#1}}
\newcommand{\proppCond}[2]{\propp{\Cond{#1}{#2}}}
%
\newcommand{\expecp}[1]{\E\Lparen{#1}}
\newcommand{\expecpCond}[2]{\expecp{\Cond{#1}{#2}}}
%
\newcommand{\varp}[1]{\VAR\Lparen{#1}}
\newcommand{\varpCond}[2]{\varp{\Cond{#1}{#2}}}
%
\newcommand{\likep}[1]{L\Lparen{#1}}
\newcommand{\likepCond}[2]{\likep{\Cond{#1}{#2}}}
%
\newcommand{\hvec}[1]{\widehat{\vec{#1}}}
\newcommand{\hmat}[1]{\widehat{\mat{#1}}}
\newcommand{\tvec}[1]{\tilde{\vec{#1}}}
\newcommand{\tmat}[1]{\tilde{\mat{#1}}}
%
\newcommand{\diag}[1]{\text{diag}{\Lparen{#1}}}
\newcommand{\deter}[1]{\left| #1 \right|}
\newcommand{\bigO}[1]{\mathcal{O}\Lparen{#1}}
%
\newcommand{\emNotee}[3]{#1_{\left. #2 \right\vert #3}}
\newcommand{\emNote}[4]{#1_{\left. #2 \right\vert #3}^{(#4)}}
%
%
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\argmaxu}[1]{\underset{#1}{\argmax}\:}
\newcommand{\argminu}[1]{\underset{#1}{\argmin}\:}
%
% Comment back if you edit code without jss commands
% \newcommand{\Prob}{P}
% \newcommand{\VAR}{Var}
% \newcommand{\E}{E}

% Math commands for section on global mode approximation

\newcommand\algGMAscore[1]{
\begin{aligned}
	 #1&\Lparen{\emNotee{\mat{V}}{t}{t-1}^{-1} + \mat{X}_t^\top (-c''(\vec{\alpha}^{(k-1)})\mat{X}_t}^{-1}
	 \left(\zeta_0 \vphantom{\Lparen{-c''(\vec{\alpha}^{(k-1)})}}
		 \emNotee{\mat{V}}{t}{t-1}^{-1} \emNotee{\vec{a}}{t}{t-1} + \zeta_0 \mat{X}_t^\top c'(\vec{\alpha}^{(k-1)})
		\right. \\
		&\hspace{50pt}\left. + \Lparen{\mat{X}_t^\top\Lparen{-c''(\vec{\alpha}^{(k-1)})}\mat{X}_t + (1-\zeta_0)   \emNotee{\mat{V}}{t}{t-1}^{-1}} \vec{a}^{(k - 1)} \right)
\end{aligned}}
%
\newcommand\algGMApPrime{
\left. \frac{\partial\log\proppCond{\vec{y}_t}{\vec{e}'}}{\partial \vec{e}'} \right|_{\vec{e}' = \mat{X}_t \vec{\alpha}}}
%
\newcommand\algGMApPrimePrime{
\left. \frac{\partial\log\proppCond{\vec{y}_t}{\vec{e}'}}{\partial \vec{e}'\partial \Lparen{\vec{e}'}^\top}\right|_{\vec{e}' = \mat{X}_t \vec{\alpha}}}
%
\newcommand\eqnGblModeTerma{
\begin{pmatrix}
  	\vec{h}'\Lparen{\mat{X}_t  \vec{a}^{(k-1)}}\varpCond{\vec{Y}_t}{\mat{X}_t\vec{\alpha}^{(k-1)}}^{-1/2} & \mat{0} \\
	\mat{0} &  \emNotee{\mat{V}}{t}{t-1}^{-1/2}
\end{pmatrix}}
\newcommand\eqnGblModeTermb{
\begin{pmatrix}\mat{X}_t \\ \mat{I} \end{pmatrix}}
\newcommand\eqnGblModeTermc{
\begin{pmatrix} \vec{b} \\  \emNotee{\vec{\alpha}}{t}{t-1} \end{pmatrix}}

% Other commands
\newcommand\notQsens{
 $\mat{Q}_0$ & $\mat{Q}_0$ can be set to a diagonal matrix with large entries, where the exact values do not have a big impact in some cases. }

\newcommand\Qsens{
 $\mat{Q}_0$ & Sensitive to the choice of $\mat{Q}_0$. }

\newcommand\prosLikelihood{
 Likelihood & We maximize the likelihood directly instead of having to work with residuals. That is, the residuals $y_{it} - h(\eta)$ in \citeAlgLine{alg:EKF:scoreVec}{alg:EKF} and \citeAlgLine{alg:EKFextra:scoreVec}{alg:EKFextra} and $\vec{y}_t - \overline{\vec{y}}$ in \citeAlgLine{alg:UKF:residual}{alg:UKF}. }

\newcommand\embParallel{
  Embarrassingly parallel & The most computational expensive part is easily computed in parallel.}

\begin{document}

<<setup, echo=FALSE, cache=FALSE>>=
knitr::render_sweave()

with(new.env(), {
  knitr_par <- par(no.readonly = TRUE)

  par_default <- function(cex_mult = 1, ...){
    cex <- .75 * cex_mult

    list(
      mar = c(5, 5, 2, 2),
      bty = "L",
      xaxs = "i",
      pch=16,
      cex= cex,
      cex.axis = 1.25,
      cex.lab = 1.4,
      lwd= 1)
  }

  knitr::knit_hooks$set(
    par_1x1 =
      function(before, options, envir) {
        if(!options$par_1x1)
          return()

        if (before){
          par(mfcol = c(1, 1))
          par(par_default(.8))
        }
      },

    par_3x3 =
      function(before, options, envir) {
        if(!options$par_3x3)
          return()

        if (before){
          par(mfcol = c(3, 3))
          tmp <- par_default(.8)
          tmp$mar <- tmp$mar + c(0.5, 0, 0, 0)
          par(tmp)
        }
    })
})

hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(
  output = function(x, options) {
   print_n_tail <- options$print_n_tail
   abbreviate_after <- options$abbreviate_after

   x <- unlist(strsplit(x, "\n"))
   is_longer <- sapply(x, nchar) > abbreviate_after
   x <- sapply(x, substring, first = 0, last = abbreviate_after)
   x <- mapply(function(x, b) if(b) paste0(x, "...") else x,
               x = x, b = is_longer)

   if (is.null(print_n_tail)) {
     x <- paste(c(x, ""), collapse = "\n")
     return(hook_output(x, options))  # pass to default hook
   }

   more <- "... output abbreviated ..."
   x <- c(more, tail(x, print_n_tail))

   x <- paste(c(x, ""), collapse = "\n")
   hook_output(paste(c(x, ""), collapse = "\n"), options)
  },
  inline = function(x) {
  if (is.numeric(x)) {
    format(x, digits = 4)
  } else x
})

knitr::opts_chunk$set(
  echo = TRUE, warning = F, message = F, dpi = 144,
  cache = T,
  fig.align = "center",
  abbreviate_after = 60,

  # See opts_hooks definition
  fig.height = -1, fig.width = -1)

options(digits = 3, scipen=7, width = 60)

knitr::opts_hooks$set(
  fig.height = function(options) {
    if(options$fig.height > 0)
      return(options)

    if (!is.null(options$par_3x3) && options$par_3x3){
      options$fig.height <- 8
    } else
      options$fig.height <- 3

    options
  },

  fig.width = function(options) {
    if(options$fig.width > 0)
      return(options)

    if (!is.null(options$par_3x3) && options$par_3x3) {
      options$fig.width <- 9
    } else
      options$fig.width <- 5

    options
  })

library(xtable)
print.xtable <- with(new.env(), {
  org_fun <- print.xtable
  function(..., comment = FALSE)
    org_fun(..., comment = comment)
})

library(zoo, quietly = T, warn.conflicts = FALSE)
@

The focus of the \pkg{dynamichazard} package is on survival analysis with time-varying coefficients in a way that scale well with the number of observations using state space models. The contribution of this paper and the package is to give an overview non-linear estimation method for state space models that scale well in the dimension of the observational equation, method that are fast, provide an easy interface to use for such methods in survival analysis and illustrate the use of the methods.

Time-varying coefficients are commonly encountered in survival Analysis. An example is when the proportional hazards assumption in a Cox proportional hazards model is not satisfied. Splines are often used to model the time-varying coefficients. Various package in \proglang{R} \citep{baseR16} on the Comprehensive R Archive Network (CRAN) take this approach. An example is \code{splineCox} from the \pkg{dynsurv} package \citep{dynsurv} which also includes a non-spline based Bayesian approach with piecewise constant coefficients and a transformation based approach. Another example is the \pkg{rstpm2} package \citep{rstpm2}. An L2 penalized B-spline intercept is implemented in the \pkg{bshazard} \citep{Rebora14} package. Further, penalized time-varying coefficients can be estimated by using any of the regularization methods in packages like \pkg{glmnet} \citep{Simon11}, \pkg{glmpath} \citep{glmpath}, \pkg{mgcv} \citep{wood06} and \pkg{penalized} \citep{Goeman10} combined with the method described by~\cite{Thomas14}.

A non-spline based approaches is used in the \pkg{timereg} package \citep{martinussen07} for both the Cox regression model and the additive hazards models. Similarly, the \code{aareg} function in the package \pkg{survival} \citep{survival} included with \proglang{R} can estimate the non-parametric Aalen model with time-varying coefficients. The \pkg{pch} package \citep{pch} fits time-varying coefficients by dividing the time into periods and making separate Poisson regressions in each period. \pkg{concreg} \citep{concreg} uses conditional logistic regression to estimate the time-varying coefficients.

Another approach to model the time-varying coefficients is using discrete state space models where the coefficients are assumed to be piecewise constant. An advantage of this approach is that we have parametric model for the coefficients which allows for extrapolation beyond the last observed period. This is implemented in this package. Moreover, the models can be implemented to have a linear computational cost relative to the number of observed individuals and some are easily computed in parallel. Consequently, they scale well to large data sets.

Various packages for state space models are on CRAN. Two reviews of packages for the standard Gaussian models from 2011 are \cite{Petris11} and \cite{Tusell11}. They briefly mention non-linear models that can be used in a survival analysis. The package \pkg{KFAS} \citep{kfas} provides non-linear models which can be used in a survival Analysis. See \cite{helske16} for the examples of the non-linear models in \pkg{KFAS}. Another package is \pkg{pomp} \citep{King16} for general non-linear models with both Bayesian and frequentist's methods. One can also use \pkg{rstan} \citep{rstan} to setup a variety of models. Common for all the packages is that they are quite general. However, this comes at a cost of making it cumbersome to set up models like those \pkg{dynamichazard} is well suited for and at a high computational cost.

The rest of this paper is structured as follows. Section~\ref{sec:notation} introduces the problem and notation in this paper. Section~\ref{sec:meth} shows the particular EM-algorithm that all the methods are based on. This is followed by four different filters used in the E-step. An example with hard disk failures will be used through-out this section to lustrate the methods. Section~\ref{sec:mod} covers the two implemented models. Section~\ref{sec:sims} illustrate the performance and computation time of the methods on simulated data. Section~\ref{sec:conc} concludes. Some key point are that:

\begin{itemize}
\tightlist
\item
  All methods are implemented is \proglang{C++} with use of \pkg{BLAS} and
  \pkg{LAPACK} \cite{laug} either by direct calls to the methods or
  through the \proglang{C++} library \pkg{Armadillo} \citep{Sanderson16}.
\item
  All methods scales linearly in computation time with the number of observations. The reported computational complexity in the rest of the paper is based on a single iteration of the EM-algorithm.
\end{itemize}

The inspiration for the package is from \cite{Fahrmeir94} and \cite{Fahrmeir92}. The current implementation uses the EM-algorithm from these papers. The reader may want further resources on the filters covered later as they are only briefly introduced. \citep[chapter 4]{durbin12} covers the Kalman filter which provides a basis for understanding all the filters in the package. \cite{Fahrmeir92} and \cite{Fahrmeir94} covers the extended Kalman filter used in this package while \citep[section 10.2]{durbin12} covers the more common form of the extended Kalman filter. \citep[section 10.3]{durbin12} and \cite{Wan00} provides an introduction to the unscented Kalman filter.

\section{Notation and problem}\label{sec:notation}

I will start by introducing the notation in the discrete time model in survival analysis setting. Outcomes in the model are binary. Either an individual has an event or not within each period. I generalize in section~\ref{sec:mod} to event times instead binary outcomes. We are observing individual $1,2,\dots,n$ who each has an \emph{event} at time $T_1,T_2,\dots,T_n$ and \emph{right-censoring times} $D_1,D_2,\dots,D_n$. By definition we set $D_i = \infty$ if the we observe an event for individual $i$. We observe covariate vectors $\vec{x}_{i1},\vec{x}_{i2},\dots$ for each individual $i$. Each covariate vector $\vec{x}_{ij}$ is valid in a period $(t_{i,j-1},t_{ij}]$. We put the observations into intervals $1, 2, \dots, d$ where each interval has length $\Delta \mathrm{t}_1, \Delta \mathrm{t}_2, \dots, \Delta \mathrm{t}_d$, respectively. In this paper, I assume that each $\Delta \mathrm{t}_t = 1$ for simplicity if it is not mentioned. Then I define a series of indicators for each individual  by:

\begin{equation}\label{eqn:binFirst}
y_{it} = 1_{\left\{T_i \in (t - 1, t]\right\}}
\end{equation}

where $1_{\{\cdot\}}$ is an indicator which is one if the condition in the subscript is satisfied. Thus, $y_{it}$  denotes whether individual $i$ experiences an event in interval $t$. Next, the \emph{risk set} in interval $t$ is given by:

\begin{equation}\label{eqn:discreteRiskSet}
R_t = \Lbrace{(i,j)\in \mathbb{Z}^2_{+}:\, t_{i,j-1} < t - 1 \leq t_{i,j}
  \wedge t < D_i}
\end{equation}

where $\mathbb{Z}_{+}$ are the natural numbers $1,2,\dots$. I will refer to this as the \emph{discrete risk set} as I later introduce a continuous version. The chance of an event for a given individual $i$ who has covariate vector $j$ in interval $t$ is given by:

\begin{equation}\begin{aligned}\label{eqn:condProb}
  \proppCond{Y_{it} = 1}{\vec{y}_{1},\dots,\vec{y}_{t-1}, \vec{\alpha}_t} =
    h(\vec{\alpha}_t^\top \vec{x}_{ij})
\end{aligned}\end{equation}

where $\vec{y}_s$ is the vector of outcomes which elements and elements ordering are given by risk set $R_s$. $\vec{\alpha}_t$ is the state vector in interval $t$ and $h$ is the inverse link function. The inverse logistic function is used by default such that $h(\eta) = \exp(\eta) / (1 + \exp(\eta))$. The models in the state space form are:
\begin{equation}\label{eqn:stateEqn}\begin{array}{ll}
  \vec{y}_t = \vec{z}_t(\vec{\alpha}_t) + \vec{\epsilon}_t \qquad &
  \vec{\epsilon}_t \sim (\vec{0}, \varpCond{\vec{y}_t}{\vec{\alpha}_t})  \\
  \vec{\alpha}_{t + 1} = \mat{F}\vec{\alpha}_t + \mat{R}\vec{\eta}_t \qquad &
  \vec{\eta}_t \sim N(\vec{0}, \Delta \mathrm{t}_t \mat{Q}) \\
\end{array} , \qquad t = 1,\dots, d
\end{equation}

The equation for $\vec{y}_t$ is denoted the \emph{observational equation}. $\sim (v,b)$ denotes a random variable with mean (vector) $v$ and variance (covariance matrix) $b$. It need not be a normal distribution. $\vec{\alpha}_t$ is the state vector with the corresponding \emph{state equation}. Again, $\Delta \mathrm{t}_t=1$ unless stated otherwise. Though, the current implementation can handle any equidistant interval length. Further, I denote the observational equation's conditional covariance matrix by $\mat{H}_t(\vec{\alpha}_t) = \varpCond{\vec{y}_t}{\vec{\alpha}_t}$. The mean $\vec{z}_t(\vec{\alpha}_t)$ and variance $\mat{H}(\vec{\alpha}_t)$ are state dependent with:
\begin{equation}\begin{aligned}
  z_{kt}(\vec{\alpha}_t) &=\expecpCond{y_{it}}{\vec{\alpha}_t} = h(\vec{\alpha}_t^\top \vec{x}_{ijt}) \\
  H_{kk't}(\vec{\alpha}_t) &= \left\{\begin{matrix}
    \varpCond{y_{it}}{\vec{\alpha}_t} & k = k' \\ 0 & \textrm{otherwise}
  \end{matrix}\right. \\
  &=\left\{\begin{matrix}
    z_{kt}(\vec{\alpha}_t)(1 - z_{kt}(\vec{\alpha}_t)) & k = k' \\ 0 & \textrm{otherwise}
  \end{matrix}\right.
\end{aligned}\end{equation}

where I have assumed that individual $i$ with covariate vector $j$ is the $k$'th index of the risk set at time $t$. The state equation is implemented with a 1. and 2. order random walk. The first order random walk has $\mat{F} = \mat{R} = \mat{I}_m$ where $m$ is the number of time-varying coefficients and $\mat{I}_m$ is the identity matrix with dimension $m$. I let $q$ denote the dimension for the state space vector. Thus, $q=m$ for the first order random walk. As for the second order random walk, we have:
\begin{equation}\mat{F} = \begin{pmatrix}
  2\mat{I}_m & - \mat{I}_m \\ \mat{I}_m & \mat{0}_m
\end{pmatrix},  \qquad
\mat{R} = \begin{pmatrix} \mat{I}_m \\ \mat{0}_m \end{pmatrix}
\end{equation}

where $\mat{0}_m$ is a $m\times m$ matrix with zeros in all entries. The vector in the state equation is ordered as $\vec{\alpha}_t = (\tvec{\alpha}_t^\top, \tvec{\alpha}_{t-1}^\top)^\top$ to match the definition of $\mat{F}$ and $\mat{R}$ where the tilde on the alphas is added to indicate the coefficients used when computing the linear predictor in equation~\ref{eqn:condProb}. Notice that the dimension of the state vector is $q = 2m$ which affects the computational cost. The likelihood of the model where state vectors are observed can be written as follows by application of the markovian property of the model:
\begin{equation}\begin{split}
	\proppCond{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}}{\vec{y}_{t},\dots,\vec{y}_d} & =
		L\Lparen{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}} \\
	 & = p(\vec{\alpha}_0)\prod_{t = 1}^d \proppCond{\vec{\alpha}_t}{\vec{\alpha}_{t-1}}
		\prod_{(i,j) \in R_t} \proppCond{y_{it}}{\vec{\alpha}_t}
\end{split}\end{equation}

which can be expand to:

\begin{equation}\label{eqn:logLike}\begin{aligned}
	\log L \Lparen{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}} \propto &
	 \mathcal{L}\Lparen{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}}
		 \\
		= & - \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^\top \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^\top\mat{R}^\top\Delta \mathrm{t}_t^{-1}\mat{Q}^{-1}\mat{R}\Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\
	&  - \frac{1}{2} \log \deter{\mat{Q}_0} - \frac{1}{2d} \log \deter{\mat{Q}} \\
	&  + \sum_{t = 1}^d \sum_{(i,j) \in R_t} l_{ijt}({\vec{\alpha}_t})
\end{aligned}\end{equation}

\begin{equation}\label{eqn:binModelLikeli}
l_{ijt}(\vec{\alpha}_t) = y_{it} \log h(\vec{x}_{ijt}^\top \vec{\alpha}_t) + (1 - y_{it})
	\log \Lparen{1 - h(\vec{x}_{ij}^\top \vec{\alpha}_t)}
\end{equation}

This completes the notation I will need for the discrete model. I continue with the methods used to fit the model.

\section{Methodology}\label{sec:meth}

The estimation function in \pkg{dynamichazard} is the \code{ddhazard} function. All the methods implemented in the current version of \code{ddhazard} use the EM-algorithm described in \cite{Fahrmeir94} and \cite{Fahrmeir92}. The EM-algorithm is similar to the method in \cite{Shumway82} but for a non-linear observational equation. The unknown parameters in the state equation~\ref{eqn:stateEqn} are the covariance matrices $\mat{Q}$ and $\mat{Q}_0$ and the initial state $\vec{\alpha}_0$. $\mat{Q}$ and $\vec{\alpha}_0$ will be estimated in the M-step of the EM-algorithm. It is common practice with Kalman filters to set the diagonal elements of $\mat{Q}_0$ to large values yielding an information matrix which has almost zero in all entries. Another approach is diffuse initialization. The idea to set $\mat{Q}_0 = c\mat{I} + \tmat{Q}_0$ for a given matrix $\tmat{Q}_0$ and letting $c\rightarrow\infty$. See~\citep[chapter 5]{durbin12}. I use the following notation for the conditional means and covariance matrix:

\begin{equation}
  \emNotee{\vec{a}}{t}{s} = \expecpCond{\vec{\alpha}_t}{\vec{y}_1,\dots,\vec{y}_s},  \qquad
    \emNotee{\mat{V}}{t}{s} = \expecpCond{\mat{V}_t}{\vec{y}_1,\dots,\vec{y}_s}
\end{equation}

Notice that the letter 'a' is used for the mean estimates while 'alpha' is used for the unknown state. The notation above both covers filter estimates in the case where $s \leq t$ and smoothed estimates when $s \geq t$. I suppress the dependence on the covariates, $\vec{x}_{ij}$, here to simplify the notation. The EM algorithm is given in algorithm~\ref{alg:EM}.

\begin{algorithm}
\caption{EM algorithm with unspecified filter.}\label{alg:EM}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0,\vec{a}_0,\mat{X}_1,\dots,\mat{X}_d,\vec{y}_1,\dots,\vec{y}_d,R_1,\dots,R_d$
\Statex Convergence threshold $\epsilon$
\State Set $\emNote{\vec{a}}{0}{0}{0} = \vec{a}_0$ and $\mat{Q}^{(0)} = \mat{Q}$
\For{$k=1,2,\dots$}
\Procedure{E-step}{}
\State Apply filter with  $\emNote{\vec{a}}{0}{0}{k-1}$, $\mat{Q}^{(k-1)}$ and $\mat{Q}_0$ to get \label{alg:EM:filter}
\StateXXX $\emNotee{\vec{a}}{1}{0},$ $\emNotee{\vec{a}}{1}{1},$ $\emNotee{\vec{a}}{2}{1},\dots,$ $\emNotee{\vec{a}}{d}{d-1},$ $\emNotee{\vec{a}}{d}{d}$ and
\StateXXX $\emNotee{\mat{V}}{1}{0},$ $\emNotee{\mat{V}}{1}{1},$ $\emNotee{\mat{V}}{2}{1},\dots,$ $\emNotee{\mat{V}}{d}{d-1},$ $\emNotee{\mat{V}}{d}{d}$
	\StateXX Apply smoother by computing
\For{$t=d,d-1,\dots,1$}
\State $\mat{B}_t^{(k)} = \emNotee{\mat{V}}{t - 1}{t - 1} \mat{F} \emNotee{\mat{V}}{t}{t - 1}^{-1}$
\State $\emNote{\vec{a}}{t - 1}{d}{k} = \emNotee{\vec{a}}{t - 1}{t - 1} + \mat{B}_t^{(k)} (
    \emNote{\vec{a}}{t}{d}{k} - \emNotee{\vec{a}}{t}{t - 1})$
\State $\emNote{\mat{V}}{t - 1}{d}{k} = \emNotee{\mat{V}}{t - 1}{t - 1} + \mat{B}_t^{(k)} (
    \emNote{\mat{V}}{t}{d}{k} - \emNotee{\mat{V}}{t}{t - 1}) \Lparen{\mat{B}_t^{(k)}}^\top$
\EndFor
\EndProcedure
\Procedure{M-step}{}
	\StateXX Update initial state and covariance matrix by
\State $\emNote{\vec{a}}{0}{0}{k} = \emNote{\vec{a}}{0}{d}{k}$
\State $\begin{aligned}\mat{Q}^{(k)} &= \frac{1}{d}	\sum_{t = 1}^d \mat{R}^\top\left(
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)
      \left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^\top \right. \\
    &\hspace{15pt} + \emNote{\mat{V}}{t}{d}{k} - \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} -
      \left( \mat{F}\mat{B}_t^{(k)}\emNote{\mat{V}}{t}{d}{k} \right) ^\top +
      \mat{F}\emNote{\mat{V}}{t - 1}{d}{k}\mat{F}^\top
      \left. \vphantom{\left( \emNote{\vec{a}}{t}{d}{k} - \mat{F}\emNote{\vec{a}}{t - 1}{d}{k} \right)^\top} \right)\mat{R}
  \end{aligned}$
\EndProcedure
\StateX Stop the if sum of relative norms is below the threshold
\State $\sum_{t=0}^d \frac{\LVert{\emNote{\vec{a}}{t}{d}{k} - \emNote{\vec{a}}{t}{d}{k - 1}}}{\LVert{\emNote{\vec{a}}{t}{d}{k - 1}}} < \epsilon$
\EndFor
\end{algorithmic}
\end{algorithm}

The matrices $\mat{X}_1,\mat{X}_2,\dots,\mat{X}_d$ are the design matrices given by the risk set $R_1,R_2\dots,R_d$ and the covariate vectors. The only unspecified part is the filter in \citeAlgLine{alg:EM:filter}{alg:EM}. Notice that the other lines only involves product of matrices and vectors of dimension equal to the state space vector dimension, $q$. Moreover, the computational cost is independent of the size of the risk sets for the specified parts of algorithm~\ref{alg:EM}. Thus, the computational complexity so far is $\bigO{q^3d}$ where $d$ is the number of intervals. The threshold for convergence is determined by the \code{eps} element of the list passed to the \code{control} argument of \code{ddhazard} (e.g. \code{list(eps = 0.01, ...)}). The EM-algorithm tend to converge slowly towards the end. However, a tolerance of $0.01$ or $0.001$ is quickly satisfied with minor differences compared to smaller tolerance from my experience for most problems. The filters implemented for \citeAlgLine{alg:EM:filter}{alg:EM} are an Extended Kalman filter (EKF), an Unscented Kalman filter (UKF), sequential approximation of the posterior modes and estimation of posterior modes. I will covers these in the respective order.

\subsection{Extended Kalman filter}\label{subsec:EKF}

The Extended Kalman Filter (EKF) presented here is due to \cite{Fahrmeir94}. It can be derived by applying the Woodbury Matrix identity to the usual EKF. The method is shown in algorithm~\ref{alg:EKF}. Commonly, the prediction step is also referred to as the time update and the correction step is also referred to as the measurement update. A few points is worth making. Firstly, the largest computational burden is in \citeAlgLine{alg:EKF:scoreMat}{alg:EKF} when the dimension of the state vector, $q$, is low compared to the number of observations at time $t$ which I denote by $n_t = \vert R_t \vert$. However, the computation here is what is commonly referred to as embarrassingly parallel. That is, it can easily be computed in parallel since there is little communication required between the parallel tasks. This is exploited with the current version of \code{ddhazard} where the computation of \citeAlgLineTwo{alg:EKF:scoreVec}{alg:EKF:scoreMat}{alg:EKF} is done in parallel using the \proglang{C++} library \pkg{thread}. All the matrices and vectors are of dimension $q\times q$ and $q$ so it is easy to confirm that the filter is $\bigO{q^3n_t}$ in computational complexity.

\begin{algorithm}
\caption{Extended Kalman Filter (EKF). The index $k$ in the correction step in line~\ref{alg:EKF:scoreVec} and line~\ref{alg:EKF:scoreMat} are implicitly set to match the index of the $(i,j)$'th pair in the risk set.}\label{alg:EKF}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0, \vec{a}_0,$ $\mat{X}_1,\dots,\mat{X}_d,$ $\vec{y}_1,\dots,\vec{y}_d,$ $R_1,\dots,R_d$
\State Set $\emNotee{\vec{a}}{0}{0} = \vec{a}_0$ and $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$
\For{$t=1,2,\dots,d$}
\Procedure{Prediction step}{}
\State $\emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}$
\State $\emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^\top + \mat{R}\mat{Q}\mat{R}^\top$
\EndProcedure
\Procedure{Correction step}{}
\StateXX Compute score vector and information matrix and set:
\State Let $\vec{a} = \emNotee{\vec{a}}{t}{t - 1}$
\State $\vec{u}_t (\vec{a}) = \sum_{(i,j) \in R_t} \vec{u}_{ijt} (\vec{a}), \quad\vec{u}_{ijt} (\vec{a})= \left. \vec{x}_{ij} \frac{\partial h(\eta)/ \partial \eta}{H_{kkt}(\vec{a})} \Lparen{y_{it} - h(\eta)} \right\vert_{\eta = \vec{x}_{ij}^\top \vec{a}}$ \label{alg:EKF:scoreVec}
\State $\mat{U}_t (\vec{a}) = \sum_{(i,j) \in R_t} \mat{U}_{ijt} (\vec{a}), \quad \mat{U}_{ijt} (\vec{a}) = \left. \vec{x}_{ij} \vec{x}_{ij}^\top \frac{\left( \partial h(\eta)/ \partial \eta \right)^2}{H_{kkt}(\vec{a})} \right\vert_{\eta = \vec{x}_{ij}^\top \vec{a}}$ \label{alg:EKF:scoreMat}
\State $\emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\vec{a})\right)^{-1}$ \label{alg:EKF:varUpdate}
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1} + \emNotee{\mat{V}}{t}{t}\vec{u}_t (\vec{a})$ \label{alg:EKF:stateUpdate}
\EndProcedure
\EndFor
\end{algorithmic}
\end{algorithm}

The \code{ddhazard} function provides some nuances to algorithm~\ref{alg:EKF}. The first is that \citeAlgLineTwo{alg:EKF:varUpdate}{alg:EKF:stateUpdate}{alg:EKF} is a similar to a Newton-Raphson step. This could motivate us to take further steps. The other nuances aims to address the issue that the EKF can diverge. Two steps are taken to overcome cases where divergence is a problem. The first step is introduce a learning rate, $\zeta_0$, in \citeAlgLine{alg:EKF:stateUpdate}{alg:EKF} when we update the state vector. The second step is to increase the variance in the denominator of the score vector and information matrix in \citeAlgLineTwo{alg:EKF:scoreVec}{alg:EKF:scoreMat}{alg:EKF}. This reduces the effect of values predicted near the boundaries of the outcome space. This is similar to the approach taken in \pkg{glmnet} \cite[page~9]{friedman10} to deal large absolute values of the linear predictor. These three changes of the correction step are shown in algorithm~\ref{alg:EKFextra}.

\begin{algorithm}
\caption{EKF with extra correction steps, learning rate and parameter $\xi$ replacing \citeAlgLineTo{alg:EKF:scoreVec}{alg:EKF:stateUpdate}{alg:EKF}.}\label{alg:EKFextra}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex Threshold $\epsilon$, learning rate $\zeta_0$ and small numbers $\delta$ and $\xi$
\State $\vec{a} = \emNotee{\vec{a}}{t}{t - 1}$
\Repeat
\State $\vec{u}_t (\vec{a}) = \sum_{(i,j) \in R_t} \vec{u}_{ijt} (\vec{\alpha}), \quad\vec{u}_{ijt} (\vec{a})= \left. \vec{x}_{ij} \frac{\partial h(\eta)/ \partial \eta}{H_{kkt}(\vec{a})+ \xi} \Lparen{y_{it} - h(\eta)} \right\vert_{\eta = \vec{x}_{ij}^\top \vec{a}}$\label{alg:EKFextra:scoreVec}
\State $\mat{U}_t (\vec{a}) = \sum_{(i,j) \in R_t} \mat{U}_{ijt} (\vec{a}), \quad \mat{U}_{ijt} (\vec{a}) = \left. \vec{x}_{ij} \vec{x}_{ij}^\top \frac{\left( \partial h(\eta)/ \partial \eta \right)^2}{H_{kkt}(\vec{a}) + \xi} \right\vert_{\eta = \vec{x}_{ij}^\top \vec{a}}$
\State $\emNotee{\mat{V}}{t}{t} = \left( \emNotee{\mat{V}}{t}{t - 1}^{-1} + \mat{U}_t (\vec{a})\right)^{-1}$
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\mat{V}}{t}{t}\Lparen{
		\mat{U}_t (\vec{a})\vec{a} + \emNotee{\mat{V}}{t}{t - 1}^{-1}\emNotee{\vec{a}}{t}{t - 1} + \zeta_0 \vec{u}_t (\vec{a})}$
\UntilElse{$\LVert{\emNotee{\vec{a}}{t}{t} - \vec{a}}/ (\LVert{\vec{a}} + \delta) < \epsilon$}{set $\vec{a} = \emNotee{\vec{a}}{t}{t}$}
\end{algorithmic}
\end{algorithm}

Extra correction steps are not taken by default. They will be taken if the element \code{NR_eps} of the list passed to the \code{control} argument of \code{ddhazard} is set to the value of $\epsilon$. The learning rate, $\zeta_0$, is set by setting the element \code{LR} in the list passed to the \code{control} argument. By default, the current implementation tries a decreasing series of learning rates starting with $\zeta_0$ until the algorithm does not diverge. $\xi$ is changed by altering the \code{denom_term} element in the list passed to the \code{control} argument. Typically, values in the range $[10^{-6},10^{-4}]$ tend to be sufficient in most cases. My experience is that the user should mainly focus on the learning rate. The motivation for the formulas in algorithm~\ref{alg:EKFextra} is to get a method that is closer to the mode estimation as shown in ddhazard vignette.

\subsubsection{Hard disk failures}

<<load_hd_dat, echo = FALSE>>=
fname <- gsub(
  "(^.+dynamichazard)(.*)$", "\\1/vignettes/.jss/HDS/HDs.RDS", getwd())
hd_dat <- readRDS(fname)

# Few have data from time zero so we set a few days in as time zero
new_start <- 24 * 4
hd_dat$tstart <- pmax(new_start, hd_dat$tstart)
hd_dat$tstart <- hd_dat$tstart - new_start
hd_dat$tstop <- hd_dat$tstop - new_start

# We need to remove the records that ends before or at the starting time
# sum(hd_dat$tstart >= hd_dat$tstop) # Number of rows thrown away
hd_dat <- hd_dat[hd_dat$tstart < hd_dat$tstop, ]

hd_dat$serial_number <- droplevels(hd_dat$serial_number)

# Re-scale time
tmp <- 24 * 30
hd_dat$tstart  <- hd_dat$tstart / tmp
hd_dat$tstop <- hd_dat$tstop / tmp

# Make sure that data is sorted
hd_dat <- hd_dat[order(hd_dat$serial_number, hd_dat$tstart), ]

# Fill in blanks with carry the last observation forward
# Define function to fill in the blanks
# TODO: Check with BackBlaze if this is a good choice
library(zoo, quietly = T)

func <- function(x)
  na.locf0(c(0, x))[-1]
func <- compiler::cmpfun(func)

# Use the function
for(n in colnames(hd_dat)["smart_12" == colnames(hd_dat)]){
  hd_dat[[n]] <- unlist(
    tapply(hd_dat[[n]], as.integer(hd_dat$serial_number), func),
    use.names = F)
}
@

I will look at time till failure for hard drives as an example through-out this paper. Predicting when a hard disk is going to fail is important for any firm who manages last amount of data stored locally in order to replace the hard disks before they fail. Self-Monitoring, Analysis and Reporting Technology (SMART) is one tool used to predict future failure of a given Hard disk. An example of paper that uses the SMART attributes is \cite{pinheiro07}. The dataset I will use is from BackBlaze which is publicly available at~\cite{backblazestats}. Backblaze is a data storage provider and currently manages more than 65,000 hard disks. They have a daily snapshot of the SMART attributes for all their hard disks going back to April 2013. The final data set I use have $\Sexpr{length(unique(hd_dat$serial_number))}$ unique hard disks. It has $\Sexpr{nrow(hd_dat)}$ rows in start-stop format for survival analysis after removing redundant rows.

BackBlaze provides a binary failure indicator. A hard disk is marked as a failure if: "... the drive will not spin up or connect to the OS, the drive will not sync, or stay synced, in a RAID Array ... [or] the Smart Stats we [BackBlaze] use show values above our [BackBlaze's] thresholds" \citep{backblaze2016Q1}. A hard drive with a failure is removed. I will not use the SMART attributes that BackBlaze uses due to the third condition. These are SMART attributes 5, 187, 188, 197 and 198 \citep{backblazesmartstatuse}. Moreover, we need to be aware that we are not only looking at a failure of a hard disk.

I will use the power-on hours (SMART attribute number 9) as the time scale in the model we estimate. The hard disks run 24 hours a day unless they are shut down due to e.g. maintenance. BackBlaze have stated that: "If one of the drives in a Storage Pod fails, we cycle down the entire Storage Pod to replace the failed drive. This only takes a few minutes and then power is reapplied and everything cycles back up. Occasionally we power cycle a Storage Pod for maintenance and on rare occasions we’ve had power failures, but generally, the drives just stay up" \citep{backblazesmartstatuse}. The quote refers the storage pods in which the hard disk are placed. BackBlaze uses storage pods with 45 to 60 hard disk in each \citep{backblazepods}. This will fact will turn out to be useful later.

The SMART attribute I will look at as a predictor is the power cycle count (SMART attribute number 12). This counts the number of times a hard disk have a had full hard disk power on/off cycle. As mentioned before hard disk run most of the time. We can speculate that the same model of hard disk or even hard disk from the same batch are in the same pods. Thus, an effect of power cycles can be caused by a confounder like hard disks being from the same batch.

% The Pearson correlation and Spearman's rank correlation coefficient is low (less than $0.4$) between the power cycle count and the other SMART statistic that BackBlaze uses. Thus, we may expect to be looking at actual failures.

<<winsorize, echo = FALSE>>=
n_per_model <-
  xtabs(~ model, hd_dat, subset = !duplicated(serial_number))

# We take those larger than a given size
factor_cut <- 400
models_to_keep <- names(n_per_model)[n_per_model >= factor_cut]
hd_dat <- hd_dat[hd_dat$model %in% models_to_keep, ]
hd_dat$model <- droplevels(hd_dat$model)

# Winsorize
win_lvl <- .99
hd_dat$smart_12 <- pmin(hd_dat$smart_12, quantile(
  hd_dat$smart_12, win_lvl))
@

I will include a factor level for the model. The motivation is that the differences in failure rates between models are large. Particularly, one model of 3 terabyte hard disks from Seagate (denoted by ST3000DM001) have had a high failure rate \citep{backblazest3tb}. I remove model with less than $\Sexpr{factor_cut}$ unique hard disk which leaves us with $\Sexpr{length(unique(hd_dat$serial_number))}$ unique hard disks. I winsorize at the $\Sexpr{win_lvl}$ quantile for the power cycle count. The quantiles for power cycle counts are:

<<smart_12_quants>>=
quantile(hd_dat$smart_12, c(0, 0.5, .75, (95:99)/100))
@

<<define_get_pretty_model_factors, echo = FALSE>>=
# Make factor level shorter
library(stringr)
get_pretty_model_factors <- function(x){
  f <- function(lvls){
    lvls <- str_replace(lvls, "^model", "")
    lvls <- str_replace(lvls, "^[A-z]+\\ ", "")

    lvls
  }

  if(class(x) == "fahrmeier_94"){
    colnames(x$state_vecs) <- f(colnames(x$state_vecs))
    return(x)
  }

  f(x)
}
@

<<model_stats, echo = FALSE, results='asis'>>=
#####
# Make data frame to find stats
library(dynamichazard)
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ model,
  data = hd_dat, by = 1, max_T = 60, use_weights = F,
  id = hd_dat$serial_number)

#####
# Make time cut variable and find # disk and # failures
tmp_dat$X$t_cut <- cut(tmp_dat$X$t, breaks = seq(0, 60, 20),
                       right = FALSE)
stats <- by(tmp_dat$X, list(tmp_dat$X$model, tmp_dat$X$t_cut), function(x){
  c("#D" = length(unique(x$serial_number)),
    "#F" = sum(tapply(x$Y, x$serial_number, any, default = 0)))
})

.names <- dimnames(stats)
stats <- sapply(stats, function(x) if(is.null(x)) c(0, 0) else x)

#####
# Format final tabel
n_models <- length(.names[[1]])
rnames <- .names[[1]]
tbl_dat <- lapply(1:length(.names[[2]]), function(i){
  x_vals <- stats[, 1:n_models + n_models * (i - 1L)]
  cnames <- paste(rownames(x_vals), .names[[2]][i])
  structure(t(x_vals), dimnames = list(rnames, cnames))
})
tbl_dat <- do.call(cbind, tbl_dat)
tbl_dat <- tbl_dat[order(tbl_dat[, 1], decreasing = TRUE), ]

# convert to string
tbl_dat <- structure(
  apply(tbl_dat, 2, sprintf, fmt = "%d"),
  dimnames = dimnames(tbl_dat))

# Find first observation and first failure and add to table
tbl_xtra <- by(hd_dat, hd_dat$model, function(x)
  c("# disks" = length(unique(x$serial_number)),
    "First failure time" = min(x$tstop[x$fails == 1])),
  simplify = FALSE)
tbl_xtra <- do.call(rbind, tbl_xtra)

tbl_xtra[, ] <- cbind(
  sprintf(tbl_xtra[, 1], fmt = "%d"),
  sprintf(tbl_xtra[, 2], fmt = "%.3f"))

rmatch <- match(row.names(tbl_dat), row.names(tbl_xtra))

tbl_dat <- cbind(tbl_xtra[rmatch, ], tbl_dat)

# Make xtable output
rownames(tbl_dat) <- get_pretty_model_factors(rownames(tbl_dat))

xtbl <- xtable(tbl_dat,
       caption = "Summary statistics for each hard disk model. The model is indicated by the row label. The number of disks is abbreviated as '\\#D' and total failures is abbreviated as '\\#F'. The columns suffixed with a $[x, y)$ indicates the numbers for this period when a discrete model is used. The total number of disks (the first column) may not seems consistent with the latter numbers. The reason is that discrete risk set in equation~\\ref{eqn:discreteRiskSet} have been used. Thus, we have not observed some hard disk long enough to be included yet.",
       label = "tab:modelDat")

align(xtbl)[] <- "r"
print_out <- capture.output(xtbl)

print_out <- sapply(
  print_out, gsub,
  pattern = "\\begin{table}[ht]",
  replacement = "\\begin{sidewaystable}\\small",
  fixed = TRUE,
  USE.NAMES = FALSE)

print_out <- sapply(
  print_out, gsub,
  pattern = "\\end{table}",
  replacement = "\\end{sidewaystable}",
  fixed = TRUE,
  USE.NAMES = FALSE)

print_out <- paste0(print_out, collapse = "\n")
cat(print_out)

# Cleanup
rm(tmp_dat, stats, tbl_dat, tbl_xtra)

#####
# Note: You can check the figures agianst https://www.backblaze.com/blog/hard-drive-failure-rates-q2-2016/
@

Information about each of the hard disk models are shown in table~\ref{tab:modelDat}. I fit the model using the EKF with a single iteration in the correction step. The \code{system.time} function is used to show the computation time. I use a natural cubic spline basis for the number of power cycles to capture potential non-linear effects. The spline is linear beyond the \code{Boundary.knots} arguments given to the \code{ns} function.

<<first_hd_fit>>=
library(splines)
library(dynamichazard)
# Define model formula
frm <- Surv(tstart, tstop, fails) ~
      -1 +              # We remove the intercept to not get a reference level
                        # for the hard disk model factor
      model +
      ns(smart_12,                   # Use a natural cubic spline for power
         knots = c(3, (1:5)*10),     # cycle count
         Boundary.knots = c(0, 60))

# Fit model
system.time(           # Used to get the computation time
  ddfit <- ddhazard(
    formula = frm,
    hd_dat,
    id = hd_dat$serial_number,
    Q_0 = diag(1, 24), # Covariance matrix for first state
    Q = diag(.01, 24), # Covariance matrix for transition
    by = 1,            # Length of intervals
    max_T = 60,        # Last time we observe when estimating
    control = list(
      method = "EKF",
      eps = .001)))
@

<<ddfit_change_fac_lvls, echo = FALSE>>=
# Make factor level shorter
ddfit <- get_pretty_model_factors(ddfit)
@

The elapsed computation time is in seconds. Figure~\ref{fig:ST3TB},~\ref{fig:otherfaclvl1} and~\ref{fig:otherfaclvl2} shows the predicted coefficients for the factor level for the models. ST3000DM001 differs from the others by being at higher odds of failure. It is interesting that some models seems to have a decreasing coefficient for the factor in figure~\ref{fig:otherfaclvl1} and~\ref{fig:otherfaclvl2} while others have an increasing. Few seems to have the "Bathtub curve" used as an idealistic example in reliability engineering~\citep{Klutke03}.  Notice that some of the curves confidence bounds gets wider in certain periods. This is because of lagging data points primarily. I only have SMART statistics for three years. Furthermore, I only have data for some model series in parts of the 60 month period due to BackBlaze purchasing patterns. Thus, we see the increasing or decreasing confidence bounds for some coefficients of factor levels in certain periods. As an example, I have little data for the 8 terabyte hard disk from Seagate denoted by ST8000DM002 as BackBlaze recently started to use these.

<<ST3TB,echo=FALSE,par_1x1 = TRUE, fig.cap="Coefficient for the model factor level ST3000DM001. It is shown in a single plot as it differs from the from the factor levels shown in figure~\\ref{fig:otherfaclvl1} and \\ref{fig:otherfaclvl2}.",cache=FALSE>>=
plot(ddfit, cov_index = 7)
@

<<ekf_single_fig_cap,echo = FALSE>>=
ekf_single_rep_fig_cap = "Cofficients for factor levels for the hard disk model with EKF with a single iteration in the correction step."
@


<<otherfaclvl1, echo = FALSE, par_3x3 = TRUE, fig.cap=ekf_single_rep_fig_cap,cache=FALSE>>=
plot(ddfit, cov_index = c(1:6, 8:10))
@

<<otherfaclvl2, echo = FALSE,par_3x3 = TRUE, fig.cap=ekf_single_rep_fig_cap,cache=FALSE>>=
plot(ddfit, cov_index = 11:17)
@

<<smart_12_plot, echo = FALSE, par_1x1 = TRUE, fig.cap="Plot of predicted terms on the linear predictor scale for different values of number of power cycles.",cache=FALSE>>=
tmp <- data.frame(
  model = rep(hd_dat$model[1], 3),
  smart_12 = c(1, 10, 40))

preds <- predict(ddfit, tmp, type = "term", sds = T)

is_ns <- grepl(
  "^ns\\(smart_12", dimnames(preds$terms)[[3]])

# Find lower and upper bounds
preds$lbs <- preds$terms[,, is_ns]  - 1.96 * preds$sds[,, is_ns]
preds$ubs <- preds$terms[,, is_ns] + 1.96 * preds$sds[,, is_ns]

cols <- c("#00F100", "#BC00BC", "#000000")
xs <- ddfit$times
plot(range(xs), range(preds$lbs, preds$ubs), type = "n",
     xlab = "Time", ylab = "Power cycle term", col = cols)
abline(h = 0, lty = 2)
matplot(xs, preds$terms[,, is_ns], type = "l", add = T, lty = 1,
        col = cols)

for(i in 1:dim(preds$terms)[2]){
  icol <- adjustcolor(cols[i], alpha.f = 0.1)
  polygon(c(xs, rev(xs)), c(preds$ubs[,i], rev(preds$lbs[,i])),
          col = icol, border = NA)
  lines(xs, preds$ubs[,i], lty = 2, col = cols[i])
  lines(xs, preds$lbs[,i], lty = 2, col = cols[i])
}

legend(
  "bottomright", bty = "n",
  legend = paste0(tmp$smart_12, " Cycles"),
  lty = rep(1, nrow(tmp)),
  col = cols,
  cex = par()$cex * 2.5)
@

<<echo=FALSE>>=
min_obs <- 50
quant <- .1
smart_12_illu_cap <- paste0("Plot showing the mean and ", quant * 100, "\\% quantile of the SMART 12 attribute for each model with those hard disk at risk at the start of each interval. The dashed lines are the quantile curves. Values for a given model in a given interval is excluded if there are less than ", min_obs, " hard disk at risk at the start of the interval. The circle radiuses are proportional to the fraction of hard disk that fail in the month. The transparency of the cirles are log proportional to the number of hard disks.")
@


<<smart_12_illu,echo = FALSE, fig.cap=paste(smart_12_illu_cap), par_1x1=TRUE,cache=FALSE>>=
library(plotrix)
library(grDevices)
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ smart_12 + model,
  data = hd_dat, by = 1, max_T = 60, use_weights = F,
  id = hd_dat$serial_number)

smart_12_mean <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$smart_12, X$t, mean))

smart_12_quant <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$smart_12, X$t, quantile, probs = .1))

n_obs <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$smart_12, X$t, length))
max_n_obs <- max(unlist(n_obs))

n_fails <- by(
  tmp_dat$X, tmp_dat$X$model, function(X)
    tapply(X$Y, X$t, sum))

fail_ratios <- mapply("/", n_fails, n_obs)

plot(c(1, 60), c(0, max(unlist(smart_12_mean))), type = "n",
     xlab = "Month", ylab = "Number of power cycles")

for(i in seq_along(smart_12_mean)){
  col <- if(names(smart_12_mean)[i] == "ST3000DM001") "Red" else "Black"

  # We remove the points with less than x observations
  is_in <- n_obs[[i]] > min_obs
  y_mean <- smart_12_mean[[i]][is_in]
  y_quant <- smart_12_quant[[i]][is_in]
  x <- as.integer(names(smart_12_mean[[i]]))[is_in]

  fail_ratio <- fail_ratios[[i]]
  radius <- fail_ratio[is_in] * 10 + 1e-3

  lines(x, y_mean, col = col)
  lines(x, y_quant, col = col, lty = 2)

  n_obs_use <- n_obs[[i]][is_in]
  for(j in seq_along(x)){
    col_circle <- adjustcolor(
      col, alpha.f = 1 - .75 * (log(max_n_obs) - log(n_obs_use[j])) /
        (log(max_n_obs) - log(min_obs)))
    draw.circle(x[j], y_mean[j], radius[j],
                col = col_circle, border = col_circle)
  }
}

rm(tmp_dat, smart_12_mean, n_obs)
@

Figure~\ref{fig:smart_12_plot} shows the predicted terms for different levels of the count power cycles. At first glance, it seems odd that the risk of failure is not monotone in the number of power cycles. However, this is may be due to ST3000DM001 model. Figure~\ref{fig:smart_12_illu} shows mean number of power cycles for each model series with those hard disk of the given model at risk at the start of the month. The dashed lines are the $\Sexpr{quant * 100}$ pct. quantile. The ST3000DM001 models is shown in red. Both the mean and the quantile stands out for ST3000DM001. This may also explain why some of the model factor coefficients have sharp changes. Thus, I refit the model without ST3000DM001. The plot for the new model is shown in figure~\ref{fig:subset_smart_12_plot}. The new plot shows a close to monotone effect as expected. The pointwise confidence bounds are though rather wide despite the large data set.

% CHECK: arguments match

<<echo = FALSE>>=
hd_dat_sub <- hd_dat
hd_dat_sub <- hd_dat_sub[hd_dat_sub$model != "ST3000DM001", ]
hd_dat_sub$model <- droplevels(hd_dat_sub$model)

new_call <- ddfit$call
new_call$data <- as.name(quote(hd_dat_sub))
new_call$id <- as.call(quote(hd_dat_sub$serial_number))
new_call$Q_0 <- as.call(quote(diag(1, 23)))
new_call$Q <- as.call(quote(diag(.01, 23)))

ddfit <- eval(new_call)

ddfit <- get_pretty_model_factors(ddfit)
@

<<subset_smart_12_plot,echo=FALSE,ref.label='smart_12_plot', par_1x1 = TRUE,fig.cap="Similar plot to figure~\\ref{fig:smart_12_plot} for the model without the ST3000DM001 hard disk model.",cache=FALSE>>=
@

\subsubsection{Examples with more iterations with EKF}
Next, I will look at taking more iterations in the correction step. Thus, I estimate the model again:

% Check: arguments

<<subset_EKF_xtra>>=
system.time(
  ddfit_xtr <- ddhazard(
    formula = frm,
    data = hd_dat_sub,     # data set without ST3000DM001
    id = hd_dat_sub$serial_number,
    Q_0 = diag(1, 24 - 1), # -1 due to removal of a factor level
    Q = diag(.01, 24 - 1),
    by = 1,
    max_T = 60,
    control = list(
      method = "EKF",
      eps = .001,
      NR_eps = .001))) # tolerance for extra iterations in correction step
@

The previous fit in symbol \code{ddfit} have been also been replaced with a fit the data set without the model ST3000DM001. Consequently, the fits can be compared w.r.t. the first 9 factor levels with the following call:

<<subset_EKF_xtra_vs_wo, par_3x3 = TRUE, fig.cap="Comparison of results with and without extra iterations in the correction step with the EKF. The red curve is the estimate with extra iterations.",cache=FALSE>>=
for(i in 1:9){
  # Shown in figure \ref&fig:subset_EKF_xtra_vs_wo;
  plot(ddfit, cov_index = i)
  plot(ddfit_xtr, cov_index = i, add = TRUE, col = "red")
}
@

The plot is is in figure~\ref{fig:subset_EKF_xtra_vs_wo}. There is a noticeable difference between the two for some of the coefficients. Those coefficients that differ are the factors levels with a sparse amount of data in some periods. You can see this by looking at table~\ref{tab:modelDat}. % CHECK: there is a difference and it is noticeable when you look at the table that it is the levels with sparse data

Another difference is the number of iterations the two methods take of the EM-algorithm. I print the number of iterations below. The method with extra iterations take fewer EM-iterations which explains the comparable computation time.

<<show_n_iter>>=
c(one_it = ddfit$n_iter, more_its = ddfit_xtr$n_iter)
@

<<EKF_cleanup, echo = FALSE>>=
rm(ddfit_xtr, hd_dat)
@


\subsection{Unscented Kalman filter}\label{subsec:UKF}
The Unscented Kalman Filter (UKF) is similar to Monte Carlo methods but using deterministically selected state vectors and weights instead of randomly drawn. The vectors and weights are respectively referred to as \emph{sigma points} and \emph{sigma weights}. The advantages of using an UKF is a potentially better approximation than the linear approximation used in the EKF. The UKF is at a lower computational cost than Monte Carlo methods. Further, the UKF does not require that one computes the Jacobian. The former two advantages are useful in this package while the latter is not a great advantage since deriving and computing the Jacobian is not complicated for the models. The UKF was introduced in \cite{Julier97}. The method is shown in algorithm~\ref{alg:UKF}.

\begin{algorithm}
\caption{The Unscented Kalman Filter (UKF) where $\displaystyle\emNotee{\mat{V}}{t}{t - 1}^{1/2}$ denotes the square root matrix of $\emNotee{\mat{V}}{t}{t - 1}$ and $\left(\displaystyle\emNotee{\mat{V}}{t}{t - 1}^{1/2}\right)_j$ denotes the $j$'th column of the square root matrix. $\diag{\cdot}$ gives a diagonal matrix with the entries of the argument vector in the diagonal. $q$ is the dimension of the state vector. $\vec{W}^{(\cdot)}$ is the vector with elements $W^{(\cdot)}_0,W^{(\cdot)}_1,\dots,W^{(\cdot)}_{2q}$.}\label{alg:UKF}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0, \vec{a}_0,$ $\mat{X}_1,\dots,\mat{X}_d,$ $\vec{y}_1,\dots,\vec{y}_d,$ $R_1,\dots,R_d$
\Statex Hyperparameters $\alpha$, $\beta$ and $\kappa$
\State Set $\emNotee{\vec{a}}{0}{0} = \vec{a}_0$ and $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$
\Statex Compute \emph{sigma weights} with $\lambda = \alpha^2 (q + \kappa) - q$
\State $W_0^{[m]} = \frac{\lambda}{q + \lambda}$\label{alg:UKF:weightsSta}
\State $W_0^{[c]} = \frac{\lambda}{q + \lambda} + 1 - \alpha^2 + \beta$
\State $W_0^{[cc]} = \frac{\lambda}{q + \lambda} + 1 - \alpha$
\State $W_j^{[m]} = W_j^{[c]} = \frac{1}{2(q+\lambda)}, \qquad j = 1,\dots, 2q$\label{alg:UKF:weightsSto}
\For{$t=1,2,\dots,d$}
\Procedure{Prediction step}{}
\State $\emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}$
\State $\emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^\top + \mat{R}\mat{Q}\mat{R}^\top$
\EndProcedure
\Procedure{Correction step}{}
\StateXX Compute \emph{sigma points}
\State $\begin{aligned}
  &\hvec{a}_0 = \emNotee{\vec{a}}{t}{t-1} \\
  &\hvec{a}_{j} = \emNotee{\vec{a}}{t}{t-1} + \sqrt{q + \lambda} \left(\emNotee{\mat{V}}{t}{t - 1}^{1/2}\right)_j \\
  &\hvec{a}_{j + q} = \emNotee{\vec{a}}{t}{t - 1} - \sqrt{q + \lambda} \left(\emNotee{\mat{V}}{t}{t - 1}^{1/2}\right)_j
\end{aligned} \qquad j = 1,2,\dots, q$ \label{alg:UKF:points}
\StateXX Compute intermediates
\State $\hvec{y}_j = \vec{z}_t \left(\hvec{a}_j \right) \qquad j = 0,1,\dots, 2q$\label{alg:UKF:expecMean}
\State $\hmat{Y} = (\hvec{y}_0, \dots, \hvec{y}_{2q})$
\State $\overline{\vec{y}} = \sum_{j = 0}^{2q} W_j^{[m]} \vec{y}_j$\label{alg:UKF:mean}
\State $\Delta\hmat{Y} = \hmat{Y} - \overline{\vec{y}} \vec{1}^\top$
\State $\hmat{H} = \xi\mat{I} + \sum_{j=0}^{2q} W_j^{[c]}\mat{H}_t(\hvec{a}_j)$ \label{alg:UKF:obsCov}
\State $\Delta\hmat{A} = (\hvec{a}_0, \dots, \hvec{a}_{2q}) - \emNotee{\vec{a}}{t}{t-1}\vec{1}^\top$
\State $\tvec{y} = \Delta \hmat{Y}^\top \hmat{H}^{-1}(\vec{y}_t - \overline{\vec{y}})$\label{alg:UKF:residual}
\State $\mat{G} = \Delta\hmat{Y}^\top\hmat{H}^{-1}\Delta\hmat{Y}$\label{alg:UKF:G}
\State $\vec{c} = \tilde{\vec{y}} - \mat{G}\left( \diag{\vec{W}^{(m)}}^{-1} + \mat{G}\right)^{-1} \tvec{y}$ \label{alg:UKF:InterC}
\State $\mat{L} = \mat{G} - \mat{G}\left( \diag{\vec{W}^{(c)}}^{-1} + \mat{G}\right)^{-1} \mat{G}$ \label{alg:UKF:InterG}
\StateXX Compute updates
\State $\emNotee{\vec{a}}{t}{t} = \emNotee{\vec{a}}{t}{t - 1} + \Delta\hmat{A}\diag{\vec{W}^{(cc)}}\vec{c}$
\State $\emNotee{\mat{V}}{t}{t} = \emNotee{\mat{V}}{t}{t - 1} -
    \Delta\hmat{A}\diag{\vec{W}^{(cc)}}\mat{L}\diag{\vec{W}^{(cc)}}\Delta\hmat{A}^\top$
\EndProcedure
\EndFor
\end{algorithmic}
\end{algorithm}

\code{ddhazard} uses the Cholesky decomposition for the square root matrix $\displaystyle\emNotee{\mat{V}}{t}{t - 1}^{1/2}$. The hyperparameters that the sigma points and sigma weights depend on can have values $0 < \alpha \leq 1$, $\kappa\in\mathbb{R}$ and $\beta\in\mathbb{R}$ under the restriction that $q + \lambda \geq 0$. I will show a small example to illustrate the intuition of the sigma points in~\citeAlgLine{alg:UKF:points}{alg:UKF} and sigma weights in~\citeAlgLineTo{alg:UKF:weightsSta}{alg:UKF:weightsSto}{alg:UKF}. Suppose we are at time $t$ of the correction step of the filter with with a two dimensional state equation, $q = 2$. Further, assume that we have:

\begin{equation}\label{eqn:UKFEx}
\emNotee{\vec{a}}{t}{t - 1} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \qquad
  \emNotee{\mat{V}}{t}{t - 1} = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}, \qquad
  \emNotee{\mat{V}}{t}{t - 1}^{1/2} = \begin{pmatrix} 1.41 & 0.707 \\ 0 & 0.707 \end{pmatrix}
\end{equation}

Then the following hyperparameters leads to the following weights:

\begin{equation}\label{eqn:UKFParamEx}
\begin{aligned}
  (\alpha,\beta,\kappa) = (1,0,1) &\quad \Rightarrow &&
    \quad \Lparen{\lambda, W_0^{[m]}, W_1^{[m]}, \dots, W_{2q}^{[m]}} =
      (1,1/3,1/6,\dots,1/6) \\
  (\alpha,\beta,\kappa) = (1/3,0,1) &\quad \Rightarrow &&
    \quad \Lparen{\lambda, W_0^{[m]}, W_1^{[m]}, \dots, W_{2q}^{[m]}}  =
     (-1,-1,1/2,\dots,1/2) \\
\end{aligned}
\end{equation}

% CHECK: Parameters above match with code

<<sigma_pts,echo=FALSE, par_1x1=TRUE, fig.cap = "Illustration of sigma points in the example from equation~\\ref{eqn:UKFEx}. The dashed lines are the contours of the density given by $\\emNotee{\\vec{a}}{t}{t - 1}$ and $\\emNotee{\\mat{V}}{t}{t - 1}$. The full lines are the direction given by the columns of the Cholesky decomposition. The filled circles are sigma points with $(\\alpha,\\beta,\\kappa) = (1,0,1)$ and the open circles are the sigma points with $(\\alpha,\\beta,\\kappa) = (1/3,0,1)$. The point at $(0,0)$ is a sigma point for both sets for hyperparameters.",cache=FALSE>>=
library(mvtnorm)
set.seed(7912351)
x.points <- seq(-3, 3,length.out=100)
y.points <- x.points
z <- matrix(0,nrow=100,ncol=100)
mu <- c(0,0)
sigma <- matrix(c(2,1,1,1),nrow=2)
for (i in 1:100) {for (j in 1:100) {
  z[i,j] <- dmvnorm(c(x.points[i],y.points[j]),
                    mean=mu,sigma=sigma)
}}


plot(c(-3, 3), c(-3, 3), xlab = "", ylab = "", type = "n",
     xlim = c(-3, 3), ylim = c(-3, 3))
contour(x.points, y.points, z, nlevels = 10,
        drawlabels = FALSE, axes = FALSE,
        frame.plot = FALSE, add = TRUE,
        lty = 3)


# Compute Cholesky decomposition
decomp <- chol(sigma)
abline(h = 0)
abline(a = 0, b = decomp[1, 2] / decomp[2, 2])

# Add two sets of sigma points
q <- 2
l1 <- 1
l2 <- -1

pts <- rbind(
  c(0, 0),
  sqrt(q + l1) * t(decomp),
  - sqrt(q + l1) * t(decomp),
  sqrt(q + l2) * t(decomp),
  - sqrt(q + l2) * t(decomp))

points(pts[, 1], pts[, 2],
       pch = c(rep(16, 5), rep(1, 4)), cex = par()$cex * 3)
@

Decreasing $\alpha$ increases the absolute size of the weights and can lead to a negative weight on the zero sigma point, $\hvec{a}_0$. $\alpha$ also controls the spread of the sigma points through the factor $\sqrt{q + \lambda}$ in~\citeAlgLine{alg:UKF:points}{alg:UKF}. Decreasing $\alpha$ decreases the spread of the sigma points. This is illustrated in figure~\ref{fig:sigma_pts}. The filled circles are the sigma points with $(\alpha,\beta,\kappa) = (1,0,1)$ and the open circles are the sigma points with $(\alpha,\beta,\kappa) = (1/3,0,1)$.

Different definitions of sigma points and sigma weights, so called \emph{sigma sets}, have been suggested. Some of these sets is what is referred to as skewed sets while others are symmetric like the set presented in this paper. The $\alpha$ parameter in~\cite{Julier04} is used to mitigate the effect of incorrectly not choosing a skewed sigma set (not symmetrical) when the state's distribution does not have a skew or vice versa. In these cases letting $\alpha \rightarrow 0^+$ can mitigate the error. However, if $\alpha$ is small then the weight of the zero sigma point can be negative, $W_0^{[m]}<0$. While it is not immediately clear from algorithm~\ref{alg:UKF}, this can cause computational issues as pointed out in~\cite{menegaz16}. This is easily seen in formulation of the UKF in~\cite{Julier04} where the covariance matrix of the observational equation can become negative definite when the weight of the zero sigma point is less than zero. This is shown in the ddhazard vignette in this package. Thus, $W_0^{[m]}$ is chosen to be positive if $\kappa$ is not specified by setting $\kappa = q (1 + \alpha^2 (W_0^{[m]} -1) / (\alpha^2 (1 - W_0^{[m]}))$ where $W_0^{[m]}>0$ is the wanted value of $W_0^{[m]}$.

\code{ddhazard} uses an equivalent specification of the three hyperparameter UKF as in \cite{Julier04} given in \cite{Wan00}. \cite{Gustafsson12} show that the claimed second order precision of first two moments is not true for one set of hyperparameter settings in \cite{Julier04} by comparison with a second order extended Kalman filter. Many different UKFs have been suggested with different hyperparameters, algorithms and sigma points. See \cite{menegaz16} for a comparison of different forms of UKFs in the literature. This is also where the arguments for the weights in~\citeAlgLineTo{alg:UKF:weightsSta}{alg:UKF:weightsSto}{alg:UKF} are given. Algorithm~\ref{alg:UKF} is derived with the weight specification in~\cite{menegaz16} with the UKF from~\cite{Wan00} and applying the Woodbury matrix identity.

Algorithm~\ref{alg:UKF} involves at most products of $q\times n_t$ and $n_t \times q$ matrices, inversion of $q\times q$ matrices and an inversion of $\hmat{H}$ which is a diagonal matrix which can be computed in $\bigO{n_t}$ time. Thus, the filter has a computational complexity of $\bigO{n_t}$. The algorithm can not as easily be done in parallel due to~\citeAlgLineTwo{alg:UKF:InterC}{alg:UKF:InterG}{alg:UKF}. However, parts of the computations (particularly~\citeAlgLineTo{alg:UKF:expecMean}{alg:UKF:obsCov}{alg:UKF}) can be done in parallel but this is not implemented in the current version of \code{ddhazard}. Further, a multi-threaded \pkg{BLAS} can decrease the computation time of the products in~\citeAlgLineTwo{alg:UKF:InterC}{alg:UKF:InterG}{alg:UKF}. I make the addition of identity matrix times $\xi$ to reduce the effect of observation predicted near the boundary of the outcome space in~\citeAlgLine{alg:UKF:obsCov}{alg:UKF} as in the EKF. I will end this section on the UKF with an example.

\subsubsection{Example with UKF}
<<define_ekf_low_Q_0, echo = FALSE>>=
ukf_ex_Q_0 <- 0.1
@


One problem with the UKF compared to the EKF is that it is more sensitive to the choice of $\mat{Q}_0$. The reason is that $\mat{Q}_0$ is used in~\citeAlgLine{alg:UKF:points}{alg:UKF} to compute the first set of sigma points at time $t=1$. I will illustrate this in the following paragraphs. I fit the model below and plot the predicted coefficients. I set $\mat{Q}_0$ to a diagonal matrix with large entries as before. I specify that I want the UKF by setting the element \code{method = "UKF"} in the list to the \code{control} argument of \code{ddhazard}. Figure~\ref{fig:ukf_large_Q_0} shows the result. Figure~\ref{fig:ukf_small_Q_0} shows the same model but with $\mat{Q}_0$'s diagonal entries equal to $\Sexpr{ukf_ex_Q_0}$. The latter figure is comparable to what we have seen previously.

% CHECK: arguments match
% CHECK: claims above is correct

<<ukf_large_Q_0_comp>>=
ddfit_ukf <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(10, 24 - 1), # Larger value
  Q = diag(.01, 24 - 1),
  by = 1,
  max_T = 60,
  control = list(
    method = "UKF",        # Use the UKF
    eps = .01))            # Decreased to get a fit
@

<<ukf_large_Q_0_change_fac_names, echo = FALSE>>=
ddfit_ukf <- get_pretty_model_factors(ddfit_ukf)
@


<<ukf_large_Q_0, par_3x3=TRUE, fig.cap = "Predicted coefficients with the UKF used on the hard disk failure dataset where $\\mat{Q}_0$ has large entries in the diagonal.",cache=FALSE>>=
# Shown in figure \ref&fig:ukf_large_Q_0;
plot(ddfit_ukf, cov_index = 1:9)
@

In contrast, the EKF with one iteration in the correction step can have large in the diagonal entries. I illustrate this by re-fitting the \code{ddfit} object using the \code{call} element of the object after altering the \code{Q_0} argument:

<<show_EKF_can_have_large_comp>>=
new_call <- ddfit$call
new_call$Q_0 <- diag(100000000, 23)
ddfit_large_Q_0 <- eval(new_call)
@

<<show_EKF_can_have_large_change_fac_names, echo = FALSE>>=
ddfit_large_Q_0 <- get_pretty_model_factors(ddfit_large_Q_0)
@

<<show_EKF_can_have_large, par_3x3=TRUE, fig.cap = "Predicted coefficients with the EKF with one iteration in the correction used on the hard disk failure dataset with $\\mat{Q}_0$ has low and large entries in the diagonal. The red curves are the estimates with a large values in the diagonal.",cache=FALSE>>=
for(i in 1:9){
  # Shown in figure \ref&fig:show_EKF_can_have_large;
  plot(ddfit_large_Q_0, cov_index = i, col = "red")
  plot(ddfit, cov_index = i, add = TRUE)
}
@

The plot is shown in figure~\ref{fig:show_EKF_can_have_large}. Notice that some of the predicted the coefficients have wider confidence bounds in the start. These are the models who have the first failure quite late or where we have few cases to start in the first period. This is shown in table~\ref{tab:modelDat}.

<<ukf_small_Q_0_est, echo = FALSE>>=
new_call <- ddfit_ukf$call
new_call$Q_0 <- diag(ukf_ex_Q_0, 23)
ctrl <- eval(new_call$control)
ctrl$eps <- .001
new_call$control <- ctrl
ddfit_ukf <- eval(new_call)
@


<<ukf_small_Q_0, par_3x3=TRUE, echo = FALSE,fig.cap = paste0("Similar plot to figure~\\ref{fig:ukf_large_Q_0} but where the diagonal entries of $\\mat{Q}_0$ are $", ukf_ex_Q_0, "$. The black curve is the estimates from the the EKF with one iteration in the correction step."),cache=FALSE>>=
for(i in 1:9){
  plot(ddfit, cov_index = i)
  plot(ddfit_ukf, cov_index = i, add = TRUE, col = "red")
}
@

A mean square bound for the estimation error of the observational equation is given in~\cite{Xiong06} for the UKF. Their finding is that the mean square for the observational equation remains bounded when $\mat{Q}$ and $\mat{Q}_0$ has an matrix $\delta \mat{I}$ added to them with a sufficiently large $\delta$. Though, taking $\delta$ value that is too large increases the error bound. However, their analysis is where the observational equation is linear and Gaussian while the state equation is non-linear with Gaussian additive noise. Consequently, it is the reverse of the models implemented in \pkg{dynamichazard} where we have a linear and Gaussian state equation and non-linear observational equation. Moreover, the noise is not Gaussian in the observational equation. It is only in a filtering setting where $\mat{Q}$ is fixed and not estimated. Nevertheless, it may explain the issue we have with selecting in $\mat{Q}_0$. We need to select a matrix that is \emph{large} but not \emph{too large}.

<<ukf_cleanup, echo = FALSE>>=
rm(ddfit_large_Q_0, ddfit_ukf)
@

\subsection{Sequential approximation of the posterior modes}\label{subsec:postApprox}
Another idea is to replace the means in the filters with the modes in each correction step. Making this replacement, we need to find the minimum of equation~\ref{eqn:modeExact} followed by an update of the covariance matrix.
\begin{equation}\label{eqn:modeExact}
\emNotee{\vec{a}}{t}{t} = \argminu{\vec{\alpha}}
  - \log \proppCond{\vec{\alpha}}{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
    -\sum_{(i,j) \in R_t} \log\proppCond{y_{it}}{\vec{\alpha}}
\end{equation}

One way of finding an approximate minimum is to replace the equation~\ref{eqn:modeExact} with $n_t$ rank-one updates of the form in equation~\ref{eqn:modeApprox} and an update of the covariance matrix. I will use the superscript to indicate the previous estimate.
\begin{equation}\label{eqn:modeApprox}
\emNote{\vec{a}}{t}{t}{k} = \argminu{\vec{\alpha}}
  -\log \proppCond{\vec{\alpha}}{\emNote{\vec{a}}{t}{t}{k-1}, \emNote{\mat{V}}{t}{t}{k-1}}
  -\log\proppCond{y_{it}}{\vec{\alpha}}
\end{equation}

I will refer to this method as the Sequential Mode Approximation (SMA). Two algorithms for doing this are shown in algorithm~\ref{alg:approxMode} and~\ref{alg:approxModeChol}. The latter replaces the correction step in the first by propagating a Cholesky decomposition of the information matrix but is otherwise identical. The advantage of using the Cholesky decomposition in algorithm~\ref{alg:approxModeChol} is that we ensure that the covariance matrix is positive semi definite. Algorithm~\ref{alg:approxModeChol} is slower although I do use that \citeAlgLine{alg:approxModeChol:LInvUpdate}{alg:approxModeChol} can be inverted in $\bigO{q^2}$ since $\mat{L}$ is a triangular matrix. Further, the rank-one update of the Cholesky decomposition in~\citeAlgLine{alg:approxModeChol:LUpdate}{alg:approxModeChol} is done using the Fortran code from~\cite{LAPACKThread} which implements the method described in~\cite{seeger04} with a time complexity of~$\bigO{q^2}$. Lastly, I exploit that $\tmat{L}$ is a triangular matrix to reduce the computational cost of the matrix and vector products. Still the correction step with algorithm~\ref{alg:approxModeChol} is slower. The constant $v$ in \citeAlgLine{alg:approxMode:findConst}{alg:approxMode} and \citeAlgLine{alg:approxModeChol:findConst}{alg:approxModeChol} is solved by the Newton Raphson method. $\proppCond{y_{it}}{\vec{\alpha}_t^\top \vec{x}_{ij} = b}$ is log concave in $b$ with a finite upper bound for the models currently implemented in \code{ddhazard} so the Newton Raphson method finds the unique minimum. Both methods are $\bigO{n_tq^2}$ in each prediction and correction step.

\begin{algorithm}
\caption{Sequential approximation of the posterior mode.}\label{alg:approxMode}
\begin{algorithmic}[1]\raggedright
\INPUT
\Statex $\mat{Q},\mat{Q}_0, \vec{a}_0,$ $\mat{X}_1,\dots,\mat{X}_d,$ $\vec{y}_1,\dots,\vec{y}_d,$ $R_1,\dots,R_d$
\Statex Learning rate $\zeta_0$
\Statex Set $\emNotee{\vec{a}}{0}{0} = \vec{a}_0$ and $\emNotee{\mat{V}}{0}{0} = \mat{Q}_0$
\For{$t=1,2,\dots,d$}
\Procedure{Prediction step}{}
\State $\emNotee{\vec{a}}{t}{t - 1} = \mat{F} \emNotee{\vec{a}}{t - 1}{t - 1}$
\State $\emNotee{\mat{V}}{t}{t - 1} = \mat{F}\emNotee{\mat{V}}{t - 1}{t - 1}\mat{F}^\top + \mat{R}\mat{Q}\mat{R}^\top$
\EndProcedure
\Procedure{Correction step}{} \label{alg:approxMode:correction}
\StateXX Set $\emNote{\mat{V}}{t}{t}{0} = \emNotee{\mat{V}}{t}{t - 1}$ and $\emNote{\vec{a}}{t}{t}{0} = \emNotee{\vec{a}}{t}{t - 1}$
\For{$k=1,2,\dots,n_t$}
\StateXXX Set $(i,j)$ to the $k$'th entry of $R_t$
\State $d_1 = \frac{1}{\vec{x}_{ij}^\top \emNote{\mat{V}}{t}{t}{k-1}\vec{x}_{ij}}$
\State $d_2 = \vec{x}_{ij}^\top \emNote{\vec{a}}{t}{t}{k-1}$
\State $ v = \argminu{b} b^2 \frac{1}{2}d_1 - b d_1d_2 - \log\proppCond{y_{it}}{\vec{\alpha}_t^\top \vec{x}_{ij} = b}$\label{alg:approxMode:findConst}
\State $g =  -\left. \frac{\log \proppCond{y_{it}}{\vec{x}_{ij}^\top \vec{\alpha} = b}}{\partial b^2} \right|_{b = v}$
\State $\emNote{\vec{a}}{t}{t}{k} = \emNote{\vec{a}}{t}{t}{k-1} - (d_1 - v) d_2 \zeta_0 \emNote{\mat{V}}{t}{t}{k-1}\vec{x}_{ij}$
\State $\emNote{\mat{V}}{t}{t}{k} = \emNote{\mat{V}}{t}{t}{k-1} - \frac{\emNote{\mat{V}}{t}{t}{k-1}\vec{x}_{ij} g \vec{x}_{ij}^\top\emNote{\mat{V}}{t}{t}{k-1}}{1 + g / d_1}$
\EndFor
\StateXX Set $\emNotee{\vec{a}}{t}{t} = \emNote{\vec{a}}{t}{t}{n_t}$ and $\emNotee{\mat{V}}{t}{t} = \emNote{\mat{V}}{t}{t}{n_t}$
\EndProcedure
\EndFor
\end{algorithmic}
\end{algorithm}

A disadvantage of SMA is that it is sequential and all matrix and vector products are in dimension $q\times q$ and $q$. Thus, there is little to gain from doing the computations in parallel unless $q$ is large. Moreover, the results depends on the order of the risk set. For this reason, the risk sets are permuted once before running the algorithm. This can be avoided by setting \code{permu = FALSE} to the \code{control} argument of \code{ddhazard}. One advantage is that we do not need to compute an expected value in each interval ($h(\eta)$ in~\citeAlgLine{alg:EKF:scoreVec}{alg:EKF} and $\overline{\vec{y}}$ in~\citeAlgLine{alg:UKF:mean}{alg:UKF}) with the SMA but instead work with the likelihood. The latter is particularly useful for the continuous time model I cover in section~\ref{subsec:contTime} as I avoid the definition of an outcome variable and instead work with the likelihood.

\begin{algorithm}
\caption{Alternative correction step in the procedure at~\citeAlgLine{alg:approxMode:correction}{alg:approxMode} with a Cholesky decomposition. Left-arrow, $\leftarrow$, indicates an update instead of an equality.}\label{alg:approxModeChol}
\begin{algorithmic}[1]\raggedright
\State Compute the Cholesky decomposition $\mat{L}\mat{L}^\top = \Lparen{\emNotee{\mat{V}}{t}{t-1}}^{-1}$ and $\tmat{L}=\Lparen{\mat{L}^{-1}}^\top$\label{alg:approxMode:setup}
\For{$k=1,2,\dots,n_t$}
\StateX Set $(i,j)$ to the $k$'th entry of $R_t$
\State $\tvec{x}_{ij} = \tmat{L}^\top\vec{x}_{ij}$
\State $d_1 = \frac{1}{\tvec{x}_{ij}^\top \tvec{x}_{ij}}$
\State $d_2 = \vec{x}_{ij}^\top \emNote{\vec{a}}{t}{t}{k-1}$
\State $v = \argminu{b} b^2 \frac{1}{2}d_1 - b d_1d_2 - \log\proppCond{y_{it}}{\vec{\alpha}_t^\top \vec{x}_{ij} = b}$\label{alg:approxModeChol:findConst}
\State $g =  -\left. \frac{\log \proppCond{y_{it}}{\vec{x}_{ij}^\top \vec{\alpha} = b}}{\partial b^2} \right|_{b = v}$
\State $\emNote{\vec{a}}{t}{t}{k} = \emNote{\vec{a}}{t}{t}{k-1} - (d_1 - v) d_2 \zeta_0 \tmat{L}\tvec{x}_{ij}$
\State $\mat{L}\mat{L}^\top \leftarrow \mat{L}\mat{L}^\top + \vec{x}_{ij} g \vec{x}_{ij}^\top$\label{alg:approxModeChol:LUpdate}
\State $\tmat{L}=\Lparen{\mat{L}^{-1}}^\top$\label{alg:approxModeChol:LInvUpdate}
\EndFor
\Statex Set $\emNotee{\vec{a}}{t}{t} = \emNote{\vec{a}}{t}{t}{n_t}$ and $\emNotee{\mat{V}}{t}{t} = \tmat{L}\tmat{L}^\top$
\end{algorithmic}
\end{algorithm}

\subsection{Global mode approximation}\label{subsec:GMA}
We can also directly minimize equation~\ref{eqn:modeExact}. I denote this as the Global Mode Approximation (GMA). This is equivalent to an L2 penalized Generalized Linear Models (GLM) since we only use models from the exponential family. This can be done with the usual iteratively reweighed ridge regression. Every iteration can be done in $\bigO{n_tq^2 + q^3}$. I will show the estimation in the following paragraphs with a Newton method. First, I will denote the gradient and the Hessian by:

\begin{equation}\begin{aligned}
	 &\tilde h(\vec{\alpha}) =  - \log \proppCond{\vec{\alpha}}{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
    -\sum_{(i,j) \in R_t} \log\proppCond{y_{it}}{\vec{\alpha}} \\
%
	&\begin{aligned}
	\tvec{g}(\vec{\alpha}) =  \tilde h'(\vec{\alpha})
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} \Lparen{\vec{\alpha} - \emNotee{\vec{a}}{t}{t-1}}
      - \left. \sum_{(i,j) \in R_t} \frac{\partial\log\proppCond{y_{it}}{\vec{\alpha}'}}{\partial \vec{\alpha}'} \right|_{\vec{\alpha}' = \vec{\alpha}} \\
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} \Lparen{\vec{\alpha} - \emNotee{\vec{a}}{t}{t-1}}
		  - \mat{X}_t^\top \underbrace{\algGMApPrime}_{c'(\vec{\alpha})} \\
%
	\tmat{G}(\vec{\alpha}) = \tilde h''(\vec{\alpha})
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} - \left. \sum_{(i,j) \in R_t} \frac{\partial^2\log\proppCond{y_{it}}{\vec{\alpha}'}}{\partial \vec{\alpha}'\partial \Lparen{\vec{\alpha}'}^\top} \right|_{\vec{\alpha}' = \vec{\alpha}} \\
		& = \emNotee{\mat{V}}{t}{t-1}^{-1} - \mat{X}_t^\top \underbrace{\algGMApPrimePrime}_{c''(\vec{\alpha})}\mat{X}_t
\end{aligned}\end{aligned}\end{equation}

Thus, the update equation is:
\begin{equation}
\begin{aligned}
\vec{a}^{(k)} &= \vec{a}^{(k - 1)} + \zeta_0 \Lparen{\tmat{G}(\vec{a}^{(k - 1)})}^{-1}\Lparen{-\tmat{g}(\vec{a}^{(k - 1)})} \\
%	&= \Lparen{\tmat{G}(\vec{a}^{(k - 1)})}^{-1}\Lparen{
%		- \mat{X}_t^\top  c''(\vec{\alpha}^{(k-1)}) \mat{X}_t  \vec{a}^{(k - 1)}
%		+ (1-\zeta_0)   \emNotee{\mat{V}}{t}{t-1}^{-1} \vec{a}^{(k-1)}
%		+ \emNotee{\mat{V}}{t}{t-1}^{-1} \emNotee{\vec{a}}{t}{t-1}
%		+ \zeta_0 \mat{X}_t^\top c'(\vec{\alpha}^{(k-1)})} \\
	& \algGMAscore{\ =}
\end{aligned}
\end{equation}

The final algorithm for the correction step with the GMA is shown in algorithm~\ref{alg:GlobalMA}.

\begin{algorithm}
\caption{Correction step with global mode approximation by Newton Raphson.}\label{alg:GlobalMA}
\begin{algorithmic}[1]\raggedright
\Statex Set $\vec{a}^{(0)} =\emNotee{\vec{a}}{t}{t-1}$ and define:
\Statex $c'(\vec{\alpha}) = \algGMApPrime$
\Statex $c''(\vec{\alpha}) = \algGMApPrimePrime$
\Repeat
\State %
$\algGMAscore{\vec{a}^{(k)} =}$\label{alg:GlobalMA:Update}
\UntilElse{$\LVert{\vec{a}^{(k)} - \vec{a}^{(k-1)}}/ (\LVert{\vec{a}^{(k-1)}} + \delta) < \epsilon$}{Set $k\leftarrow k + 1$}
\State $\emNotee{\mat{V}}{t}{t} = \Lparen{\tmat{G}(\vec{a}^{(k - 1)})}^{-1}$
\end{algorithmic}
\end{algorithm}

An alternative to algorithm~\ref{alg:GlobalMA} for the exponential family is to re-write the original problem to use working responses to get a weighted least squares problem of the form:

\begin{equation}\label{eqn:GMAAlt}
\begin{aligned}
&\vec{b} = \mat{X}_t  \vec{a}^{(k-1)} + \vec{h}'\Lparen{\mat{X}_t  \vec{a}^{(k-1)}}^{-1}\Lparen{\vec{y}_t - \vec{h}\Lparen{\mat{X}_t  \vec{a}^{(k-1)}}}   \\
%
&\vphantom{\LVert{\underbrace{\eqnGblModeTerma}_{\tmat{W}}}}
%
\argmin_{\vec{\alpha}}\LVert{
\smash{\underbrace{\eqnGblModeTerma}_{\tmat{C}^{1/2}}}\vphantom{\eqnGblModeTerma}
 \Lparen{
  \smash{\underbrace{\eqnGblModeTermb}_{\tmat{X}_t}}\vphantom{\eqnGblModeTermb} \vec{\alpha}
  - \smash{\underbrace{\eqnGblModeTermc}_{\tvec{b}}}\vphantom{\eqnGblModeTermc}}}
\end{aligned}\end{equation}%
%
where $\vec{b}$ is the working responses, $\vec{h}$ temporarily denotes the inverse link function and the derivative $\vec{h}'$ is with respect to the state vector $\vec{\alpha}$. The inverse link function $\vec{h}$ implicitly depends on the risk set at time $t$. I show in the ddhazard vignette that one iteration of solving~\ref{eqn:GMAAlt} is equivalent to the EKF with a single iteration in the correction step.

Algorithm~\ref{alg:GlobalMA} is sensitive to the choice of $\mat{Q}_0$. An extreme example is if we have no events in the first interval and only an intercept. Then setting $\mat{Q}_0$ to a diagonal matrix with large entries (in this case $\mat{Q}_0$ is a scalar) implies almost no restrictions on the intercept. Thus, it will be optimal to select a value tending towards minus infinity. Only $c''(\vec{\alpha})$ and $c'(\vec{\alpha})$ is computed in parallel with \code{OpenMP}. Building with a multithreaded BLAS can decrease the computation time of $\mat{X}_t^\top c''(\vec{\alpha}) \mat{X}_t$ along with the other matrix and vector products.

The global mode approximation is somewhat similar to~\cite[Section 10.6]{durbin12}. The major difference is that \cite{durbin12} makes the Taylor expansion \emph{before} running the filter using the current estimate of $\vec{\alpha}_0,\vec{\alpha}_1\dots,\vec{\alpha}_d$. In contrast, the GMA method makes the expansion at each correction step \emph{within} the filter using the current estimate of $\emNotee{\vec{a}}{t}{t-1}$ and potentially make further iterations in each correction step. The package \pkg{KFAS} implements the method in~\cite[Section 10.6]{durbin12}. They further ease the computation in \pkg{KFAS} by using the sequential method for the correction step for the Kalman filter described in~\cite{Koopman00}. % Lastly, if the negative value under the minimization operator in equation~\ref{eqn:modeExact} is not log-concave then the global instead of sequential optimization can be done by the second method in~\cite{Durbin00} or by the methods in~\cite{So03}. Similar changes can be made to the GMA method presented here.

\subsubsection{Examples with SMA and GMA}
I will use the hard disk failures data set to compare the SMA and GMA methods with the EKF with a single iteration in the correction step. I illustrate this below by estimating the model with SMA method, GMA method and EKF method. I use the correction step with the Cholesky decomposition in algorithm~\ref{alg:approxModeChol} by setting the \code{posterior_version = "cholesky"} in the list passed to the \code{control} argument.

<<echo=FALSE, cache = FALSE>>=
set.seed(914587)
@

% CHECK: arguments match

<<fit_SMA_hd_fail>>=
ddfit_SMA <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  by = 1,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(1, 23),
  Q = diag(0.01, 23),
  control = list(
    eps = 0.001,
    method = "SMA",                  # Use SMA
    posterior_version = "cholesky")) # The Cholesky method in algorithm \ref&alg:approxModeChol;
@

% CHECK: arguments match

<<fit_GMA_hd_fail>>=
ddfit_GMA <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  by = 1,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(1, 23),
  Q = diag(0.01, 23),
  control = list(
    eps = 0.001,
    method = "GMA"))                 # Use GMA instead
@

Figure~\ref{fig:EKF_vs_SMA_n_GMA} shows the three sets of predicted coefficients where the black lines are the coefficients from the EKF, the gray lines are with the SMA and the red line are with the GMA. The coefficients in figure~\ref{fig:EKF_vs_SMA_n_GMA} seems similar.

<<EKF_vs_SMA_n_GMA, par_3x3 = TRUE, fig.cap= "Predicted coefficients using the EKF, GMA and SMA for the hard disk failure data set. The gray lines are the coefficients from the SMA, red lines are coefficients from the GMA and the black lines are the coefficients form the EKF.", echo = FALSE, cache = FALSE>>=
for(i in 1:9){
  plot(ddfit, cov_index = i)
  plot(ddfit_SMA, cov_index = i, col = "gray40", add = T)
  plot(ddfit_GMA, cov_index = i, col = "red", add = T)
}
@

The SMA can have large entries in the diagonal of $\mat{Q}_0$ similar to the EKF with one iteration. I make the following fit to show this:

<<SMA_seed_large_Q0, echo=FALSE, cache = FALSE>>=
set.seed(914587)
@

<<SMA_w_large_Q_0>>=
new_call <- ddfit_SMA$call
new_call$Q_0 <- diag(100000000, 23)
ddfit_SMA_large_Q_0 <- eval(new_call)
@

<<SMA_w_large_Q_0_plot, par_3x3=TRUE, fig.cap = "Similar plot to figure~\\ref{fig:show_EKF_can_have_large} but with the SMA instead. The red curves are the estimates with the SMA with large entries in the diagonal of $\\mat{Q}_0$. The black curves are the same EKF fit as in figure~\\ref{fig:show_EKF_can_have_large}.", echo = FALSE>>=
ddfit_SMA_large_Q_0 <- get_pretty_model_factors(ddfit_SMA_large_Q_0)
for(i in 1:9){
  # Shown in figure \ref&fig:SMA_w_large_Q_0_plot;
  plot(ddfit_SMA_large_Q_0, cov_index = i, col = "red")
  plot(ddfit, cov_index = i, add = TRUE)
}
@

The first set of predicted coefficients are shown in figure~\ref{fig:SMA_w_large_Q_0_plot}. The difference here is bigger than for the EKF as seen in figure~\ref{fig:show_EKF_can_have_large}.

<<SMA_n_GMA_cleanup, echo = FALSE>>=
rm(ddfit_SMA_large_Q_0, ddfit_SMA, ddfit_GMA)
@


\subsection{Summary on filters}
All the filters have computational complexity $\bigO{n_t}$. Thus, the final EM algorithm in algorithm~\ref{alg:EM} is $\bigO{n_t}$ as the computation time of the specified parts of~\ref{alg:EM} is independent of $n_t$. I summaries the pros and cons of the EKF, UKF, SMA and GMA in table~\ref{tab:proConsEKF},~\ref{tab:proConsUKF},~\ref{tab:proConsSMA} and~\ref{tab:proConsGMA} respectively. Some of the points are under the assumption that the size of the risk set, $n_t$, is much greater than the dimension of the state vector, $q$.

\WiTbl{
\textbf{Pros of EKF} & \\
\hline
\embParallel{} \\
\hline
 \notQsens{} \\
\textbf{Cons of EKF} & \\
\hline
Linearisation & The linearisation may be a poor approximation.
}{The pros and cons for the EKF with \emph{one iteration} in the correction step.\label{tab:proConsEKF}}

\WiTbl{
\textbf{Pros of UKF} & \\
\hline
Approximation & Potentially better approximation than the EKF. \\
\hline
Parallel & The matrix and vector products can be computed in parallel with a multithreaded \pkg{BLAS}. Moreover, parts of the computations could be computed in parallel although this is not done with the current implementation.  \\
\textbf{Cons of UKF} & \\
\hline
\Qsens{} \\
\hline
Hyperparameters & Additional hyperparameters $(\alpha,\beta,\kappa)$ have to be specified. My experience is that $(\alpha,\beta) = (1,0)$ tend to do well with $\kappa > 0$ to ensure a positive weight on the zeroth sigma point.
}{The pros and cons for the UKF.\label{tab:proConsUKF}}


\WiTbl{
\textbf{Pros of SMA} & \\
\hline
  \prosLikelihood{} \\
\hline
  \notQsens{} \\
\textbf{Cons of SMA} & \\
\hline
  Sequential & The updates are sequential and thus does not benefit from a parallel implementation. \\ \hline
  Ordering & The final outcome will depend on the order of the risk sets.
}{The pros and cons for the SMA.\label{tab:proConsSMA}}

\WiTbl{
\textbf{Pros of GMA} & \\
\hline
  \prosLikelihood{} \\
\hline
\embParallel{} Though, only partly implemented at this point. The matrix and vector products can be computed in parallel with a multithreaded \pkg{BLAS}. \\
\textbf{Cons of GMA} & \\
\hline \Qsens{}
}{The pros and cons for the GMA. The EKF with more iteration in the correction step is similar as shown in the ddhazard vignette. Thus, some of the points also applies to algorithm~\ref{alg:EKFextra}.\label{tab:proConsGMA}}

\subsection{Constant effects}
We may assume that some of the coefficients are constant (time-invariant). Two methods are implemented to estimate such coefficients: one which makes the estimation in the E-step and one that estimates the coefficients by a first order Taylor expansion in the M-step. The computation in the E-step is achieved by augmenting the state vector with the fixed coefficients. Further, the entries of the rows and columns of $\mat{Q}$ for the fixed coefficients are set to zero and the corresponding diagonal entries of $\mat{Q}_0$ are set to large values. It is a common way of estimating parameters in filtering (see e.g. \cite{Harvey79}) and is equivalent to Recursive Least Squares if all coefficients are fixed in a regular Kalman Filter. This approach is also used in~\cite{Fahrmeir92} with the EKF.

The other method is to estimate the fixed coefficients in the M-step. The fixed coefficients times the corresponding covariates acts as offsets in the filters. Moreover, the formulas for $\vec{a}_0$ and $\mat{Q}$ in the M-step are not affected since the only relevant terms for fixed effects in the M-step is the last line of the log-likelihood in equation~\ref{eqn:logLike}. However, the optimization is not easily solved exactly in the M-step for the fixed coefficients. To illustrate this, let $\vec{\gamma}$ denote the fixed coefficients. Then the log likelihood we need to maximize in the M-step is:

\begin{equation}\label{eqn:fixedWithExpec}
\argmaxu{\vec{\gamma}}\expecpCond{
  \sum_{t = 1}^d \sum_{(i,j) \in R_t} l_{ijt}({\vec{\alpha}_t}, \vec{\gamma})}{
\emNotee{\vec{a}}{0}{d}, \emNotee{\mat{V}}{0}{d}, \dots,
\emNotee{\vec{a}}{d}{d}, \emNotee{\mat{V}}{d}{d}}
\end{equation}

where I temporarily add an additional argument in the log likelihood terms, $l_{ijt}$, for the fixed effects. Equation~\ref{eqn:fixedWithExpec} is not easy to optimize. It is further complicated by the fact that $\emNotee{\vec{a}}{0}{d}, \emNotee{\mat{V}}{0}{d}, \dots,
\emNotee{\vec{a}}{d}{d}, \emNotee{\mat{V}}{d}{d}$ are modes and not means when we use the EKF, SMA or GMA. I make a first order Taylor expansion around the $\emNotee{\vec{a}}{1}{d}, \dots, \emNotee{\vec{a}}{d}{d}$ in the current implementation to get:

\begin{equation}
\begin{aligned}
  \argmaxu{\vec{\gamma}}\expecpCond{
  \sum_{t = 1}^d \sum_{(i,j) \in R_t} l_{ijt}({\vec{\alpha}_t}, \vec{\gamma})}{
\emNotee{\vec{a}}{0}{d}, \emNotee{\mat{V}}{0}{d}, \dots,
\emNotee{\vec{a}}{d}{d}, \emNotee{\mat{V}}{d}{d}} \hspace{-120pt}& \\
  & \approx \argmaxu{\vec{\gamma}} \sum_{t = 1}^d \sum_{(i,j) \in R_t} l_{ijt}(\emNotee{\vec{a}}{t}{d}, \vec{\gamma})
\end{aligned}
\end{equation}

One of the advantages of doing this is that the problem can be solved with regular methods for GLMs when the model is from the exponential family. However, the design matrix will be big as each individual will yield multiple rows due to different offsets. Each time interval will have a different dot product between the time-varying coefficients, $\emNotee{\vec{a}}{t}{d}$, and the corresponding covariates. To overcome this problem the, I use the same \proglang{Fortran} code  from~\citep{Miller92} to do a series of rank-one updates of the QR-decomposition to solve the iteratively re-weighted least squares. This is the same approach as in the package \pkg{biglm}~\citep{biglm}. The computational complexity of each update is $\bigO{c^2}$ where $c$ is the dimension of $\vec{\gamma}$.

\subsection{Second order random walk}
I will end this part of the paper by illustrating that the difference between the two ways of estimating the fixed coefficients. Further, I will illustrate the use of the second order random walk. I estimate the model below where the factor levels for the hard disk model follow a second order random walk and the spline for the SMART 12 attribute is fixed. I get the second order random walk for the factor levels by setting the argument \code{order = 2}. I specify which terms are fixed by wrapping the terms in the formula with the \code{ddFixed} function. The fixed effect estimation method is selected to the M-step method by setting \code{fixed_terms_method = "M_step"} in the list passed to the \code{control} argument. I avoid divergence by decreasing the learning rate by passing a \code{LR} in the list passed to the control argument.

% CHECK: Arguments for spline match. Make sure the argument in the two calls match

<<order_2_est>>=
# Define new formula
frm_fixed <-
  Surv(tstart, tstop, fails) ~ -1 + model +
  ddFixed(ns(smart_12, knots = c(3, (1:2) * 10), Boundary.knots = c(0, 30)))

# Estimation in M-step
ddfit_fixed_M <- ddhazard(
  formula = frm_fixed,
  data = hd_dat_sub,
  by = 1,
  order = 2,                        # Use 2. order model
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(10, 32),               # Needs more elements
  Q = diag(0.01, 16),
  control = list(
    method = "GMA",
    eps = 0.005,
    LR = .2,                        # Decrease learning rate
    fixed_terms_method = "M_step")) # Use M-step method

# Estimation in E-step
ddfit_fixed_E <- ddhazard(
  formula = frm_fixed,
  data = hd_dat_sub,
  by = 1,
  order = 2,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(10, 32),
  Q = diag(0.01, 16),
  control = list(
    method = "GMA",
    NR_eps = .001,
    eps = 0.005,
    LR = .2,
    fixed_terms_method = "E_step")) # Use E-step method
@


<<order_2_plot, echo = FALSE, par_3x3 = TRUE, fig.cap="Predicted coefficients for some of the factor levels with the second order random walk. The gray lines are coefficients with fixed effects estimated in the E-step, the red lines are with the coefficients estimated in the M-step. The black lines is the first order random walk where all coefficients are time-varying.",cache=FALSE>>=
ddfit_fixed_E <- get_pretty_model_factors(ddfit_fixed_E)
ddfit_fixed_M <- get_pretty_model_factors(ddfit_fixed_M)

for(i in 1:9){
  # Shown in figure \ref&fig:order_2_plot;
  plot(ddfit_fixed_E, cov_index = i, lwd = 2, col = "darkgray")
  plot(ddfit_fixed_M, cov_index = i, col = "red", add = TRUE)
  plot(ddfit, cov_index = i, add = TRUE)
}
@

<<order_2_plot_fixed, echo = FALSE, par_1x1 = TRUE, fig.cap="Fixed effects estimates for the SMART 12 attribute on the linear predictor scale. The red curve is the spline estimated in the E-step and the black curve is the spline estimated in the M-step.",cache=FALSE>>=
expr <- tail(colnames(attr(terms(ddfit_fixed_E$formula), "factors")), 1)
x <- with(
  list(smart_12 = (x_org <- seq(0, 70, length.out = 1000))),
  eval(parse(text = expr)))

spline_E <- drop(x %*% ddfit_fixed_E$fixed_effects)
spline_M <- drop(x %*% ddfit_fixed_M$fixed_effects)

plot(x_org, spline_E, ylim = range(spline_E, spline_M),
     type = "l", xlab = "SMART 12 attribute", ylab = "Linear predictor term")
lines(x_org, spline_M, col = "red")
@

Figure~\ref{fig:order_2_plot} shows the predicted factor levels for the hard disk model and figure~\ref{fig:order_2_plot_fixed} plots the spline estimate. The difference with the spline estimate may be due to the sparse amount of examples with high values of the SMART 12 attribute in certian periods. I plot the quantiles through time of the SMART 12 attribute to illustrate this in figure~\ref{fig:smart_12_through_time}. It is my experince that the difference between the two methods of estimating fixed effects is not as big as in this example in most cases. As an example, you can run the shiny app with the package by calling \code{ddhazard_app}. This provides a simulation example where, among other things, you can estimate with the two methods for fixed effects.

<<smart_12_through_time, echo = FALSE, par_1x1 = TRUE, fig.cap="Quantiles of the SMART 12 attribute through time. The quantiles are at 5\\%, 10\\%, \\ldots, 95\\%. This is the data set wihtout the ST3000DM001 model.",cache=FALSE>>=
# Find quantiles through time
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ smart_12,
  data = hd_dat_sub, by = 1, max_T = 60, use_weights = F,
  id = hd_dat_sub$serial_number)
quants <- tapply(
  tmp_dat$X$smart_12, tmp_dat$X$t, quantile, probs = (1:19)/20)
quants <- do.call(cbind, quants)

# Plot
x <- as.numeric(colnames(quants))
plot(range(x), range(quants), type = "n", xlab = "Month", ylab = "SMART 12 attribute")
col <- rgb(0, 0, 0, alpha = .1)
for(i in 1:floor(nrow(quants) / 2)){
  lb <- quants[i, ]
  ub <- quants[nrow(quants) - (i - 1), ]
  polygon(c(x, rev(x)), c(ub, rev(lb)), col = col, border = NA)
}

# Cleanup
rm(tmp_dat)
@


\section{Models}\label{sec:mod}

I will cover the two implemented models in the current implementation of \code{ddhazard} in this section. The first is the discrete model where we have binary outcomes in each interval for each individual. Then I cover the continuous time model where we model event times instead of binary outcomes.

\subsection{Dynamic discrete time model}\label{subsec:logi}
The dynamic discrete time model is where we use the log likelihood terms, $l_{ijt}(\vec{\alpha}_t)$, as shown in equation~\ref{eqn:binModelLikeli} where $h$ is the inverse logistic function, $h(x) = \exp (x) / (1 + \exp(x))$. This model is suited for situations where the events occur at discrete times and the covariates change at discrete times. An example is corporate default prediction where covariates are values from the financial statements which are reported on yearly or quarterly basis. However, depending on the true distribution we may lose information and potentially introduce biases when events or covariates are updated at continuous point. I make the following example to illustrate this issue.

Suppose we have two intervals in our model and time-varying covariates. Further, let both the event times and the point in which we observe new covariates happen at continuous points in time. Figure~\ref{fig:binning_fig} illustrates such a situation. Each horizontal line represent an individual. A cross represents when the covariate values change for the individual and a filled circle represents an event has happened for the individual. Lines that ends with an open circle are right censored. The vertical dashed lines in the figure represents the time interval borders. The first vertical line from the left is where we start our estimation, the second vertical line is where the first time interval ends and the second time intervals starts and the third vertical line is where the time interval ends.

<<binning_fig, echo=FALSE, results="hide", fig.cap = "Illustration of a data set with 7 individuals with time-varying covariates. Each horizontal line represents an individual. Each number indicates a start time and/or stop time in the initial data. A cross indicates that new covariates are observed while a filled circle indicates that the individual has an event. An open circle indicates that the individual is right censored. Vertical dashed lines are time interval borders.", fig.height=3.5, fig.width=6, par_1x1 = TRUE,cache=FALSE>>=
par(mar = c(1, 5, 1, 2), cex = par()$cex * 1.33, xpd=TRUE)
plot(c(0, 4), c(0, 1), type="n", xlab="", ylab="", axes = F)

abline(v = c(0.5, 1.5, 2.5), lty = 2)

text(1, 0.01, "1st interval", adj = .5)
text(2, 0.01, "2nd interval", adj = .5)

n_series = 7
y_pos = seq(0, 1, length.out = n_series + 2)[-c(1, n_series +2)]

set.seed(1992)
x_vals_and_point_codes = list(
  cbind(c(0, .8, 2.2, 3, 3.7) + c(.1, rep(0, 4)),
        c(rep(4, 4), 1)),
  cbind(c(0.1, 1, 1.5 + runif(1)),
        c(4, 4, 16)),
  cbind(c(0.1, .8, 1.9, 2 + runif(1, min = 0, max = .25)),
        c(4, 4, 4, 16)),
  cbind(c(0.1, runif(1) + .33), c(4, 16)),
  cbind(c(0.1, .6, 2.1, 3.1 + runif(1)),
        c(4, 4, 4, 16)),
  cbind(c(2, 2.33),
        c(4, 16)),
  cbind(c(0.1, 1.3),
        c(4, 1)))

x_vals_and_point_codes = x_vals_and_point_codes[
  c(n_series, sample(n_series - 1, n_series - 1, replace = F))]

for(i in seq_along(x_vals_and_point_codes)){
  vals = x_vals_and_point_codes[[i]]
  y = y_pos[i]

  xs = vals[, 1]
  n_xs = length(xs)

  # add lines
  segments(xs[-n_xs], rep(y, n_xs - 1),
           xs[-1], rep(y, n_xs - 1))

  # Add point
  points(xs, rep(y, n_xs), pch = vals[, 2],
         cex = ifelse(vals[, 2] == 1, par()$cex * 2.5, par()$cex))

  # Add numbers
  text(xs, rep(y + .05, n_xs), as.character(0:(n_xs -1)))
}

# add letters
text(rep(0, n_series), rev(y_pos), letters[1:n_series], cex = par()$cex * 1.5)
@

% CHECK: Text here match with text in the start

The \code{ddhazard} function uses the discrete risk set given equation~\ref{eqn:discreteRiskSet}. One consequence is that we use covariates from 0 for individuals a, c, d and f  for the entire period of the first time interval even though the covariates change at 1. Furthermore, g is not included at all since we only know that he survives parts of the first time interval. Lastly, we never include b as we do not know his covariate vector at the start of the second interval.

\subsection{Continuous time model}\label{subsec:contTime}
The continuous  time model implemented in \code{ddhazard} assumes that:

\begin{itemize}
\item Coefficients change at the end of time intervals. I.e. $\vec{\alpha}(t) = \vec{\alpha}_{\lceil t\rceil}$ when interval lengths are 1, \code{by = 1}, where $\lceil t\rceil$ gives the ceiling of $t$. This is the same as for the dynamic discrete time model as illustrated in figure~\ref{fig:binning_fig} where the coefficients change at the vertical lines.
\item The individuals covariates change at discrete times. I.e. $\vec{x}_{i}(t) = \vec{x}_{ij}$ where $j = \{ k:\  t_{i,k-1} < t \leq t_{i,k}  \}$. This implies that the covariates jump at the crosses in figure~\ref{fig:binning_fig}.
\item We have instantaneous hazards given by $\exp(\vec{x}_{i}(t)^\top\vec{\alpha}(t))$.
\end{itemize}

The instantaneous hazard change when either the individual's covariates change or the coefficients change. Thus, an individual's event time is piecewise constant exponential distributed given the state vectors. The log-likelihood of individual $i$ having an event at time $t_i$ is:

\begin{equation}\label{eqn:condPropEvent}
\log\Lparen{\likepCond{t_i}{\vec{\alpha}_0,\dots,\vec{\alpha}_d}} =
  \vec{x}_i(t_i)^\top\vec{\alpha}(t_i)
  -\int_0^{t_i}\exp\Lparen{\vec{x}_i(u)^\top\vec{\alpha}(u)}\, du
\end{equation}

where $\likep{\cdot}$ denotes the likelihood. Due to our assumptions, the complete data log likelihood in equation~\ref{eqn:logLike} for simplifies to:

\begin{equation}
\begin{aligned}
\mathcal{L}\Lparen{\vec{\alpha}_0, \dots, \vec{\alpha}_{d}} =&
  - \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^\top \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^\top \mat{Q}^{-1} \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\
	&  - \frac{1}{2} \log\deter{\mat{Q}_0} - \log\frac{1}{2d} \deter{\mat{Q}} \\
  &+ \sum_{t=1}^d\sum_{(i,j) \in \mathcal{R}_t} l_{ijt}(\vec{\alpha}_t)
\end{aligned}
\end{equation}

where:

\begin{equation}
\begin{aligned}
&y_{ijt} = 1_{\left\{T_i \in (t_{i,j-1}, t_{ij}]\, \wedge\, t - 1 < t_{ij} \leq t \right\}} \\
%
&l_{ijt}(\vec{\alpha}_t) = y_{ijt}\vec{x}_{ij}^\top\vec{\alpha}_t
  - \exp\Lparen{\vec{x}_{ij}^\top\vec{\alpha}_t}
  \Lparen{\min\{ t, t_{ij} \} - \max\{ t - 1, t_{i,j-1} \}}
\end{aligned}
\end{equation}

The $l_{ijt}$ terms are a simplification of equation~\ref{eqn:condPropEvent} where I use that both the covariates $\vec{x}_i(t)$ and coefficients $\vec{\alpha}(t)$ are piecewise constant. $y_{ijt}$ is a generalization of equation~\ref{eqn:binFirst}. It is and indicator for whether individual $i$ experiences an event with \emph{the $j$’th covariate vector} in interval $t$. Further, $\mathcal{R}_t$ is the \emph{continuous risk set} given by:

\begin{equation}
\mathcal{R}_t = \Lbrace{(i,j) \in \mathbb{Z}^2_+:\, t_{i,j-1} < t \wedge t_{ij} > t - 1}
\end{equation}

The two conditions in $\mathcal{R}_t$ is that the observation must start before the intervals ends ($t_{i,j-1} < t$) and end after the intervals start ($t_{ij} > t - 1$). The above is easily implemented with the SMA and GMA as we work directly with the likelihoods, $l_{ijt}$, in~\citeAlgLine{alg:approxMode:findConst}{alg:approxMode},~\citeAlgLine{alg:approxModeChol:findConst}{alg:approxModeChol} and~\citeAlgLine{alg:GlobalMA:Update}{alg:GlobalMA}. However, the EKF and UKF require that we have a variable with a link function in~\citeAlgLine{alg:EKF:scoreVec}{alg:EKF},~\citeAlgLine{alg:EKFextra:scoreVec}{alg:EKFextra} and~~\citeAlgLine{alg:UKF:expecMean}{alg:UKF}. I will use what I denote as the right clipped time variable with a jump term. First I show the definition and then I motivate the definition. I cover the definitions as other options are implemented but not included here for space reasons. By right clipping a random variable $b$ at $v$ I mean that the clipped variable $\tilde b$ is:
\begin{equation}
  \tilde b = \left\{ \begin{matrix} b & b \leq v \\ v & b > v \end{matrix}\right.
\end{equation}

Next, define $\delta_{ijt} = \min\{ t_{ij}, t\} - \max \{ t_{i,j-1}, t-1\}$ as the length in time that $x_{i}(s) = \vec{x}_{ij}$ in interval $t$. That is, the length in time of the intersection of $(t_{i,j-1}, t_{ij}]$ and $(t-1, t]$. Then we define right clipped time variable with a jump term for individual $i$ with the covariate vector $j$ in interval $t$ as:

\begin{equation}
\begin{aligned}
\Lambda_{ijt}
  &= \delta_{ijt}1_{\{T_i > \min\Lbrace{t_{ij},t}\}} + (T_i - \min\Lbrace{t_{ij},t}) 1_{\{T_i \leq \min\Lbrace{t_{ij},t}\}} \\
  &= \left\{\begin{matrix}
    T_i - \min\Lbrace{t_{ij},t} &  T_i \leq \min\Lbrace{t_{ij},t} \\
    \delta_{ijt} & T_i > \min\Lbrace{t_{ij},t}\end{matrix} \right.
\end{aligned}
\end{equation}

I assume in the following comments that the individual have not had an event up to $\max \{ t_{i,j-1}, t-1\}$. In this case, $\Lambda_{ijt}\in [-\delta_{ijt},0]\cup\{\delta_{ijt}\}$. $T_i > \min\Lbrace{t_{ij},t}$ implies that the individual did not have an event with the $j$'th covariate vector and/or did not have an event in the $t$'th interval. $\Lambda_{ijt} \leq 0$ if the individual had an event before time $t$. The greater the value of $\Lambda_{ijt}$ the closer the event is to $\min\Lbrace{t_{ij},t}$ as long as $\Lambda_{ijt} \leq 0$. The \emph{jump} comes from the change from $0$ to $\delta_{ijt}$ in case of no event. I use the term \emph{clip} because we clip the piecewise constant exponential distributed $T_i$ to the period we currently look at.

%The connection to the stop time, $T_i$, can be illustrated as follows. Suppose that the covariates vectors change at time $1,2,3,\dots$ such that $t_{i,j} - t_{i,j-1} = 1$ unless we have an event or the individual is right censored. Then:

%\begin{equation}
%\begin{aligned}
%k &= \lceil t \rceil - 1 & \\
%\likep{T_i = t} &= \likep{T_i > 1} \likepCond{T_i > 2}{T_i > 1} \cdots \likepCond{T_i = t}{T_i > k} \\
%%
%               &\hspace{-20pt}= \likep{\Lambda_{i11} = 1} \likepCond{\Lambda_{i22} = 1}{\Lambda_{i11} = 1} \cdots  \likepCond{
%	\Lambda_{i\lceil t \rceil\lceil t \rceil} = t - \lceil t \rceil}{\Lambda_{ikk} = 1} \\
%%
%\likep{T_i > t} &= \likep{T_i > 1} \likepCond{T_i > 2}{T_i > 1} \cdots \likepCond{T_i > t}{T_i > k} \\
%
%               &\hspace{-20pt}= \likep{\Lambda_{i11} = 1} \likepCond{\Lambda_{i22} = 1}{\Lambda_{i11} = 1} \cdots  \likepCond{
%	\Lambda_{i\lceil t \rceil\lceil t \rceil} > t - \lceil t \rceil}{\Lambda_{ikk} = 1} \\
%\end{aligned}
%\end{equation}

%where $\lceil \cdot \rceil$ is the ceiling function.

Next, I will motivate why I use the jump term with the following example. Suppose that we observe event times in a period from $0$ to $100$ and we set the interval lengths (the \code{by} argument) to $10$. Let the events time be reported as integers (e.g. says days). Say individual $i$ has a single covariate vector and an event at $T_i = 30$. We then get $(\Lambda_{i1,1} = 10, \Lambda_{i1,2} = 10, \Lambda_{i1,3} = 0)$. However, if I remove the jump term and instead set $\Lambda_{iji}$ to zero in case of no event then we get $(\tilde\Lambda_{i1,1} = 0, \tilde\Lambda_{i1,2} = 0, \tilde\Lambda_{i1,3} = 0)$. The tilde have been added to stress the different definition. There is no difference in the latter case between no events and an event occurring on the boundary or change of covariate vector. Thus, I use the jump term. This is only an issue when the reported time scale is not exactly continuous.

\subsubsection{Example with continuous model}
The start and stop times in the hard disk failure data set are in factions of months. The precision in the data set is on hourly basis. Thus, the continuous model is valid candidate for the data set. Below I fit the model using the right clipped time variable with a jump term The plot of the first estimated factor levels are shown in figure~\ref{fig:cont_plot}.

% CHECK: arguments match with previous

<<cont_fit>>=
ddfit_cont <- ddhazard(
  formula = frm,
  data = hd_dat_sub,
  model = "exp_clip_time_w_jump", # Change model from default
  by = 1,
  max_T = 60,
  id = hd_dat_sub$serial_number,
  Q_0 = diag(1, 23),
  Q = diag(0.01, 23),
  control = list(
    NR_eps = 0.001,  # EKF with extra iterations
    eps = 0.001,
    LR = .8,
    method = "EKF")) # Use EKF
@

<<cont_plot, echo=FALSE, par_3x3 = TRUE, fig.cap="Predicted factor levels coefficients with the model using right clipped time variable with a jump term.",cache=FALSE>>=
ddfit_cont <- get_pretty_model_factors(ddfit_cont)
plot(ddfit_cont)
@

<<cont_cleanup, echo = FALSE>>=
rm(ddfit_cont)
@

The results are comparable to what we have seen previously. E.g. see figure~\ref{fig:subset_EKF_xtra_vs_wo} where I also used the EKF with extra iterations in the correction step but with the discrete model.

\section{Simulations}\label{sec:sims}
<<echo = FALSE,cache = FALSE>>=
# Although caching is used, we also make an additional copy of the results
# as this part takes a while and to avoid re-computations in case of some
# minor change in the code here or similar
 library(tcltk)

tmp_path <- stringr::str_extract(getwd(), ".+/dynamichazard")
tmp_path <- paste0(tmp_path, "/vignettes/.jss/")

result_file <- paste0(tmp_path, "results_from_simulation.Rds")
sim_env_file <- paste0(tmp_path, "sim_env.Rds")
already_computed <-
  file.exists(result_file) &&
  file.exists(sim_env_file) &&
  (!tclvalue(tkmessageBox(
    title = "Rerun", message = "Want to rerun the simulation?",
    type = "yesno")) == "yes")

if(!already_computed){
  with(sim_env <- new.env(), {
    #####
    # Function to make sampling go quicker
    get_exp_draw <- with(environment(ddhazard), get_exp_draw())
    get_unif_draw <- with(environment(ddhazard), get_unif_draw())
    get_norm_draw <- with(environment(ddhazard), get_norm_draw())

    #####
    # Define simulation function
    sim_func <- function(
      n_series, n_vars = 10L, t_0 = 0L, t_max = 30L, cov_params = 1,
      re_draw = T, beta_start = rnorm(n_vars), intercept_start,
      sds = rep(1, n_vars + 1), run_n = 1){
      # Make output matrix
      n_row_max <- n_row_inc <- 10^5
      res <- matrix(NA_real_, nrow = n_row_inc, ncol = 4 + n_vars,
                    dimnames = list(NULL, c("id", "tstart", "tstop", "event", paste0("x", 1:n_vars))))
      cur_row <- 1

      if(re_draw){
        get_unif_draw(re_draw = T)
        get_exp_draw(re_draw = T)
        get_norm_draw(re_draw = T)
      }

      if(length(beta_start) == 1)
        beta_start <- rep(beta_start, n_vars)

      # draw betas
      betas <- matrix(get_norm_draw((t_max - t_0 + 1) * (n_vars + 1)),
                      ncol = n_vars + 1, nrow = t_max - t_0 + 1)
      betas <- t(t(betas) * sds)
      betas[1, ] <- c(intercept_start, beta_start)
      betas <- apply(betas, 2, cumsum)

      # covariate sim expression
      cov_exp <- expression(cov_params * get_norm_draw(n_vars))

      # Simulate
      for(id in 1:n_series){
        interval_start <- tstart <- tstop <-
          max(floor(get_unif_draw(1) *  2 * t_max) - t_max, 0L)
        repeat{
          tstop <- tstop + 5L
          if(tstop >= t_max)
            tstop <- t_max

          x_vars <- eval(cov_exp)
          l_x_vars <- c(1, x_vars)

          tmp_t <- tstart
          while(tmp_t <= interval_start &&  interval_start < tstop){
            exp_eta <- exp(.Internal(drop(betas[interval_start + 2, ] %*% l_x_vars)))
            event <- exp_eta / (1 + exp_eta) > get_unif_draw(1)

            interval_start <- interval_start + 1L
            if(event){
              tstop <- interval_start
              break
            }

            tmp_t <- tmp_t + 1L
          }

          res[cur_row, ] <- c(id, tstart, tstop, event, x_vars)

          if(cur_row == n_row_max){
            n_row_max <- n_row_max + n_row_inc
            res = rbind(res, matrix(NA_real_, nrow = n_row_inc, ncol = 4 + n_vars))
          }
          cur_row <- cur_row + 1

          if(event || tstop >= t_max)
            break

          tstart <- tstop
        }
      }

      list(res = as.data.frame(res[1:(cur_row - 1), ]), betas = betas)
    }

    sim_func <- compiler::cmpfun(sim_func)

    #####
    # Define parameters
    n_series <- 2^(9:20)
    n_vars <- c(20, 20)
    t_max <- 30L
    intercept_start <- -3.5
    beta_sd <- c(.33, .33)
    intercept_sd <- 0.1
    Q_0_arg <- 1e5
    Q_arg <- 0.01
    denom_term <- 0.00001
    LR <- 1
    n_max <- 9
    eps <- 0.01
    cov_params <- list(c("sigma" = 1), c("sigma" = .33))

    NR_eps = 0.1
    SMA_meth = "woodbury"
    Q_0_small <- 1

    n_sims <- 11
    set.seed(4368560)
    seeds <- sample.int(n_sims)

    ukf_alpha <- 1
    ukf_w0 <- 0.0001
    ukf_beta <- 0
    ukf_max <- 2^18
    ukf_kappa <- (2 * n_vars + 1) * (1 + ukf_alpha^2 * (ukf_w0 - 1)) / (ukf_alpha^2 * (1 - ukf_w0))
    ukf_Q_0 <- .01

    # Sanity check
    m <- 2 * n_vars + 1
    lambda <- ukf_alpha^2 * (m + ukf_kappa) - m
    stopifnot(all.equal(lambda / (m + lambda), rep(ukf_w0, 2)))
    rm(m, lambda)

    n_threads <- max(parallel::detectCores() - 1, 2)
    options(ddhazard_max_threads = n_threads)

    results <- array(
      NA_real_, dim = c(2, length(seeds), length(n_series), 5, 3),
      dimnames = list(
        NULL, NULL, NULL,
        c("EKF", "EKFx", "UKF", "SMA", "GMA"),
        c("elapsed", "MSE", "niter")))

    #####
    # Function to get estimates
    get_fit <- eval(bquote(
      function(data, method, run_n = 1){
        gc() # Make sure garbage collection is run before

        try({
          time <- system.time(fit <- ddhazard(
            Surv(tstart, tstop, event) ~ . - tstart - tstop - event - id,
            data = data, by = 1L, max_T = .(t_max), id = data$id,
            Q_0 = diag(
              ifelse(method %in% c("GMA", "EKFx"),
                     .(Q_0_small),
                     ifelse(method == "UKF" ,
                            .(ukf_Q_0), .(Q_0_arg))),
              .(n_vars)[run_n] + 1),
            Q = diag(.(Q_arg), .(n_vars)[run_n] + 1),
            control = list(
              LR = if(method == "EKF") .(LR) else 1,
              method = stringr::str_replace(method, "x", ""),
              alpha = .(ukf_alpha), beta = .(ukf_beta),
              kappa = .(ukf_kappa)[run_n], denom_term = .(denom_term),
              save_risk_set = F, save_data = F,
              LR_max_try = 1, # we only try one learning rate
              n_max = .(n_max),
              eps = .(eps),
              posterior_version = .(SMA_meth),
              NR_eps =  if(method == "EKFx") NR_eps else NULL
            )))["elapsed"]

          return(list(fit = fit, time = time))
        })

        return(NULL)
      }))

    #####
    # Function to get MSE
    mse_func <- function(betas, fit)
      mean((betas[-1, ] - fit$state_vecs[-1, ])^2) # remove the first entry which
                                                   # cannot is just a same as period
                                                   # one estimate

    ######
    # Run experiment
    for(run_n in 1:2){
      for(i in seq_along(seeds)){
        s <- seeds[i]
        for(j in seq_along(n_series)){
          print(paste0("Using seed ", i, " with number of series index ", j,
                       " in run ", run_n))
          n <- n_series[j]
          set.seed(s)

          # Simulate
          sims <- sim_func(
            n_series = n, n_vars = n_vars[run_n], t_max = t_max,
            intercept_start = intercept_start, cov_params = cov_params[[run_n]],
            sds = c(intercept_sd, rep(beta_sd[run_n], n_vars[run_n])),
            run_n = run_n)

          # EKF
          out <- get_fit(data = sims$res, "EKF", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "EKF", "elapsed"] <- out$time
            results[run_n, i, j, "EKF", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "EKF", "niter"] <- out$fit$n_iter
          }

          # EKFx
          out <- get_fit(data = sims$res, "EKFx", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "EKFx", "elapsed"] <- out$time
            results[run_n, i, j, "EKFx", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "EKFx", "niter"] <- out$fit$n_iter
          }

          # UKF
          if(n <= ukf_max){
            out <- get_fit(data = sims$res, "UKF", run_n)
            if(!is.null(out)){
              results[run_n, i, j, "UKF", "elapsed"] <- out$time
              results[run_n, i, j, "UKF", "MSE"] <- mse_func(sims$betas, out$fit)
              results[run_n, i, j, "UKF", "niter"] <- out$fit$n_iter
            }
          }

          # SMA
          out <- get_fit(data = sims$res, "SMA", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "SMA", "elapsed"] <- out$time
            results[run_n, i, j, "SMA", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "SMA", "niter"] <- out$fit$n_iter
          }

          # GMA
          out <- get_fit(data = sims$res, "GMA", run_n)
          if(!is.null(out)){
            results[run_n, i, j, "GMA", "elapsed"] <- out$time
            results[run_n, i, j, "GMA", "MSE"] <- mse_func(sims$betas, out$fit)
            results[run_n, i, j, "GMA", "niter"] <- out$fit$n_iter
          }

          print(results[run_n, i, j,,])
        }

        print(results[run_n, i,,,])
        rm(out, sims)
      }
    }
  })

  # Take copy of results and clean up enviroment
  results <- sim_env$results
  rm(results, i, j, s, get_fit, mse_func, envir = sim_env)

  # Save for later
  saveRDS(sim_env, file = sim_env_file)
  saveRDS(results, file = result_file)
} else {
  sim_env <- readRDS(sim_env_file)
  results <- readRDS(result_file)
}
@

In this section, I will simulate data using the first order random walk model and illustrate the computation time and mean square error of the predicted coefficients as the number of individuals increases. The simulation is done as follows. I use a first order random walk for the coefficients with \Sexpr{sim_env$n_vars[1] + 1} coefficients. The intercept starts at \Sexpr{sim_env$intercept_start} and the other coefficients start at points drawn from the standard normal distribution. I set the intercept to a low value to decrease the baseline likelihood of an event in every interval. I let covariance matrix $\mat{Q}$ be a diagonal matrix which first diagonal entry is equal to $\Sexpr{sim_env$intercept_sd}^2$ (the intercept) and $\Sexpr{sim_env$beta_sd[1]}^2$ for rest of the diagonal entries. The standard deviation is chosen lower for the intercept ensure that the intercept does not change too much with high probability. An example of a draw of coefficients is given in figure~\ref{fig:sim_coefficients_ex}.

<<sim_coefficients_ex, echo=FALSE, results="hide", fig.cap = "Example of coefficients in the simulation experiment. The black curve is the intercept and the gray curves are the coefficients for the covariates.", par_1x1 = TRUE, cache = FALSE>>=
with(sim_env, {
  n <- 100
  sims <- sim_func(
          n_series = n, n_vars = n_vars[1], t_max = t_max,
          intercept_start = intercept_start,
          sds = c(intercept_sd, rep(beta_sd[1], n_vars[1])))

  matplot(sims$betas, type = "l", lty = 1,
          col = c("black", rep("gray40", n_vars[1])),
          xlab = "Time", ylab = "Coefficient")
})
@


I then simulate a different number of individuals with $n = 2^{\Sexpr{min(log(sim_env$n_series, 2))}}, 2^{\Sexpr{min(log(sim_env$n_series, 2)) + 1L}}, \dots, 2^{\Sexpr{max(log(sim_env$n_series, 2))}}$ in each trail. Each individual is right censored at time $\Sexpr{sim_env$t_max}$ and I set the interval lengths to $1$. Further, I randomly start to observe each individual at time ${0,1,\dots,\Sexpr{sim_env$t_max - 1L}}$ with a 50\% chance of $0$ and uniform chance on the other points. Each individual has time-varying covariates which a change after five periods. Thus, if an individual starts at time $2$ then his covariate vector changes at time $7, 12, \dots, 27$. The covariates are drawn from an iid standard normal distribution. For each value of $n$ I make $\Sexpr{sim_env$n_sims}$ replications. I only estimate the UKF model up to $n = 2^{\Sexpr{log(sim_env$ukf_max, 2)}}$ due to the computation time. Further, I set the UKF hyperparameters to $(\alpha,\beta,\kappa) = (\Sexpr{sim_env$ukf_alpha},\Sexpr{sim_env$ukf_beta},\Sexpr{sim_env$ukf_kappa[1]})$ which yields $W_0^{[m]} = \Sexpr{sim_env$ukf_w0}$. $\mat{Q}_0$ for the EKF with extra iterations and GMA is a diagonal matrix with entries $\Sexpr{sim_env$Q_0_small}$. The UKF has $\Sexpr{sim_env$ukf_Q_0}$ as the diagonal entries. The EKF without extra iterations and the SMA have $\Sexpr{sim_env$Q_0_arg}$ in the diagonal entries of $\mat{Q}_0$. All the filters set the starting value $\mat{Q}$ as a diagonal matrix with $\Sexpr{sim_env$Q_arg}$ in the diagonal elements. All the methods takes at most $\Sexpr{sim_env$n_max + 1}$ iterations of the EM-algorithm if the convergence criteria is not meet before.

% CHECK: comment below

<<results='asis',echo=FALSE,cache=FALSE>>=
# # Check that all runs did succeed
# is_complete <- t(apply(results, c(1, 4), complete.cases)[,, 1])
# stopifnot(all(is_complete))

# Compute means and medians
medians <- apply(results, c(1, 3:5), median, na.rm = T)
means <- apply(results, c(1, 3:5), mean, na.rm = T)

tbl_sum <- matrix(
  NA_real_, nrow = 2, ncol = dim(results)[4],
  dimnames = list(
    c("Run time", "Log-log slope"),
    dimnames(results)[[4]]))

tbl_sum["Run time", ] <-
  apply(medians[1, , , "elapsed"], 2, max, na.rm = T)

log_reg_cut_off <- 2^14

for(n in dimnames(tbl_sum)[[2]]){
  tmp <- results[1, , , n, "elapsed"]
  tbl_sum["Log-log slope", n] <-
    lm(log(c(t(tmp[, log_reg_cut_off<= sim_env$n_series]))) ~
       log(rep(sim_env$n_series[sim_env$n_series >= log_reg_cut_off],
               sim_env$n_sims)))$coefficient[2]
}

colnames(tbl_sum)[colnames(tbl_sum) == "EKFx"] <- "EKF with extra iterations"

xtable(tbl_sum, digits = getOption("digits"),
       caption = paste0("Summary information of the computation time in the simulation study. The first row prints the median run time for largest number of individuals. The UKF is only up to $n=",
                        sim_env$ukf_max, "$. The second row shows the log-log slope of the computation time regressed on number of individuals for $n\\geq ",
                        log_reg_cut_off, "$."),
       label = "tab:runSummaryStats")
@

The simulations are run on a laptop running Windows 10 with a Intel\textregistered~core\texttrademark~i7-6700hq cpu @ 2.60ghz processor and with 16 GB ram. The 64 bit Rtools 34 is used to build \proglang{R} and the \pkg{dynamichazard} package. The medians and means of the computation time are shown in figure~\ref{fig:sim_comp_time}. Print of the median computation time for the largest value of $n$ is printed in table~\ref{tab:runSummaryStats} along with a the log-log regression slope of the computation time regressed on the number of individuals. All methods have a slop at or below 1 implying the $\bigO{n_t}$ computation time. The slope for the EKF may be lower due to the overhead of the parallel computation. All computation times are including the time of setting up the data frame and running a weighted GLM to get starting values for $\vec{\alpha}_0$. This takes over $1/2$ of the computation time reported with the EKF. The setup time is equal for all methods.

The SMA and GMA are comparable in computation time. This may be explained by a Floating-Point Operations (FLOP) count of the matrix and vector product effected by the number of observation. I use the method derived by the Woodbury matrix identity in algorithm~\ref{alg:approxMode} for the SMA in the simulation study. The FLOP count for this algorithm is $(6q + 4q^2)n_t$ at each correction step. In contrast, the GMA method shown in algorithm~\ref{alg:GlobalMA} has a count of $(6q + 2q^2)n_t$ for a single iteration in the correction step. Thus, the computation time of the GMA and SMA should be roughly equal for large $q$ if we only make two iterations with the GMA in the correction step. A second explanation may be the additional search for the step length in~\citeAlgLine{alg:approxMode:findConst}{alg:approxMode}. A third explanation may be that $\mat{X}_t$ is never formed in continuous memory with SMA in the current implementation. Instead, random access is used to the matrix with all the covariate vectors. This is advantages as it reduces the memory requirements but may come at a cost in terms of computation time. If the latter is important then similar conclusion applies for the EKF method. The two EKF methods are almost equal in computation time due to additional iterations of the EM-algorithm when only one iteration step is taken in the correction step. This is can be seen from figure~\ref{fig:n_iter_plot} which shows the median number of iterations of the EM-algorithm.

A plot of the predicted mean square error is shown in figure~\ref{fig:sim_MSE}. The EKF with one iteration in the correction step does not improve much as $n$ increases. Hence, more iterations seems preferable in this example. It is interesting that the UKF flatlines after a given number of observations.

<<sim_labels_setup_n_more, echo = FALSE>>=
pchs <- c(15, 4, 16, 17, 5)
sim_fig_cap <-
  "The EKF, EKF with extra iteration, UKF, SMA and GMA are respectivly the filled squares, crosses, circles, triangles and open square."

col_medians <- "black"
col_means <- rgb(0, 0, 0, alpha = .5)
# CHECK: cap match with symbols
@


<<sim_comp_time, echo=FALSE, results="hide", fig.cap = paste("Median computation times of the simulations for each method for different values of $n$. The gray symbols to the right are the means.", sim_fig_cap, "The scales are logarithmic so a linear trend indicates that computation time is a power of $n$."), par_1x1 = TRUE, cache = FALSE>>=
# Plot parameters

# Plot for computation time
marg <- 1
time_plot_exp <- expression({
  par(xpd=TRUE)
  with(sim_env, {
    matplot(
      n_series, medians[marg,,, "elapsed"], log  = "xy", col = col_medians,
      pch = pchs, type = "p", xaxt='n',
      xlab = "Number of individuals", ylab = "Computation time (seconds)")
    axis(1, at = sim_env$n_series)
    matlines(n_series, medians[marg,,, "elapsed"], lty = 2, col = col_medians)
    matplot(
      n_series * 1.15, means[marg,,, "elapsed"], col = col_means,
      pch = pchs, type = "p", add = T)
  })})

eval(time_plot_exp)
@

<<sim_MSE, echo=FALSE, results="hide", fig.cap = paste("Median mean square error of predicted coefficients of the simulations for each method for different values of $n$. The gray symbols to the right are the means.", sim_fig_cap, "The axis are on the logarithmic scale."), par_1x1 = TRUE, cache = FALSE>>=
# Plot for MSE
marg <- 1

plot_exp <- expression({
  par(xpd=TRUE, yaxs = "i")
  with(sim_env, {
    matplot(
      n_series, medians[marg,,, "MSE"], log  = "xy", col = col_medians,
      pch = pchs, xaxt='n',
      xlab = "Number of individuals", ylab = "MSE of predicted coefficients",
      ylim = c(min(medians[marg,,, "MSE"], na.rm = T),
               max(medians[marg,,, "MSE"], na.rm = T)))
    axis(1, at = sim_env$n_series)
    matlines(n_series,medians[marg,,, "MSE"], lty = 2, col = col_medians)
    matplot(
      n_series * 1.15, means[marg,,, "MSE"], col = col_means,
      pch = pchs, type = "p", add = T)
  })
})

eval(plot_exp)
@

<<n_iter_plot, echo = FALSE, fig.cap = paste0("Median number of iterations of the EM-algorithm. ", sim_fig_cap), par_1x1= TRUE, cache = FALSE>>=
par(xpd=TRUE)

with(sim_env, {
  matplot(n_series, medians[1,,, "niter"],
          pch = pchs, xaxt='n',
          ylim = c(0, 10.5), log  = "x",
          col = col_medians,
          xlab = "Number of individuals", ylab = "Iterations of the EM")
  axis(1, at = sim_env$n_series)
  matplot(n_series, medians[1,,, "niter"],
          type = "l", lty = 2,
          col = col_medians, add = T)
})
@


There are some points worth stressing. Firstly, the computation time of the UKF and GMA can be reduced by using a multithreaded \pkg{BLAS} library. I have seen a reduction up to a factor $2$ for larger datasets on the setup used in the simulation when \pkg{OpenBLAS} \citep{Xianyi12} is used. From experience with \pkg{OpenBLAS}, the SMA and EKF benefits less from a multithreaded BLAS library properly because the matrix and vector products are only in a dimension equal to the dimension of the state vector, $q$. Moreover, one can do trail-and-error tuning with the UKF.

<<hist_of_lp_dens, echo = FALSE, par_1x1=TRUE, fig.cap="Estimated density for the linear predictor in the last interval in the first simulation experiment.">>=
tmp_env <- new.env(parent = sim_env)
with(tmp_env,{
  set.seed(6339855)

  lps <- replicate(1e3, {
    # Draw state at time 30
    n_vars <- n_vars[1]
    a0 <- c(intercept_start, eval(formals(sim_func)$beta_start))
    a30 <- mapply(
      rnorm, n = 1, mean = a0,
      sd = sqrt(t_max) * c(intercept_sd, rep(beta_sd[1], n_vars[1])))

    # Draw linear predictors
    n <- 1e3
    q <- n_vars
    X <- rnorm(n * (q + 1), sd = formals(sim_func)$cov_params)
    dim(X) <- c(q + 1, n)

    drop(a30 %*% X)
    })

  # Compute density estimate
  # hist(lp, breaks = 50)
  dens <- density(c(lps))
  plot(dens$x, dens$y, type = "l",
       main = "", xlab = "Linear predictor", xlim = range(dens$x), yaxs="i")
})

rm(tmp_env)
@

The simulation here is a "extreme" in that the linear predictor can take large absolute values in the last interval with non-negligible probability. Figure~\ref{fig:hist_of_lp_dens} illustrates this by plotting the an estimate of the density of the linear predictor in the last interval when both the state vector and the covariate vector are random as in the simulation.

For the above reason, I perform a second simulation where I draw the covariates from a normal distribution with zero mean and variance $\Sexpr{sim_env$cov_params[[2]]["sigma"]}^2$. Thus, I re-run the simulation experiment with the latter distribution for the covariates. The mean square error is shown in figure~\ref{fig:sim_MSE_altered}. The difference between the filters is small in terms mean square error. Still, it seems that EKF with extra iterations and GMA are preferred especially given the running times which are similar to the previous example as shown in figure~\ref{fig:sim_comp_time_altered}.

In general, I have found it easier to get what seems like sensible results (smooth coefficients curves in reasonable range) with the EKF with extra iteration in the correction step and the GMA method when you alter the learning rate. Thus, the result of the simulation study here is comforting.

<<echo = FALSE>>=
altered_plot_text <- paste0(
  "but where each element of the covariate vectors are drawn from $N\\Lparen{0, ", sim_env$cov_params[[2]]["sigma"], "^2}$.")
@

<<sim_comp_time_altered, echo=FALSE, results="hide", fig.cap = paste("Similar plot to figure~\\ref{fig:sim_comp_time}", altered_plot_text), par_1x1 = TRUE, cache = FALSE>>=
marg <- 2
eval(time_plot_exp)
@

<<sim_MSE_altered, echo=FALSE, results="hide", fig.cap = paste("Similar plot to figure~\\ref{fig:sim_MSE}", altered_plot_text), par_1x1 = TRUE, cache = FALSE>>=
marg <- 2
eval(plot_exp)
@

\section{Conclusion}\label{sec:conc}
I have covered the EM-algorithm implementation in \pkg{dynamichazard} and the four different filters that are available in the \code{ddhazard} function. Pros and cons of the different filters have been highlighted. Further, the dynamic discrete time model and the continuous time model have been covered. A larger real world data set have been analyzed. The simulation study shows that the filters scale well with the number of observation. Further, the  simulation study also showed how the mean square error of the predicted coefficients performs with different number of observations.

I have not covered all the \code{S3} methods that are provided in the \pkg{dynamichazard} package. These include \code{plot}, \code{predict}, \code{hatvalues} and \code{residuals}. It is possible to include weights for the individuals for all the filters. The details hereof are in the ddhazard vignette of this package. Furthermore, bootstrap is implemented in the \code{ddhazard_boot} function. Weights are used in \code{ddhazard_boot} with case resampling which reduce the computation time. Vignettes are provided with the \pkg{dynamichazard} package which illustrates the use of the functions mentioned. A demo of the models is available by running \code{ddhazard_app}. I will end the conclusion by looking at potential further developments.

\subsection{Further developments}

I will summarize some potential future developments of the \pkg{dynamichazard} in this section. Firstly, we could replace the random walk model with a parametric model like an ARMA process for each coefficient. This will require additional parameter to be estimated the matrix $\mat{F}$ in equation~\ref{eqn:stateEqn}. This can be done in the M-step of the EM-algorithm. The constrained EM-algorithm in \pkg{MARSS} \citep{Holmes13} package can be used here. The details of the EM-algorithm may be found in~\cite{Holmes13}. The structure of the equations in the constrained algorithm can be used to avoid the Kronecker products, vectorizion etc. to get implementation that has cubic complexity in the number of parameters to be estimated. Further, we could extend the model to allow the user to specify that certain entries of the covariance matrix $\mat{Q}$ are restricted to zero by using the same formulas.

We can extend the methods to sparse time series for the coefficients. This have received some attention in the Signal Processing literature. An examples is given by~\cite{Charles11} who explore different penalties in the transition of the state vector for the linear Gaussian observational observational equation and non-linear Gaussian state equation. The penalties includes $L1$, $L2$ and a combination of the two (elastic net). Another example is given by~\cite{Angelosante09} who apply the group-Lasso to the linear Gaussian state space models.

The starting value for $\vec{a}_0$ is found with one Fisher scoring step with a GLM model with time-invariant coefficients. The computational complexity of doing this is $\bigO{\Lparen{\sum_{t=1}^d n_t}q^3}$. It takes more than 30\% of the computation time with EKF in the simulation study in section~\ref{sec:sims}. The Fischer Scoring step is made with the function \code{speedglm} from the package \pkg{speedglm} \citep{Enea17} using the eigenvalue decomposition instead of a QR factorization. A cheaper way to find the starting value for the filter can decrease the computation time further. However, we still need a "good" starting point to ensure that the filter do not diverge in my experience.

Mixture models can be implemented to model heterogeneity. The mixture probabilities could be estimated in the M-step. Moreover, both the filters and each iteration of the EM-algorithm could stay at computational complexity of $\bigO{q^3}$ if we assume that the coefficient vectors in the mixtures in the state vector are independent of each other between the mixture densities.

Other models can be implemented in survival analysis like recurrent events and competing risk (see \cite{Fahrmeir96}). Furthermore, the methods can also be used a outside survival analysis. For instance, we could observe real valued outcomes, multinomial outcomes, ordinal outcome etc. for each individual in each interval. The underlying time can depend on the context. E.g. it could be calender time or time since enrollment.

The current implementation of parallel computation for the EKF is based on shared memory. However, it can be extended to a distributed network instead. Different ways of approaching this are introduced in~\cite[chapter 3]{rigatos17}. Two approaches are either to distribute the work in each step of the filter or to run separate filters and aggregate the filters at the end.

An alternative to the filters in the E-step is to use the linearisation method described in~\cite[Section 10.6]{durbin12} mentioned in subsection~\ref{subsec:GMA}. It would be interesting to also implement this approach in the package. A similar idea to the linearisation method in \cite[Section 10.6]{durbin12} is covered in ~\cite{Fahrmeir91} where the authors show a Gauss-Newton and Fisher scoring method.

The methods here can be used as the initial input to the importance sampler with use of antithetic variables and control variables shown in~\cite{Durbin00}. This can extend the methods to cover state vectors with heavy-tailed distribution. This approach is implemented in the package \pkg{KFAS} \citep{kfas}.

<<echo = FALSE, comment = "%", results = "asis", cache = FALSE>>=
# Log session info to tex file
cat(paste("%", capture.output(sessionInfo())), sep = "\n")

cat("%\n%\n% The date and time is", as.character(Sys.time()), "\n")
@


\section*{Acknowledgments}
I would like to thank Hans-Rudolf Künsch and Olivier Wintenberger for constructive conversations. Furthermore, thanks to Søren Feodor Nielsen for feedback and ideas on most aspect of the \code{dynamichazard} package.

\bibliography{bibliography}

\end{document}
